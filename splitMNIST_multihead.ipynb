{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VCL PYTORCH IMPL \n",
    "import torch\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------This class extends the Dataset class and basicly takes a Dataset [images,labels] and sort out the indexes with a specified sub_labels\n",
    "#------see how it it used later to make more sense of it.\n",
    "class SubDataset(Dataset): #FROM https://github.com/GMvandeVen/continual-learning/blob/master/data.py\n",
    "    '''To sub-sample a dataset, taking only those samples with label in [sub_labels].\n",
    "    After this selection of samples has been made, it is possible to transform the target-labels,\n",
    "    which can be useful when doing continual learning with fixed number of output units.'''\n",
    "\n",
    "    def __init__(self, original_dataset, sub_labels, target_transform=None):\n",
    "        super().__init__()\n",
    "        self.dataset = original_dataset\n",
    "        self.sub_indeces = []\n",
    "        for index in range(len(self.dataset)):\n",
    "            if hasattr(original_dataset, \"train_labels\"):\n",
    "                if self.dataset.target_transform is None:\n",
    "                    label = self.dataset.train_labels[index]\n",
    "                else:\n",
    "                    label = self.dataset.target_transform(self.dataset.train_labels[index])\n",
    "            elif hasattr(self.dataset, \"test_labels\"):\n",
    "                if self.dataset.target_transform is None:\n",
    "                    label = self.dataset.test_labels[index]\n",
    "                else:\n",
    "                    label = self.dataset.target_transform(self.dataset.test_labels[index])\n",
    "            else:\n",
    "                label = self.dataset[index][1]\n",
    "            if label in sub_labels:\n",
    "                self.sub_indeces.append(index)\n",
    "        self.target_transform = target_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to /FashionMNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26427392it [00:02, 11266662.83it/s]                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /FashionMNIST\\raw\\train-images-idx3-ubyte.gz to /FashionMNIST\\raw\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to /FashionMNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32768it [00:00, 490105.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /FashionMNIST\\raw\\train-labels-idx1-ubyte.gz to /FashionMNIST\\raw\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to /FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4423680it [00:00, 9302926.37it/s]                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz to /FashionMNIST\\raw\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to /FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8192it [00:00, 92780.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz to /FashionMNIST\\raw\n",
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ralle\\Anaconda3\\lib\\site-packages\\torchvision\\datasets\\mnist.py:43: UserWarning: train_labels has been renamed targets\n",
      "  warnings.warn(\"train_labels has been renamed targets\")\n"
     ]
    }
   ],
   "source": [
    "n_tasks = 5 #one for each number\n",
    "classes_per_task = 2 #because n_task = 10\n",
    "\n",
    "\n",
    "# prepare permutation to shuffle label-ids (to create different class batches for each random seed)\n",
    "#permutation = np.random.permutation(list(range(10)))\n",
    "#--------WE ACTUALLY DONT DO THE ABOVE. we always want the tasks to be the same so no permutation.\n",
    "permutation = list(range(10))\n",
    "#print(\"random permutation of labels\",permutation)\n",
    "#Lambda transform is a user defined transform.\n",
    "#-------nothhing really happens here\n",
    "target_transform = transforms.Lambda(lambda y, x=permutation: int(permutation[y]))\n",
    "\n",
    "dataset_transform = transforms.Compose([transforms.ToTensor()])#A lambda transform to random permutation of pixels can be added here in the permutetMNIST\n",
    "\n",
    "#-------Here the entire MNIST dataset is loaded\n",
    "mnist_train = torchvision.datasets.FashionMNIST(\"/\", train=True,\n",
    "                            download=True, transform=dataset_transform, target_transform=target_transform)\n",
    "mnist_test = torchvision.datasets.FashionMNIST(\"/\", train=False,\n",
    "                            download=True, transform=dataset_transform, target_transform=target_transform)\n",
    "\n",
    "#---------- generate labels-per-task. \n",
    "labels_per_task = [list(np.array(range(classes_per_task)) + classes_per_task * task_id) for task_id in range(n_tasks)]\n",
    "#print(labels_per_task)# [[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]]\n",
    "\n",
    "#----------- split them up into sub-tasks\n",
    "#-------we make a list where all sub dataset get put in.\n",
    "#HERE IS HOW WE take out one task data and labels. this is used in the training loop:\n",
    "#xtrain_set = train_datasets[task_no].dataset.data[train_datasets[task_no].sub_indeces]\n",
    "#ytrain_set = train_datasets[task_no].dataset.targets[train_datasets[task_no].sub_indeces]\n",
    "\n",
    "train_datasets = []\n",
    "test_datasets = []\n",
    "for labels in labels_per_task:\n",
    "    #target_transform = transforms.Lambda(lambda y, x=labels[0]: y - x) if scenario=='domain' else None #We are Task-IL: task is given, is it 1 or not\n",
    "    target_transform = None\n",
    "    train_datasets.append(SubDataset(mnist_train, labels, target_transform=target_transform))\n",
    "    test_datasets.append(SubDataset(mnist_test, labels, target_transform=target_transform))\n",
    "    \n",
    "#train_datasets is a list of 5 tasks with the entire dataset in each.\n",
    "#Each task has the sub_indicies that is the indicies where the target value fits the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Gotta do some coresets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset FashionMNIST\n",
      "    Number of datapoints: 60000\n",
      "    Root location: /\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               ToTensor()\n",
      "           )\n",
      "Target transform: Lambda()\n",
      "12000\n",
      "tensor(3)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(3)\n",
      "tensor(3)\n",
      "tensor(2)\n",
      "tensor(3)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(3)\n",
      "tensor(3)\n",
      "tensor(3)\n",
      "tensor(3)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(3)\n",
      "tensor(3)\n",
      "tensor(2)\n",
      "tensor(3)\n",
      "tensor(3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x1000 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# this is just BS\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(train_datasets[1].dataset)\n",
    "print(len(train_datasets[1].sub_indeces))\n",
    "\n",
    "for i in range(20):\n",
    "    print(train_datasets[1].dataset.targets[train_datasets[1].sub_indeces[i]])\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(1,5):\n",
    "    for j in range(1,5):\n",
    "        plt.subplot(4,4,(j-1)*4+i)\n",
    "        plt.imshow(train_datasets[1].dataset.data[train_datasets[1].sub_indeces[i*j]])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_true(pred,y):\n",
    "    n = len(y)\n",
    "    prec=0\n",
    "    for i in range(n):\n",
    "        pred_index = torch.argmax(pred[i])\n",
    "        true_index = torch.argmax(y[i])\n",
    "        if  pred_index == true_index:\n",
    "            prec +=1\n",
    "    return prec\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (criterion): BCEWithLogitsLoss()\n",
      "  (hidden_w_mean): ParameterList(\n",
      "      (0): Parameter containing: [torch.FloatTensor of size 784x256]\n",
      "      (1): Parameter containing: [torch.FloatTensor of size 256x256]\n",
      "  )\n",
      "  (hidden_b_mean): ParameterList(\n",
      "      (0): Parameter containing: [torch.FloatTensor of size 256x1]\n",
      "      (1): Parameter containing: [torch.FloatTensor of size 256x1]\n",
      "  )\n",
      "  (hidden_w_var): ParameterList(\n",
      "      (0): Parameter containing: [torch.FloatTensor of size 784x256]\n",
      "      (1): Parameter containing: [torch.FloatTensor of size 256x256]\n",
      "  )\n",
      "  (hidden_b_var): ParameterList(\n",
      "      (0): Parameter containing: [torch.FloatTensor of size 256x1]\n",
      "      (1): Parameter containing: [torch.FloatTensor of size 256x1]\n",
      "  )\n",
      "  (out_layers_w_mean): ParameterList()\n",
      "  (out_layers_b_mean): ParameterList()\n",
      "  (out_layers_w_var): ParameterList()\n",
      "  (out_layers_b_var): ParameterList()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as tdist\n",
    "import copy\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# FOR THE SPLITMNSIST MULTIHEAD PROBLEM\n",
    "input_size = 28*28 #size of images\n",
    "hidden_size = 256 #hidden layers size\n",
    "n_hidden_layers = 2\n",
    "output_size = 2\n",
    "n_tasks = n_tasks # also the number of heads\n",
    "init_var = 0   #This is the log(var) we use when we initiate layers. 0 works well with the other hyper parameters.\n",
    "init_var_hid = 0\n",
    "init_var_pri = 0 #Priors are initiated with this log(var)\n",
    "n_samples = 10 #The number of samples we take from the posterior distribution to sample a new weight to use when training\n",
    "\n",
    "torch.manual_seed(1)\n",
    "np.random.seed(1)\n",
    "\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.criterion = torch.nn.BCEWithLogitsLoss()\n",
    "        #Empty parameters to hold layers when these are added\n",
    "        \n",
    "        #notice that posteriors is Parameter list, so they get updated and priors is just a list.\n",
    "        #i may fuck this up when i add layers or something, but as long as the priors are not updated by the optimizer we are good\n",
    "        self.hidden_w_mean = torch.nn.ParameterList([])\n",
    "        self.hidden_b_mean = torch.nn.ParameterList([])\n",
    "        self.hidden_w_var = torch.nn.ParameterList([])\n",
    "        self.hidden_b_var = torch.nn.ParameterList([])\n",
    "\n",
    "        #And the priors\n",
    "        self.pri_hidden_w_mean = []\n",
    "        self.pri_hidden_b_mean = []\n",
    "        self.pri_hidden_w_var = []\n",
    "        self.pri_hidden_b_var = []\n",
    "\n",
    "        #initiating hidden layers. weights are initiatet with N(0,0.1)\n",
    "        for i in range(n_hidden_layers):\n",
    "            if i == 0:\n",
    "                inp_size = input_size\n",
    "            else:\n",
    "                inp_size = hidden_size\n",
    "            self.hidden_w_mean.append(torch.nn.Parameter(torch.randn(inp_size,hidden_size)*0.1, requires_grad=True))\n",
    "            self.hidden_b_mean.append(torch.nn.Parameter(torch.randn(hidden_size,1)*0.1, requires_grad=True))\n",
    "            self.hidden_w_var.append(torch.nn.Parameter(torch.ones(inp_size,hidden_size)*init_var_hid, requires_grad=True))\n",
    "            self.hidden_b_var.append(torch.nn.Parameter(torch.ones(hidden_size,1)*init_var_hid, requires_grad=True))\n",
    "\n",
    "            #And the priors\n",
    "            self.pri_hidden_w_mean.append(torch.zeros(inp_size,hidden_size))\n",
    "            self.pri_hidden_b_mean.append(torch.zeros(hidden_size,1))\n",
    "            self.pri_hidden_w_var.append(torch.ones(inp_size,hidden_size)*init_var_pri)\n",
    "            self.pri_hidden_b_var.append(torch.ones(hidden_size,1)*init_var_pri)\n",
    "\n",
    "       #Empty parameters to hold output layers when these are added in the add_task function. again \n",
    "        #priors will just be a list(or maybe not when we addtask() but it does not matter as long as they are not updated by optimizer)\n",
    "        self.out_layers_w_mean = torch.nn.ParameterList([])\n",
    "        self.out_layers_b_mean = torch.nn.ParameterList([])\n",
    "        self.out_layers_w_var = torch.nn.ParameterList([])\n",
    "        self.out_layers_b_var = torch.nn.ParameterList([])\n",
    "        \n",
    "        self.pri_out_layers_w_mean = None\n",
    "        self.pri_out_layers_b_mean = None\n",
    "        self.pri_out_layers_w_var = None\n",
    "        self.pri_out_layers_b_var = None\n",
    "        \n",
    "           \n",
    "    def forward(self, x, y, task_no, train=True):#y must be 1 of k encoded\n",
    "        #sample from posterior using the reparameterization trick\n",
    "        #from hidden layer\n",
    "        batch_size = x.shape[0] \n",
    "        \n",
    "        x = x.view(-1,input_size) #make it a vector\n",
    "        x = x/256\n",
    "        \n",
    "        for i in range(n_hidden_layers):\n",
    "            if train == False:   #if we are validating the network we just use the current mean of the weights. no need to sample\n",
    "                hidden_w = self.hidden_w_mean[i]\n",
    "                hidden_b = self.hidden_b_mean[i]\n",
    "            else:\n",
    "                with torch.no_grad(): #sample from N(0,1). nograd because of reparameterization trick. SEE TAHT WE take n_samples!\n",
    "                    inp_size,out_size = self.hidden_w_mean[i].size()\n",
    "                    hidden_w = torch.nn.Parameter(torch.randn(n_samples,inp_size,out_size))\n",
    "                    hidden_b = torch.nn.Parameter(torch.randn(n_samples,out_size,1))\n",
    "                \n",
    "                #WE MEAN THE SAMPLES AND generate weights from them\n",
    "                hidden_w = torch.add(self.hidden_w_mean[i],torch.mul(torch.mean(hidden_w,dim=0),torch.exp(0.5*self.hidden_w_var[i])))\n",
    "                hidden_b = torch.add(self.hidden_b_mean[i],torch.mul(torch.mean(hidden_b,dim=0),torch.exp(0.5*self.hidden_b_var[i])))\n",
    "            #then we put data through the layer. ReLU is shit when network is randomly initiatet, but sigmoid works well.(paper uses relu)\n",
    "            x = F.relu(torch.tensordot(torch.transpose(hidden_w,0,1),x,([1],[1]))+hidden_b)\n",
    "            x = torch.transpose(x,0,1)\n",
    "        \n",
    "        # WE DO the same thing as above with the output layer.\n",
    "        #here we chose the layer that maches the task_no\n",
    "        if train == False:\n",
    "            out_w = self.out_layers_w_mean[task_no]\n",
    "            out_b = self.out_layers_b_mean[task_no]\n",
    "        else:        \n",
    "            with torch.no_grad():\n",
    "                inp_size,out_size = self.out_layers_w_mean[task_no].size()\n",
    "                out_w = torch.nn.Parameter(torch.randn(n_samples,inp_size,out_size))\n",
    "                out_b = torch.nn.Parameter(torch.randn(n_samples,out_size,1))\n",
    "            out_w = torch.add(self.out_layers_w_mean[task_no],torch.mul(torch.mean(out_w,dim=0),torch.exp(0.5*self.out_layers_w_var[task_no])))\n",
    "            out_b = torch.add(self.out_layers_b_mean[task_no],torch.mul(torch.mean(out_b,dim=0),torch.exp(0.5*self.out_layers_b_var[task_no])))\n",
    "           \n",
    "        #predict\n",
    "        #print(\"x shape input\",x.shape)\n",
    "        \n",
    "        #print(\"x shape input after view\",x.shape)\n",
    "        #print(\"x shape after unsqueeze\",x.shape)\n",
    "        #print(\"x type\",x.type())\n",
    "        \n",
    "        #print(hidden_w.shape,x.shape,hidden_b.shape)\n",
    "        \n",
    "        x = torch.tensordot(torch.transpose(out_w,0,1),x,([1],[1]))+out_b\n",
    "        \n",
    "        #print(x.shape) # [2xbatch]\n",
    "        #x = torch.transpose(x,0,1)\n",
    "        #print(\"before softmax: \",x)\n",
    "        pred = x\n",
    "        #print(\"after softmax: \",pred)\n",
    "        pred = torch.transpose(pred, 0, 1)\n",
    "        #print(\"after softmax: \",pred)\n",
    "        #Eq*log(likelyhood)-Eq*KL(estimated_prior/prior)\n",
    "        \n",
    "        \n",
    "        # SEE THE loss function\n",
    "        loss, likelihood, kl = self.calc_loss(pred,y,task_no,batch_size)\n",
    "        #print(likelihood, kl)\n",
    "        \n",
    "        return pred, loss, kl\n",
    "            \n",
    "    def calc_loss(self,y,t,task_no,batch_size):#FROM EX 7.2:\n",
    "        # \n",
    "        likelihood = self.criterion(y, t)\n",
    "        \n",
    "        kl = torch.tensor(0.)\n",
    "        \n",
    "        #For the KL loss i use the formula for two multivariate gaussians(but ours is just a single gaussian for each weight)\n",
    "        #for i in range(n_hidden_layers):\n",
    "        #    kl += torch.sum(0.5*(torch.exp(self.hidden_w_var[i]-self.pri_hidden_w_var[i]) + (self.pri_hidden_w_mean[i]-self.hidden_w_mean[i])**2/(torch.exp(self.pri_hidden_w_var[i]))-1+self.pri_hidden_w_var[i]-self.hidden_w_var[i]))\n",
    "        #    kl += torch.sum(0.5*(torch.exp(self.hidden_b_var[i]-self.pri_hidden_b_var[i]) + (self.pri_hidden_b_mean[i]-self.hidden_b_mean[i])**2/(torch.exp(self.pri_hidden_b_var[i]))-1+self.pri_hidden_b_var[i]-self.hidden_b_var[i]))\n",
    "        \n",
    "        ##also for output layers\n",
    "        #kl += torch.sum(0.5*(torch.exp(self.out_layers_w_var[task_no]-self.pri_out_layers_w_var[task_no]) + (self.pri_out_layers_w_mean[task_no]-self.out_layers_w_mean[task_no])**2/(torch.exp(self.pri_out_layers_w_var[task_no]))-1+self.pri_out_layers_w_var[task_no]-self.out_layers_w_var[task_no]))\n",
    "        #kl += torch.sum(0.5*(torch.exp(self.out_layers_b_var[task_no]-self.pri_out_layers_b_var[task_no]) + (self.pri_out_layers_b_mean[task_no]-self.out_layers_b_mean[task_no])**2/(torch.exp(self.pri_out_layers_b_var[task_no]))-1+self.pri_out_layers_b_var[task_no]-self.out_layers_b_var[task_no]))\n",
    "        #print(\"kl\",kl)\n",
    "        \n",
    "        ################THEIR KL ALGO\n",
    "        for i in range(n_hidden_layers):\n",
    "            if i == 0:\n",
    "                inp_size = input_size\n",
    "            else:\n",
    "                inp_size = hidden_size\n",
    "            const_term = -0.5 * hidden_size * inp_size\n",
    "            log_std_diff = 0.5 * torch.sum(self.pri_hidden_w_var[i] - self.hidden_w_var[i])\n",
    "            mu_diff_term = 0.5 * torch.sum((torch.exp(self.hidden_w_var[i]) + (self.pri_hidden_w_mean[i] - self.hidden_w_mean[i])**2) / torch.exp(self.pri_hidden_w_var[i]))\n",
    "            kl += const_term + log_std_diff + mu_diff_term\n",
    "            \n",
    "            const_term = -0.5 * hidden_size\n",
    "            log_std_diff = 0.5 * torch.sum(self.pri_hidden_b_var[i] - self.hidden_b_var[i])\n",
    "            mu_diff_term = 0.5 * torch.sum((torch.exp(self.hidden_b_var[i]) + (self.pri_hidden_b_mean[i] - self.hidden_b_mean[i])**2) / torch.exp(self.pri_hidden_b_var[i]))\n",
    "            kl += const_term + log_std_diff + mu_diff_term\n",
    "        \n",
    "        \n",
    "        #for i in range(len(self.out_layers_b_mean)):\n",
    "        const_term = -0.5 * hidden_size * output_size\n",
    "        log_std_diff = 0.5 * torch.sum(self.pri_out_layers_w_var[task_no] - self.out_layers_w_var[task_no])\n",
    "        mu_diff_term = 0.5 * torch.sum((torch.exp(self.out_layers_w_var[task_no]) + (self.pri_out_layers_w_mean[task_no] - self.out_layers_w_mean[task_no])**2) / torch.exp(self.pri_out_layers_w_var[task_no]))\n",
    "        kl += const_term + log_std_diff + mu_diff_term\n",
    "        \n",
    "        const_term = -0.5  * output_size\n",
    "        log_std_diff = 0.5 * torch.sum(self.pri_out_layers_b_var[task_no] - self.out_layers_b_var[task_no])\n",
    "        mu_diff_term = 0.5 * torch.sum((torch.exp(self.out_layers_b_var[task_no]) + (self.pri_out_layers_b_mean[task_no] - self.out_layers_b_mean[task_no])**2) / torch.exp(self.pri_out_layers_b_var[task_no]))\n",
    "        kl += const_term + log_std_diff + mu_diff_term\n",
    "        \n",
    "        kl = torch.div(kl,batch_size)\n",
    "        ############################\n",
    "\n",
    "        # Combining the two terms in the evidence lower bound objective (ELBO) \n",
    "        #I DONT KNOW IF THE KL needs some kind of scaling since we just added up for every weight in the network\n",
    "        ELBO = torch.add(likelihood, kl) ############################\n",
    "        \n",
    "        #print(\"elbo: \",likelihood, \"kl\", kl)\n",
    "        return ELBO, likelihood, kl\n",
    "    \n",
    "    def add_task(self):\n",
    "        \n",
    "        if self.pri_out_layers_w_mean is None: #if we need to add the first output layer (initiates as the hidden layers)\n",
    "            self.out_layers_w_mean.append(torch.nn.Parameter(torch.randn(hidden_size,output_size)*0.1, requires_grad=True))\n",
    "            self.out_layers_b_mean.append(torch.nn.Parameter(torch.randn(output_size,1)*0.1, requires_grad=True))\n",
    "            self.out_layers_w_var.append(torch.nn.Parameter(torch.ones(hidden_size,output_size)*init_var, requires_grad=True))\n",
    "            self.out_layers_b_var.append(torch.nn.Parameter(torch.ones(output_size,1)*init_var, requires_grad=True))\n",
    "            \n",
    "            self.pri_out_layers_w_mean = [torch.nn.Parameter(torch.zeros(hidden_size,output_size), requires_grad=False)]\n",
    "            self.pri_out_layers_b_mean = [torch.nn.Parameter(torch.zeros(output_size,1), requires_grad=False)]\n",
    "            self.pri_out_layers_w_var = [torch.nn.Parameter(torch.ones(hidden_size,output_size)*init_var_pri, requires_grad=False)]\n",
    "            self.pri_out_layers_b_var = [torch.nn.Parameter(torch.ones(output_size,1)*init_var_pri, requires_grad=False)]\n",
    "            \n",
    "        else:#if it is not the first we initiate the means as the means for the prev layer, but the variance as we defined so we allow it to change more\n",
    "            self.out_layers_w_mean.append(torch.nn.Parameter(torch.randn(hidden_size,output_size)*0.1, requires_grad=True))\n",
    "            self.out_layers_b_mean.append(torch.nn.Parameter(torch.randn(output_size,1)*0.1, requires_grad=True))\n",
    "            self.out_layers_w_var.append(torch.nn.Parameter(torch.ones(hidden_size,output_size)*init_var, requires_grad=True))#initialize new head as the same as previous\n",
    "            self.out_layers_b_var.append(torch.nn.Parameter(torch.ones(output_size,1)*init_var, requires_grad=True))#initialize new head as the same as previous\n",
    "            \n",
    "            #self.out_layers_w_mean.append(torch.nn.Parameter(self.out_layers_w_mean[-1].clone().detach()))#initialize new head as the same as previous\n",
    "            #self.out_layers_b_mean.append(torch.nn.Parameter(self.out_layers_b_mean[-1].clone().detach()))#initialize new head as the same as previous\n",
    "            #self.out_layers_w_var.append(torch.nn.Parameter(self.out_layers_w_var[-1].clone().detach()))#initialize new head as the same as previous\n",
    "            #self.out_layers_b_var.append(torch.nn.Parameter(self.out_layers_b_var[-1].clone().detach()))#initialize new head as the same as previous\n",
    "            \n",
    "            #self.out_layers_w_var.append(torch.nn.Parameter(torch.ones(hidden_size,output_size)*init_var, requires_grad=True))\n",
    "            #self.out_layers_b_var.append(torch.nn.Parameter(torch.ones(output_size,1)*init_var, requires_grad=True))\n",
    "            \n",
    "            #AND THE priors as always:\n",
    "            self.pri_out_layers_w_mean.append(torch.nn.Parameter(torch.zeros(hidden_size,output_size), requires_grad=False))\n",
    "            self.pri_out_layers_b_mean.append(torch.nn.Parameter(torch.zeros(output_size,1), requires_grad=False))\n",
    "            self.pri_out_layers_w_var.append(torch.nn.Parameter(torch.ones(hidden_size,output_size)*init_var_pri, requires_grad=False))\n",
    "            self.pri_out_layers_b_var.append(torch.nn.Parameter(torch.ones(output_size,1)*init_var_pri, requires_grad=False))\n",
    "            \n",
    "            #self.pri_out_layers_w_mean.append(self.pri_out_layers_w_mean[-1])\n",
    "            #self.pri_out_layers_b_mean.append(self.pri_out_layers_b_mean[-1])\n",
    "            #self.pri_out_layers_w_var.append(self.pri_out_layers_w_var[-1])\n",
    "            #self.pri_out_layers_b_var.append(self.pri_out_layers_b_var[-1])\n",
    "            \n",
    "    def update_prior(self,task_no):#call before backward and after forward so we have used the old ones to get loss but copy the old posterior before it is updated in backward\n",
    "        for i in range(n_hidden_layers):# just copies the current mean and var into the priors in a weird way to make shure they requires grad =False\n",
    "            self.pri_hidden_w_mean[i] = self.hidden_w_mean[i].clone().detach()\n",
    "            self.pri_hidden_b_mean[i] = self.hidden_b_mean[i].clone().detach()\n",
    "            self.pri_hidden_w_var[i] = self.hidden_w_var[i].clone().detach()\n",
    "            self.pri_hidden_b_var[i] = self.hidden_b_var[i].clone().detach()\n",
    "\n",
    "            self.pri_hidden_w_mean[i].requires_grad = False\n",
    "            self.pri_hidden_b_mean[i].requires_grad = False\n",
    "            self.pri_hidden_w_var[i].requires_grad = False\n",
    "            self.pri_hidden_b_var[i].requires_grad = False\n",
    "\n",
    "        self.pri_out_layers_w_mean[task_no] = self.out_layers_w_mean[task_no].clone().detach()\n",
    "        self.pri_out_layers_b_mean[task_no] = self.out_layers_b_mean[task_no].clone().detach()\n",
    "        self.pri_out_layers_w_var[task_no] = self.out_layers_w_var[task_no].clone().detach()\n",
    "        self.pri_out_layers_b_var[task_no] = self.out_layers_b_var[task_no].clone().detach()\n",
    "            \n",
    "        self.pri_out_layers_w_mean[task_no].requires_grad = False\n",
    "        self.pri_out_layers_b_mean[task_no].requires_grad = False\n",
    "        self.pri_out_layers_w_var[task_no].requires_grad = False\n",
    "        self.pri_out_layers_b_var[task_no].requires_grad = False\n",
    "    \n",
    "    def reset_var_and_priors(self): #This only resets the first output layer\n",
    "        print(init_var_hid,init_var,init_var_pri)\n",
    "        for i in range(n_hidden_layers):\n",
    "            if i == 0:\n",
    "                inp_size = input_size\n",
    "            else:\n",
    "                inp_size = hidden_size\n",
    "            self.hidden_w_var[i] = torch.nn.Parameter(torch.ones(inp_size,hidden_size)*init_var_hid, requires_grad=True)\n",
    "            self.hidden_b_var[i] = torch.nn.Parameter(torch.ones(hidden_size,1)*init_var_hid, requires_grad=True)\n",
    "            \n",
    "            self.pri_hidden_w_mean[i] = self.hidden_w_mean[i].clone().detach()\n",
    "            self.pri_hidden_b_mean[i]= self.hidden_b_mean[i].clone().detach()\n",
    "            self.pri_hidden_w_mean[i].requires_grad = False\n",
    "            self.pri_hidden_b_mean[i].requires_grad = False\n",
    "            \n",
    "            self.pri_hidden_w_var[i] = torch.ones(inp_size,hidden_size)*init_var_pri\n",
    "            self.pri_hidden_b_var[i] = torch.ones(hidden_size,1)*init_var_pri\n",
    "\n",
    "            \n",
    "        \n",
    "        self.out_layers_w_var[0] = torch.nn.Parameter(torch.ones(hidden_size,output_size)*init_var, requires_grad=True)\n",
    "        self.out_layers_b_var[0] = torch.nn.Parameter(torch.ones(output_size,1)*init_var, requires_grad=True)\n",
    "        \n",
    "        self.pri_out_layers_w_mean[0] = self.out_layers_w_mean[0].clone().detach()\n",
    "        self.pri_out_layers_b_mean[0] = self.out_layers_b_mean[0].clone().detach()\n",
    "        self.pri_out_layers_w_mean[0].requires_grad = False\n",
    "        self.pri_out_layers_b_mean[0].requires_grad = False\n",
    "        \n",
    "        self.pri_out_layers_w_var[0] = torch.nn.Parameter(torch.ones(hidden_size,output_size)*init_var_pri, requires_grad=False)\n",
    "        self.pri_out_layers_b_var[0] = torch.nn.Parameter(torch.ones(output_size,1)*init_var_pri, requires_grad=False)\n",
    "            \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#when resetting i set prior mean to current mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task:  4 \tloss:  tensor(0.1430)\n",
      "ex outvar:  tensor([-5.9896]) \tex hidvar:  tensor([-5.8892])\n",
      "ex pri outvar:  tensor([0.]) \tex pri hidvar:  tensor([-5.8905])\n",
      "ex out m:  tensor([0.0255]) \tex hidmean:  tensor([0.0181])\n",
      "ex pri out m:  tensor([0.]) \tex pri hidmean:  tensor([0.0162])\n",
      "validation prec:  0.9777\n",
      "kl:  tensor(0.1268, grad_fn=<DivBackward0>)\n",
      "9777 10000\n"
     ]
    }
   ],
   "source": [
    "#THE TRAINING AND VALIDATION PART\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "\n",
    "no_epochs = 120\n",
    "batch_size = 60000\n",
    "lr = 0.001\n",
    "\n",
    "xtest_set = [test_datasets[0].dataset.data[test_datasets[0].sub_indeces]]\n",
    "xtrain_set = torch.tensor([])\n",
    "ytest_set = [test_datasets[0].dataset.targets[test_datasets[0].sub_indeces]]\n",
    "ytrain_set = torch.tensor([])\n",
    "\n",
    "batch_loss_list = []\n",
    "\n",
    "#########PRE TRAINING\n",
    "print(\"pretraining\")\n",
    "pre_batch_size = 64\n",
    "net.add_task()\n",
    "optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "xtrain_set = train_datasets[0].dataset.data[train_datasets[0].sub_indeces]\n",
    "ytrain_set = train_datasets[0].dataset.targets[train_datasets[0].sub_indeces]\n",
    "    \n",
    "for epoch in range(1):\n",
    "    print(\"pretraing epoch\", epoch)\n",
    "    num_samples = len(xtrain_set)\n",
    "    num_batches = int(np.ceil(num_samples / float(pre_batch_size)))\n",
    "    for batch in range(num_batches):\n",
    "        net.train()\n",
    "        #get indexes for current batch\n",
    "        idx = range(batch*pre_batch_size, np.minimum((batch+1)*pre_batch_size, num_samples))#indexes to use from batch\n",
    "        X_batch_tr = xtrain_set[idx]\n",
    "        y_batch_tr_raw = ytrain_set[idx]\n",
    "        #TRANSFORM Y:\n",
    "        y_batch_tr = torch.zeros((len(idx),2))\n",
    "        for i in range(len(y_batch_tr_raw)):\n",
    "            if y_batch_tr_raw[i] in [0,2,4,6,8]:\n",
    "                y_batch_tr[i,0] = 1\n",
    "            else: #i used this when i had 2 output neurons\n",
    "                y_batch_tr[i,1] = 1\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output, batch_loss, _ = net(X_batch_tr.type(torch.FloatTensor),y_batch_tr.type(torch.FloatTensor),0)#task no so we choose correct head. calls forward function.\n",
    "        batch_loss.backward() #Update weights and biases to new posterior\n",
    "        optimizer.step()            \n",
    "#DONE PRE TRAINING\n",
    "\n",
    "init_var = -6   #This is the log(var) we use when we initiate layers. 0 works well with the other hyper parameters.\n",
    "init_var_hid = -6\n",
    "init_var_pri = 0\n",
    "\n",
    "\n",
    "net.reset_var_and_priors()\n",
    "\n",
    "\n",
    "\n",
    "#for plotting\n",
    "test_vali = []\n",
    "test_iter = []\n",
    "test_i = 0\n",
    "test_vali_seperate = [[],[],[],[],[]]\n",
    "\n",
    "for task_no in range(n_tasks):\n",
    "    print(\"TASK NUM: \",task_no)\n",
    "    #Get the train and test \n",
    "    xtrain_set = train_datasets[task_no].dataset.data[train_datasets[task_no].sub_indeces]\n",
    "    ytrain_set = train_datasets[task_no].dataset.targets[train_datasets[task_no].sub_indeces]\n",
    "    \n",
    "    \n",
    "    if task_no != 0: #Also whe a new taks comes we append this to the test set\n",
    "        xtest_set.append(test_datasets[task_no].dataset.data[test_datasets[task_no].sub_indeces])\n",
    "        ytest_set.append(test_datasets[task_no].dataset.targets[test_datasets[task_no].sub_indeces])\n",
    "        \n",
    "        net.add_task()#initialize a new head for the network\n",
    "\n",
    "    #TRAIN\n",
    "    optimizer = optim.Adam(net.parameters(), lr=lr) #REDEFINE OPTIMIZER AFTER LAYERS ARE ADDED!\n",
    "\n",
    "    for epoch in range(no_epochs):\n",
    "        net.train()\n",
    "        num_samples = len(xtrain_set)\n",
    "        num_batches = int(np.ceil(num_samples / float(batch_size)))\n",
    "        print(\"num batches: \",num_batches)\n",
    "        \n",
    "        indexes = list(range(num_samples))\n",
    "        np.random.shuffle(list(indexes))\n",
    "        \n",
    "        for batch in range(num_batches):\n",
    "            net.train()\n",
    "            #get indexes for current batch\n",
    "            idx = indexes[slice(batch*batch_size, np.minimum((batch+1)*batch_size, num_samples))]#indexes to use from batch\n",
    "            X_batch_tr = xtrain_set[idx]\n",
    "            y_batch_tr_raw = ytrain_set[idx]\n",
    "            #TRANSFORM Y:\n",
    "            y_batch_tr = torch.zeros((len(idx),2))\n",
    "            for i in range(len(y_batch_tr_raw)):\n",
    "                if y_batch_tr_raw[i] in [0,2,4,6,8]:\n",
    "                    y_batch_tr[i,0] = 1\n",
    "                else: #i used this when i had 2 output neurons\n",
    "                    y_batch_tr[i,1] = 1\n",
    "  \n",
    "            optimizer.zero_grad()\n",
    "            net.zero_grad()\n",
    "            output, batch_loss, kl= net(X_batch_tr.type(torch.FloatTensor),y_batch_tr.type(torch.FloatTensor),task_no, train=True)#task no so we choose correct head. calls forward function.\n",
    "            batch_loss.backward(retain_graph = True) #Update weights and biases to new posterior\n",
    "            optimizer.step()            \n",
    "            \n",
    "            if(batch%100 == 0):#THis is just for evaluation and plotting\n",
    "                #print(net)\n",
    "                net.eval()\n",
    "                vali_accu = 0\n",
    "                batch_loss_list.append(batch_loss.data)\n",
    "                n_test_samples = 0\n",
    "                for i in range(len(xtest_set)):\n",
    "                    task_test_no = i\n",
    "                    \n",
    "                    ytest_set_onehot = torch.zeros((len(ytest_set[i]),2))\n",
    "                    for j in range(len(ytest_set[i])):\n",
    "                        if ytest_set[i][j] in [0,2,4,6,8]:\n",
    "                            ytest_set_onehot[j,0] = 1\n",
    "                        else:\n",
    "                            ytest_set_onehot[j,1] = 1       \n",
    "                    output, _, _ = net(xtest_set[task_test_no].type(torch.FloatTensor),ytest_set_onehot.type(torch.FloatTensor),task_test_no, False)#task no so we choose correct head. calls forward function.\n",
    "                    vali_accu += get_n_true(output,ytest_set_onehot)\n",
    "                    n_test_samples += len(ytest_set_onehot)\n",
    "                    \n",
    "                #for plotting\n",
    "                clear_output(wait=True)\n",
    "                test_vali.append(vali_accu/n_test_samples)#i is task_no\n",
    "                test_iter.append(test_i)\n",
    "                test_i += 1\n",
    "                                \n",
    "                fig = plt.figure(figsize=(12,4))\n",
    "                plt.plot(test_iter,test_vali, label='valid_accu')\n",
    "                #plt.plot(test_iter,batch_loss_list)\n",
    "                plt.legend()\n",
    "                plt.show()\n",
    "                    \n",
    "                print(\"task: \",task_no,\"\\tloss: \",batch_loss.data)\n",
    "                print(\"ex outvar: \", net.out_layers_b_var[task_no][0].data, \"\\tex hidvar: \", net.hidden_b_var[0][0].data)\n",
    "                print(\"ex pri outvar: \", net.pri_out_layers_b_var[task_no][0].data, \"\\tex pri hidvar: \", net.pri_hidden_b_var[0][0].data)\n",
    "                print(\"ex out m: \", net.out_layers_b_mean[task_no][0].data, \"\\tex hidmean: \", net.hidden_b_mean[0][0].data)\n",
    "                print(\"ex pri out m: \", net.pri_out_layers_b_mean[task_no][0].data, \"\\tex pri hidmean: \", net.pri_hidden_b_mean[0][0].data)\n",
    "                print(\"validation prec: \",vali_accu/n_test_samples)\n",
    "                print(\"kl: \",kl)\n",
    "                print(vali_accu,n_test_samples)\n",
    "    \n",
    "    net.update_prior(task_no)\n",
    "    \n",
    "    net.eval()\n",
    "    #THIS is used when we make the last plot of the end results for each taks\n",
    "    for i in range(task_no+1):\n",
    "        y_batch_test_raw = test_datasets[i].dataset.targets[test_datasets[i].sub_indeces]\n",
    "        #TRANSFORM Y TO one hot:\n",
    "        y_batch_test = torch.zeros((len(y_batch_test_raw),2))\n",
    "        for j in range(len(y_batch_test_raw)):\n",
    "            if y_batch_test_raw[j] in [0,2,4,6,8]:\n",
    "                y_batch_test[j,0] = 1\n",
    "            else:\n",
    "                y_batch_test[j,1] = 1\n",
    "        output, _, _ = net(test_datasets[i].dataset.data[test_datasets[i].sub_indeces].type(torch.FloatTensor), y_batch_test.type(torch.FloatTensor), i, False)\n",
    "        test_vali_seperate[i].append([task_no, get_n_true(output,y_batch_test)/len(y_batch_test)])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "for i in range(5):\n",
    "    plt.plot(np.transpose(test_vali_seperate[i])[0],np.transpose(test_vali_seperate[i])[1])\n",
    "    plt.plot(np.transpose(test_vali_seperate[i])[0],np.transpose(test_vali_seperate[i])[1],\"x\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('split_fashion4.pickle', 'wb') as f:\n",
    "    pickle.dump(test_vali_seperate, f)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('split_fashion4.pickle', 'rb') as f:\n",
    "   test_vali_seperate_1 = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "for i in range(5):\n",
    "    plt.plot(np.transpose(test_vali_seperate_1[i])[0],np.transpose(test_vali_seperate_1[i])[1])\n",
    "    plt.plot(np.transpose(test_vali_seperate_1[i])[0],np.transpose(test_vali_seperate_1[i])[1],\"x\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(net.named_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.log(torch.tensor(0.0024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(net.out_layers_b_var[0].requires_grad)\n",
    "print(net.pri_out_layers_b_var[0].requires_grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('hidden_w_mean.0', Parameter containing:\n",
      "tensor([[ 2.1353, -2.3715,  2.3291,  ..., -2.3068,  2.3694,  2.3030],\n",
      "        [ 2.3663, -2.2922,  2.3625,  ..., -2.3557, -2.3699, -2.3070],\n",
      "        [ 2.3397,  2.3608,  2.2949,  ..., -2.1888,  2.3516,  2.3501],\n",
      "        ...,\n",
      "        [ 2.3077, -2.3618, -2.2659,  ..., -2.3371, -2.3140,  2.2923],\n",
      "        [ 2.3638, -2.3181, -2.3490,  ...,  2.3543,  2.3608, -2.3579],\n",
      "        [-2.3075, -2.3575, -2.3455,  ...,  2.3712, -2.3460,  2.3696]],\n",
      "       requires_grad=True))\n",
      "('hidden_w_mean.1', Parameter containing:\n",
      "tensor([[-0.2620,  0.3067, -0.0851,  ..., -0.1688,  0.1854,  0.1521],\n",
      "        [ 1.0431,  2.3428,  0.9990,  ...,  2.3702,  2.2967,  2.1707],\n",
      "        [ 0.1489,  0.1878, -0.1783,  ...,  0.3391,  0.3604,  0.5605],\n",
      "        ...,\n",
      "        [ 2.0066,  2.3320, -0.0949,  ..., -2.3012, -2.3646, -2.3490],\n",
      "        [ 1.0625,  0.3373, -0.0715,  ..., -0.3154,  1.1491,  0.8978],\n",
      "        [ 1.0238,  0.9340, -0.1178,  ..., -1.0098, -1.0133, -0.9456]],\n",
      "       requires_grad=True))\n",
      "('hidden_b_mean.0', Parameter containing:\n",
      "tensor([[ 7.3184e-02],\n",
      "        [ 2.3095e+00],\n",
      "        [ 2.3928e-01],\n",
      "        [-6.4068e-01],\n",
      "        [-1.7990e+00],\n",
      "        [-1.4562e-01],\n",
      "        [-1.9122e+00],\n",
      "        [-2.1879e+00],\n",
      "        [-3.2464e-01],\n",
      "        [ 2.1377e+00],\n",
      "        [-1.0339e+00],\n",
      "        [-2.1257e+00],\n",
      "        [ 1.5887e-01],\n",
      "        [-2.2501e+00],\n",
      "        [ 1.8045e+00],\n",
      "        [-1.4799e-01],\n",
      "        [-2.0567e-01],\n",
      "        [ 1.8289e+00],\n",
      "        [ 3.6023e-01],\n",
      "        [-1.6240e+00],\n",
      "        [ 4.3147e-01],\n",
      "        [-4.6853e-01],\n",
      "        [ 1.6872e+00],\n",
      "        [ 1.2820e+00],\n",
      "        [-1.9373e+00],\n",
      "        [-4.3383e-02],\n",
      "        [-9.8448e-01],\n",
      "        [ 1.9939e+00],\n",
      "        [ 2.0662e+00],\n",
      "        [-1.8107e-01],\n",
      "        [-1.5909e-01],\n",
      "        [ 1.9802e-01],\n",
      "        [ 1.0824e+00],\n",
      "        [-2.9198e-01],\n",
      "        [ 3.6427e-01],\n",
      "        [ 1.4874e+00],\n",
      "        [ 6.5592e-01],\n",
      "        [ 2.2273e+00],\n",
      "        [ 1.6841e-01],\n",
      "        [-1.2515e+00],\n",
      "        [ 2.1709e+00],\n",
      "        [ 1.5919e+00],\n",
      "        [-1.5683e+00],\n",
      "        [ 2.0946e+00],\n",
      "        [-1.4337e-01],\n",
      "        [-4.7994e-01],\n",
      "        [ 2.1111e+00],\n",
      "        [-2.3243e+00],\n",
      "        [-1.6241e+00],\n",
      "        [ 1.4818e+00],\n",
      "        [-1.1841e+00],\n",
      "        [-2.3649e+00],\n",
      "        [ 8.9621e-01],\n",
      "        [ 2.2648e+00],\n",
      "        [-1.0091e+00],\n",
      "        [-2.2968e-02],\n",
      "        [-1.9315e+00],\n",
      "        [ 6.4050e-01],\n",
      "        [-1.0873e+00],\n",
      "        [-3.4036e-01],\n",
      "        [ 2.1877e-01],\n",
      "        [ 1.6753e+00],\n",
      "        [ 1.0929e-01],\n",
      "        [ 7.7575e-01],\n",
      "        [ 1.9257e+00],\n",
      "        [-1.9215e+00],\n",
      "        [-1.5054e+00],\n",
      "        [-2.1113e+00],\n",
      "        [ 6.9371e-01],\n",
      "        [ 8.2430e-01],\n",
      "        [ 2.0484e+00],\n",
      "        [-2.0822e+00],\n",
      "        [ 2.2434e+00],\n",
      "        [ 3.8886e-01],\n",
      "        [ 1.8353e+00],\n",
      "        [ 2.3222e+00],\n",
      "        [-9.8905e-01],\n",
      "        [ 1.9898e-01],\n",
      "        [ 1.7899e+00],\n",
      "        [-2.2631e+00],\n",
      "        [-2.9900e-01],\n",
      "        [ 1.3400e+00],\n",
      "        [-1.4398e+00],\n",
      "        [-4.5716e-01],\n",
      "        [-1.8784e+00],\n",
      "        [ 1.5705e+00],\n",
      "        [ 1.4737e+00],\n",
      "        [ 2.0819e+00],\n",
      "        [ 1.5742e+00],\n",
      "        [ 1.7253e+00],\n",
      "        [-4.8422e-04],\n",
      "        [ 2.2888e+00],\n",
      "        [ 1.7451e+00],\n",
      "        [-2.3041e+00],\n",
      "        [ 1.9469e+00],\n",
      "        [ 1.5928e-02],\n",
      "        [-1.6836e+00],\n",
      "        [ 3.2561e-02],\n",
      "        [-2.4873e-01],\n",
      "        [-9.2043e-02],\n",
      "        [ 1.0977e+00],\n",
      "        [-5.3692e-01],\n",
      "        [ 2.2932e+00],\n",
      "        [ 2.2232e+00],\n",
      "        [-8.9688e-01],\n",
      "        [-1.8646e+00],\n",
      "        [-1.7328e+00],\n",
      "        [-2.3480e+00],\n",
      "        [-2.3502e+00],\n",
      "        [-2.2532e+00],\n",
      "        [-5.4669e-02],\n",
      "        [ 2.3352e+00],\n",
      "        [-1.6414e+00],\n",
      "        [-2.3347e+00],\n",
      "        [-3.9569e-01],\n",
      "        [ 5.7118e-01],\n",
      "        [-2.1611e+00],\n",
      "        [ 3.3158e-01],\n",
      "        [-2.2303e+00],\n",
      "        [ 1.5624e+00],\n",
      "        [ 6.3964e-01],\n",
      "        [ 2.1244e+00],\n",
      "        [ 1.1079e+00],\n",
      "        [ 2.3239e+00],\n",
      "        [-2.0749e+00],\n",
      "        [-2.3692e+00],\n",
      "        [-6.4252e-01],\n",
      "        [-1.2143e-01],\n",
      "        [-3.6193e-01],\n",
      "        [-1.7455e+00],\n",
      "        [ 5.9657e-01],\n",
      "        [ 1.9306e+00],\n",
      "        [-1.2006e+00],\n",
      "        [-1.7680e+00],\n",
      "        [-4.0732e-01],\n",
      "        [ 1.6800e+00],\n",
      "        [ 1.9934e+00],\n",
      "        [ 1.8685e+00],\n",
      "        [-9.0322e-01],\n",
      "        [ 7.2160e-01],\n",
      "        [ 9.7510e-01],\n",
      "        [-1.6433e+00],\n",
      "        [ 1.6456e+00],\n",
      "        [ 2.2558e+00],\n",
      "        [-2.5056e-01],\n",
      "        [ 1.5971e+00],\n",
      "        [ 1.8624e+00],\n",
      "        [-1.3201e+00],\n",
      "        [-2.2802e+00],\n",
      "        [-9.4493e-01],\n",
      "        [ 2.2291e+00],\n",
      "        [ 2.0318e+00],\n",
      "        [-1.4662e+00],\n",
      "        [ 1.7874e+00],\n",
      "        [ 5.5809e-01],\n",
      "        [ 3.2492e-01],\n",
      "        [ 9.5593e-01],\n",
      "        [ 1.5520e+00],\n",
      "        [ 1.1289e+00],\n",
      "        [-1.9133e+00],\n",
      "        [-1.5007e+00],\n",
      "        [-5.1735e-01],\n",
      "        [-1.3164e+00],\n",
      "        [-1.1719e+00],\n",
      "        [-8.8193e-02],\n",
      "        [-2.7175e-01],\n",
      "        [ 8.1259e-01],\n",
      "        [ 1.5646e+00],\n",
      "        [-1.3514e+00],\n",
      "        [ 1.4241e+00],\n",
      "        [-2.1133e+00],\n",
      "        [ 4.6230e-01],\n",
      "        [ 8.1658e-01],\n",
      "        [ 1.9523e+00],\n",
      "        [ 3.3370e-01],\n",
      "        [-7.5962e-01],\n",
      "        [-6.6080e-01],\n",
      "        [ 1.0954e+00],\n",
      "        [ 5.7147e-01],\n",
      "        [ 2.2426e+00],\n",
      "        [ 2.0248e+00],\n",
      "        [-3.1514e-01],\n",
      "        [ 1.2594e+00],\n",
      "        [ 1.2579e+00],\n",
      "        [-1.4259e+00],\n",
      "        [-2.3306e+00],\n",
      "        [ 3.7031e-01],\n",
      "        [ 2.9298e-01],\n",
      "        [-6.0299e-01],\n",
      "        [ 4.9932e-03],\n",
      "        [ 1.6673e+00],\n",
      "        [ 1.3413e-01],\n",
      "        [ 1.8450e-01],\n",
      "        [-1.8123e+00],\n",
      "        [ 1.1755e+00],\n",
      "        [ 1.8019e+00],\n",
      "        [ 2.3171e+00],\n",
      "        [ 2.1262e+00],\n",
      "        [-1.4032e+00],\n",
      "        [ 9.0120e-03],\n",
      "        [-2.3621e+00],\n",
      "        [ 1.4306e+00],\n",
      "        [ 4.9074e-01],\n",
      "        [-2.1489e+00],\n",
      "        [-5.8219e-01],\n",
      "        [-1.9163e-01],\n",
      "        [-1.3347e+00],\n",
      "        [-1.9459e+00],\n",
      "        [ 1.8615e+00],\n",
      "        [ 2.2288e+00],\n",
      "        [-1.7391e+00],\n",
      "        [ 1.5195e+00],\n",
      "        [-2.2377e+00],\n",
      "        [-1.3816e+00],\n",
      "        [ 2.2324e+00],\n",
      "        [-1.6699e+00],\n",
      "        [ 1.6949e+00],\n",
      "        [ 1.4249e-01],\n",
      "        [ 2.2246e+00],\n",
      "        [-1.1422e+00],\n",
      "        [-7.6349e-02],\n",
      "        [-2.2832e+00],\n",
      "        [ 2.0721e+00],\n",
      "        [ 2.1739e+00],\n",
      "        [-5.3654e-01],\n",
      "        [ 1.3899e+00],\n",
      "        [-2.2327e+00],\n",
      "        [-2.2353e+00],\n",
      "        [-1.3775e+00],\n",
      "        [-2.0349e+00],\n",
      "        [-1.9381e-02],\n",
      "        [-2.2919e+00],\n",
      "        [-1.9251e+00],\n",
      "        [ 6.7566e-02],\n",
      "        [ 2.3295e+00],\n",
      "        [-5.6011e-01],\n",
      "        [-2.0987e+00],\n",
      "        [-4.4961e-02],\n",
      "        [ 2.1857e+00],\n",
      "        [-1.7167e+00],\n",
      "        [ 1.6171e+00],\n",
      "        [ 1.1269e+00],\n",
      "        [ 3.8000e-01],\n",
      "        [ 2.8544e-02],\n",
      "        [-1.7354e+00],\n",
      "        [-1.9645e-01],\n",
      "        [ 9.8288e-01],\n",
      "        [-1.9034e+00],\n",
      "        [ 1.1573e-01],\n",
      "        [-6.8918e-01],\n",
      "        [-1.2816e+00],\n",
      "        [ 1.0396e+00],\n",
      "        [-1.2297e+00],\n",
      "        [ 2.3051e+00],\n",
      "        [-1.7230e+00],\n",
      "        [ 9.5795e-01]], requires_grad=True))\n",
      "('hidden_b_mean.1', Parameter containing:\n",
      "tensor([[ 2.3284],\n",
      "        [-2.2878],\n",
      "        [ 2.3243],\n",
      "        [ 2.2949],\n",
      "        [ 2.2987],\n",
      "        [-2.3647],\n",
      "        [ 2.2774],\n",
      "        [-2.3026],\n",
      "        [-2.3524],\n",
      "        [-2.3574],\n",
      "        [-1.9553],\n",
      "        [ 2.2866],\n",
      "        [ 2.3241],\n",
      "        [ 2.2968],\n",
      "        [-2.3202],\n",
      "        [ 2.3536],\n",
      "        [-2.3653],\n",
      "        [-2.3072],\n",
      "        [ 2.2792],\n",
      "        [-2.3388],\n",
      "        [ 2.3295],\n",
      "        [ 2.3564],\n",
      "        [ 2.3659],\n",
      "        [ 2.2979],\n",
      "        [ 2.3445],\n",
      "        [-2.2872],\n",
      "        [ 2.3401],\n",
      "        [-2.3069],\n",
      "        [ 2.3704],\n",
      "        [ 2.3466],\n",
      "        [ 2.3628],\n",
      "        [-2.2957],\n",
      "        [-2.3082],\n",
      "        [-2.2802],\n",
      "        [-2.3298],\n",
      "        [ 2.3469],\n",
      "        [ 2.3365],\n",
      "        [-2.3269],\n",
      "        [-2.3694],\n",
      "        [-2.3457],\n",
      "        [ 2.2776],\n",
      "        [ 2.2621],\n",
      "        [-2.3373],\n",
      "        [-2.2931],\n",
      "        [-2.3598],\n",
      "        [-2.3182],\n",
      "        [ 2.3262],\n",
      "        [-2.3710],\n",
      "        [ 2.3650],\n",
      "        [-2.3577],\n",
      "        [-1.8887],\n",
      "        [-2.3508],\n",
      "        [ 2.3511],\n",
      "        [ 2.3332],\n",
      "        [ 2.3098],\n",
      "        [-2.3287],\n",
      "        [ 2.3653],\n",
      "        [ 2.3656],\n",
      "        [ 2.2948],\n",
      "        [-2.3042],\n",
      "        [-2.3792],\n",
      "        [ 2.3684],\n",
      "        [ 2.3656],\n",
      "        [ 2.3088],\n",
      "        [ 2.3282],\n",
      "        [ 2.3370],\n",
      "        [-2.3430],\n",
      "        [-2.3169],\n",
      "        [ 2.2968],\n",
      "        [ 2.3291],\n",
      "        [-2.3328],\n",
      "        [ 2.3664],\n",
      "        [-2.3319],\n",
      "        [ 2.3057],\n",
      "        [ 2.3442],\n",
      "        [-2.2979],\n",
      "        [-2.3433],\n",
      "        [-2.3009],\n",
      "        [ 2.3592],\n",
      "        [ 2.3472],\n",
      "        [-2.3461],\n",
      "        [ 2.3012],\n",
      "        [ 2.3420],\n",
      "        [-2.3672],\n",
      "        [-2.3662],\n",
      "        [ 2.3282],\n",
      "        [ 2.2819],\n",
      "        [ 2.3574],\n",
      "        [-2.3191],\n",
      "        [ 2.3138],\n",
      "        [ 2.3170],\n",
      "        [-2.3093],\n",
      "        [ 2.3199],\n",
      "        [-2.2916],\n",
      "        [-2.3305],\n",
      "        [-2.3099],\n",
      "        [-2.2948],\n",
      "        [-2.3504],\n",
      "        [-2.2695],\n",
      "        [-2.3451],\n",
      "        [ 2.3058],\n",
      "        [-2.3656],\n",
      "        [-2.2801],\n",
      "        [-2.3643],\n",
      "        [ 1.8750],\n",
      "        [-2.2265],\n",
      "        [ 2.2886],\n",
      "        [-2.3628],\n",
      "        [ 2.2371],\n",
      "        [-2.3357],\n",
      "        [-2.3137],\n",
      "        [-2.3420],\n",
      "        [-2.2875],\n",
      "        [-2.3271],\n",
      "        [-2.2890],\n",
      "        [-2.3204],\n",
      "        [ 2.3380],\n",
      "        [ 1.8184],\n",
      "        [-2.3558],\n",
      "        [-2.3475],\n",
      "        [ 2.3649],\n",
      "        [-2.3241],\n",
      "        [-2.3274],\n",
      "        [ 2.3552],\n",
      "        [-2.2963],\n",
      "        [ 2.3693],\n",
      "        [-2.3324],\n",
      "        [-2.3361],\n",
      "        [-2.3443],\n",
      "        [-2.3695],\n",
      "        [-2.3119],\n",
      "        [-2.3477],\n",
      "        [-2.3212],\n",
      "        [-2.3517],\n",
      "        [-2.2951],\n",
      "        [-2.3110],\n",
      "        [ 2.3643],\n",
      "        [-2.3010],\n",
      "        [-2.3569],\n",
      "        [ 2.3329],\n",
      "        [-2.3310],\n",
      "        [-1.7785],\n",
      "        [-2.3209],\n",
      "        [ 2.2924],\n",
      "        [ 2.3718],\n",
      "        [ 2.3123],\n",
      "        [-2.3400],\n",
      "        [-2.3383],\n",
      "        [ 2.3310],\n",
      "        [-2.3521],\n",
      "        [ 2.3703],\n",
      "        [-2.3221],\n",
      "        [ 2.3252],\n",
      "        [-2.1616],\n",
      "        [ 2.2350],\n",
      "        [-2.3053],\n",
      "        [ 2.3177],\n",
      "        [ 2.3522],\n",
      "        [-2.3342],\n",
      "        [-2.3005],\n",
      "        [-2.2968],\n",
      "        [ 2.3373],\n",
      "        [ 2.2686],\n",
      "        [ 2.3101],\n",
      "        [ 2.3499],\n",
      "        [-2.3389],\n",
      "        [-2.2929],\n",
      "        [ 2.3472],\n",
      "        [-2.3122],\n",
      "        [-2.3185],\n",
      "        [ 2.3090],\n",
      "        [ 2.3157],\n",
      "        [-2.2910],\n",
      "        [-2.3282],\n",
      "        [-2.2993],\n",
      "        [ 2.3680],\n",
      "        [-2.3551],\n",
      "        [ 2.3056],\n",
      "        [-2.3530],\n",
      "        [-2.3505],\n",
      "        [ 2.3072],\n",
      "        [ 2.3607],\n",
      "        [-2.2717],\n",
      "        [-2.3275],\n",
      "        [ 2.2858],\n",
      "        [-2.3661],\n",
      "        [-2.3214],\n",
      "        [ 2.3426],\n",
      "        [ 2.3242],\n",
      "        [-2.3657],\n",
      "        [-2.3042],\n",
      "        [ 2.3584],\n",
      "        [ 2.3047],\n",
      "        [-2.2786],\n",
      "        [-2.3598],\n",
      "        [-2.3631],\n",
      "        [ 2.3590],\n",
      "        [-2.3500],\n",
      "        [ 2.3580],\n",
      "        [-2.3660],\n",
      "        [ 2.3535],\n",
      "        [ 2.3250],\n",
      "        [-2.3521],\n",
      "        [ 2.2906],\n",
      "        [ 2.3656],\n",
      "        [ 2.3079],\n",
      "        [-2.3331],\n",
      "        [ 2.3408],\n",
      "        [-2.3617],\n",
      "        [-2.2947],\n",
      "        [-2.2921],\n",
      "        [ 2.3076],\n",
      "        [-2.3033],\n",
      "        [ 2.3467],\n",
      "        [-2.2843],\n",
      "        [ 2.3641],\n",
      "        [-2.3158],\n",
      "        [-2.3615],\n",
      "        [ 2.3284],\n",
      "        [-2.3645],\n",
      "        [ 2.3521],\n",
      "        [ 1.8501],\n",
      "        [ 2.3185],\n",
      "        [ 2.3118],\n",
      "        [-2.3156],\n",
      "        [-2.3615],\n",
      "        [ 2.3002],\n",
      "        [ 2.3662],\n",
      "        [-2.3595],\n",
      "        [-2.3262],\n",
      "        [-2.2898],\n",
      "        [-2.2942],\n",
      "        [-2.3319],\n",
      "        [-2.2954],\n",
      "        [-2.3178],\n",
      "        [-2.3587],\n",
      "        [-2.2946],\n",
      "        [ 2.3545],\n",
      "        [ 2.3711],\n",
      "        [ 2.3465],\n",
      "        [-2.3293],\n",
      "        [ 2.3500],\n",
      "        [ 2.3704],\n",
      "        [-2.3585],\n",
      "        [-2.3441],\n",
      "        [ 2.3054],\n",
      "        [ 2.2893],\n",
      "        [ 2.3242],\n",
      "        [-2.3352],\n",
      "        [ 2.3248],\n",
      "        [ 2.2629],\n",
      "        [ 2.3529],\n",
      "        [-2.3582],\n",
      "        [ 2.3660],\n",
      "        [-2.2939],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        [ 1.7611]], requires_grad=True))\n",
      "('hidden_w_var.0', Parameter containing:\n",
      "tensor([[-2.0060, -2.0060, -2.0060,  ..., -2.0060, -2.0060, -2.0060],\n",
      "        [-2.0060, -2.0060, -2.0060,  ..., -2.0060, -2.0060, -2.0060],\n",
      "        [-2.0060, -2.0060, -2.0060,  ..., -2.0060, -2.0060, -2.0060],\n",
      "        ...,\n",
      "        [-2.0060, -2.0060, -2.0060,  ..., -2.0060, -2.0060, -2.0060],\n",
      "        [-2.0060, -2.0060, -2.0060,  ..., -2.0060, -2.0060, -2.0060],\n",
      "        [-2.0060, -2.0060, -2.0060,  ..., -2.0060, -2.0060, -2.0060]],\n",
      "       requires_grad=True))\n",
      "('hidden_w_var.1', Parameter containing:\n",
      "tensor([[-2.0043, -1.9662, -1.9985,  ..., -2.0133, -2.0021, -2.0196],\n",
      "        [-1.9822, -2.0060, -2.0309,  ..., -2.0060, -2.0060, -2.0060],\n",
      "        [-1.9854, -2.0167, -1.9416,  ..., -2.0111, -2.0151, -1.9843],\n",
      "        ...,\n",
      "        [-2.0060, -2.0061, -2.0152,  ..., -2.0060, -2.0060, -2.0062],\n",
      "        [-2.0036, -2.0177, -1.9933,  ..., -2.0207, -1.9812, -2.0193],\n",
      "        [-1.9610, -2.0431, -1.9964,  ..., -2.0184, -1.9940, -2.0399]],\n",
      "       requires_grad=True))\n",
      "('hidden_b_var.0', Parameter containing:\n",
      "tensor([[-2.0101],\n",
      "        [-2.0063],\n",
      "        [-2.0104],\n",
      "        [-2.0159],\n",
      "        [-2.0087],\n",
      "        [-2.0054],\n",
      "        [-2.0009],\n",
      "        [-2.0058],\n",
      "        [-2.0135],\n",
      "        [-2.0087],\n",
      "        [-2.0133],\n",
      "        [-2.0069],\n",
      "        [-2.0025],\n",
      "        [-2.0082],\n",
      "        [-2.0030],\n",
      "        [-2.0068],\n",
      "        [-2.0014],\n",
      "        [-2.0048],\n",
      "        [-1.9935],\n",
      "        [-2.0063],\n",
      "        [-2.0007],\n",
      "        [-2.0073],\n",
      "        [-2.0175],\n",
      "        [-2.0107],\n",
      "        [-2.0053],\n",
      "        [-2.0074],\n",
      "        [-2.0087],\n",
      "        [-2.0094],\n",
      "        [-2.0048],\n",
      "        [-2.0108],\n",
      "        [-2.0042],\n",
      "        [-2.0062],\n",
      "        [-2.0028],\n",
      "        [-2.0072],\n",
      "        [-2.0056],\n",
      "        [-2.0032],\n",
      "        [-2.0057],\n",
      "        [-2.0073],\n",
      "        [-2.0074],\n",
      "        [-2.0134],\n",
      "        [-2.0010],\n",
      "        [-2.0171],\n",
      "        [-2.0095],\n",
      "        [-2.0109],\n",
      "        [-2.0014],\n",
      "        [-2.0016],\n",
      "        [-2.0065],\n",
      "        [-2.0059],\n",
      "        [-2.0039],\n",
      "        [-1.9952],\n",
      "        [-2.0125],\n",
      "        [-2.0060],\n",
      "        [-2.0204],\n",
      "        [-2.0062],\n",
      "        [-2.0043],\n",
      "        [-2.0124],\n",
      "        [-2.0060],\n",
      "        [-2.0061],\n",
      "        [-2.0111],\n",
      "        [-2.0034],\n",
      "        [-2.0058],\n",
      "        [-1.9966],\n",
      "        [-2.0069],\n",
      "        [-1.9872],\n",
      "        [-2.0090],\n",
      "        [-2.0001],\n",
      "        [-2.0036],\n",
      "        [-2.0069],\n",
      "        [-2.0111],\n",
      "        [-2.0021],\n",
      "        [-2.0064],\n",
      "        [-2.0044],\n",
      "        [-2.0053],\n",
      "        [-2.0086],\n",
      "        [-2.0017],\n",
      "        [-2.0079],\n",
      "        [-2.0048],\n",
      "        [-2.0113],\n",
      "        [-2.0017],\n",
      "        [-2.0064],\n",
      "        [-1.9941],\n",
      "        [-2.0079],\n",
      "        [-2.0107],\n",
      "        [-2.0097],\n",
      "        [-2.0072],\n",
      "        [-2.0078],\n",
      "        [-2.0086],\n",
      "        [-2.0065],\n",
      "        [-2.0031],\n",
      "        [-2.0183],\n",
      "        [-2.0065],\n",
      "        [-2.0055],\n",
      "        [-2.0117],\n",
      "        [-2.0094],\n",
      "        [-2.0049],\n",
      "        [-2.0131],\n",
      "        [-1.9951],\n",
      "        [-2.0027],\n",
      "        [-2.0082],\n",
      "        [-2.0073],\n",
      "        [-2.0050],\n",
      "        [-2.0063],\n",
      "        [-2.0050],\n",
      "        [-2.0026],\n",
      "        [-2.0010],\n",
      "        [-2.0096],\n",
      "        [-2.0064],\n",
      "        [-2.0066],\n",
      "        [-2.0060],\n",
      "        [-2.0066],\n",
      "        [-2.0013],\n",
      "        [-2.0063],\n",
      "        [-2.0059],\n",
      "        [-2.0061],\n",
      "        [-2.0084],\n",
      "        [-2.0078],\n",
      "        [-2.0081],\n",
      "        [-2.0111],\n",
      "        [-2.0061],\n",
      "        [-1.9988],\n",
      "        [-2.0034],\n",
      "        [-2.0077],\n",
      "        [-1.9921],\n",
      "        [-2.0059],\n",
      "        [-2.0113],\n",
      "        [-2.0060],\n",
      "        [-2.0018],\n",
      "        [-2.0096],\n",
      "        [-2.0079],\n",
      "        [-2.0130],\n",
      "        [-2.0019],\n",
      "        [-2.0062],\n",
      "        [-2.0035],\n",
      "        [-2.0042],\n",
      "        [-1.9977],\n",
      "        [-2.0091],\n",
      "        [-2.0061],\n",
      "        [-1.9996],\n",
      "        [-2.0078],\n",
      "        [-1.9990],\n",
      "        [-1.9877],\n",
      "        [-1.9953],\n",
      "        [-2.0069],\n",
      "        [-2.0092],\n",
      "        [-2.0102],\n",
      "        [-2.0072],\n",
      "        [-2.0144],\n",
      "        [-2.0274],\n",
      "        [-2.0061],\n",
      "        [-2.0083],\n",
      "        [-2.0063],\n",
      "        [-2.0097],\n",
      "        [-2.0142],\n",
      "        [-2.0083],\n",
      "        [-1.9908],\n",
      "        [-1.9995],\n",
      "        [-2.0051],\n",
      "        [-2.0081],\n",
      "        [-2.0121],\n",
      "        [-2.0018],\n",
      "        [-2.0018],\n",
      "        [-2.0002],\n",
      "        [-2.0078],\n",
      "        [-2.0035],\n",
      "        [-1.9985],\n",
      "        [-2.0131],\n",
      "        [-2.0095],\n",
      "        [-2.0092],\n",
      "        [-2.0184],\n",
      "        [-2.0050],\n",
      "        [-2.0073],\n",
      "        [-2.0098],\n",
      "        [-2.0039],\n",
      "        [-2.0061],\n",
      "        [-2.0031],\n",
      "        [-2.0078],\n",
      "        [-2.0055],\n",
      "        [-2.0065],\n",
      "        [-2.0126],\n",
      "        [-2.0057],\n",
      "        [-2.0040],\n",
      "        [-2.0015],\n",
      "        [-2.0131],\n",
      "        [-2.0094],\n",
      "        [-2.0109],\n",
      "        [-2.0060],\n",
      "        [-2.0096],\n",
      "        [-2.0061],\n",
      "        [-2.0048],\n",
      "        [-2.0158],\n",
      "        [-2.0007],\n",
      "        [-2.0118],\n",
      "        [-2.0074],\n",
      "        [-2.0099],\n",
      "        [-2.0048],\n",
      "        [-1.9970],\n",
      "        [-2.0060],\n",
      "        [-2.0047],\n",
      "        [-2.0082],\n",
      "        [-2.0048],\n",
      "        [-2.0060],\n",
      "        [-2.0051],\n",
      "        [-2.0054],\n",
      "        [-2.0029],\n",
      "        [-1.9978],\n",
      "        [-2.0140],\n",
      "        [-2.0060],\n",
      "        [-2.0056],\n",
      "        [-2.0139],\n",
      "        [-2.0053],\n",
      "        [-2.0061],\n",
      "        [-2.0106],\n",
      "        [-2.0060],\n",
      "        [-2.0056],\n",
      "        [-2.0066],\n",
      "        [-2.0058],\n",
      "        [-2.0053],\n",
      "        [-2.0129],\n",
      "        [-2.0085],\n",
      "        [-2.0033],\n",
      "        [-2.0090],\n",
      "        [-2.0069],\n",
      "        [-2.0084],\n",
      "        [-2.0064],\n",
      "        [-2.0051],\n",
      "        [-2.0069],\n",
      "        [-2.0088],\n",
      "        [-2.0052],\n",
      "        [-2.0102],\n",
      "        [-2.0066],\n",
      "        [-2.0023],\n",
      "        [-2.0064],\n",
      "        [-2.0062],\n",
      "        [-2.0010],\n",
      "        [-2.0060],\n",
      "        [-2.0003],\n",
      "        [-2.0064],\n",
      "        [-2.0046],\n",
      "        [-2.0054],\n",
      "        [-2.0075],\n",
      "        [-2.0028],\n",
      "        [-2.0136],\n",
      "        [-2.0021],\n",
      "        [-2.0050],\n",
      "        [-2.0121],\n",
      "        [-2.0113],\n",
      "        [-2.0069],\n",
      "        [-2.0112],\n",
      "        [-2.0068],\n",
      "        [-2.0152],\n",
      "        [-2.0085],\n",
      "        [-2.0128],\n",
      "        [-2.0059],\n",
      "        [-2.0060],\n",
      "        [-2.0033],\n",
      "        [-2.0102]], requires_grad=True))\n",
      "('hidden_b_var.1', Parameter containing:\n",
      "tensor([[-2.0057],\n",
      "        [-2.0049],\n",
      "        [-2.0053],\n",
      "        [-2.0057],\n",
      "        [-2.0065],\n",
      "        [-2.0060],\n",
      "        [-2.0062],\n",
      "        [-2.0066],\n",
      "        [-2.0066],\n",
      "        [-2.0061],\n",
      "        [-2.0048],\n",
      "        [-2.0061],\n",
      "        [-2.0057],\n",
      "        [-2.0067],\n",
      "        [-2.0048],\n",
      "        [-2.0060],\n",
      "        [-2.0061],\n",
      "        [-2.0062],\n",
      "        [-2.0061],\n",
      "        [-2.0061],\n",
      "        [-2.0057],\n",
      "        [-2.0052],\n",
      "        [-2.0065],\n",
      "        [-2.0063],\n",
      "        [-2.0058],\n",
      "        [-2.0056],\n",
      "        [-2.0070],\n",
      "        [-2.0065],\n",
      "        [-2.0058],\n",
      "        [-2.0061],\n",
      "        [-2.0061],\n",
      "        [-2.0052],\n",
      "        [-2.0060],\n",
      "        [-2.0064],\n",
      "        [-2.0061],\n",
      "        [-2.0056],\n",
      "        [-2.0060],\n",
      "        [-2.0061],\n",
      "        [-2.0060],\n",
      "        [-2.0062],\n",
      "        [-2.0053],\n",
      "        [-2.0061],\n",
      "        [-2.0063],\n",
      "        [-2.0055],\n",
      "        [-2.0061],\n",
      "        [-2.0054],\n",
      "        [-2.0059],\n",
      "        [-2.0061],\n",
      "        [-2.0060],\n",
      "        [-2.0068],\n",
      "        [-2.0059],\n",
      "        [-2.0045],\n",
      "        [-2.0058],\n",
      "        [-2.0056],\n",
      "        [-2.0061],\n",
      "        [-2.0066],\n",
      "        [-2.0052],\n",
      "        [-2.0065],\n",
      "        [-2.0051],\n",
      "        [-2.0064],\n",
      "        [-2.0060],\n",
      "        [-2.0058],\n",
      "        [-2.0069],\n",
      "        [-2.0074],\n",
      "        [-2.0070],\n",
      "        [-2.0055],\n",
      "        [-2.0061],\n",
      "        [-2.0060],\n",
      "        [-2.0066],\n",
      "        [-2.0078],\n",
      "        [-2.0059],\n",
      "        [-2.0060],\n",
      "        [-2.0069],\n",
      "        [-2.0049],\n",
      "        [-2.0072],\n",
      "        [-2.0064],\n",
      "        [-2.0054],\n",
      "        [-2.0061],\n",
      "        [-2.0053],\n",
      "        [-2.0063],\n",
      "        [-2.0052],\n",
      "        [-2.0064],\n",
      "        [-2.0062],\n",
      "        [-2.0060],\n",
      "        [-2.0070],\n",
      "        [-2.0048],\n",
      "        [-2.0068],\n",
      "        [-2.0060],\n",
      "        [-2.0062],\n",
      "        [-2.0057],\n",
      "        [-2.0053],\n",
      "        [-2.0060],\n",
      "        [-2.0051],\n",
      "        [-2.0072],\n",
      "        [-2.0060],\n",
      "        [-2.0051],\n",
      "        [-2.0052],\n",
      "        [-2.0067],\n",
      "        [-2.0048],\n",
      "        [-2.0047],\n",
      "        [-2.0055],\n",
      "        [-2.0060],\n",
      "        [-2.0063],\n",
      "        [-2.0061],\n",
      "        [-2.0057],\n",
      "        [-2.0044],\n",
      "        [-2.0059],\n",
      "        [-2.0059],\n",
      "        [-2.0042],\n",
      "        [-2.0063],\n",
      "        [-2.0062],\n",
      "        [-2.0061],\n",
      "        [-2.0066],\n",
      "        [-2.0058],\n",
      "        [-2.0054],\n",
      "        [-2.0054],\n",
      "        [-2.0061],\n",
      "        [-2.0057],\n",
      "        [-2.0070],\n",
      "        [-2.0062],\n",
      "        [-2.0065],\n",
      "        [-2.0070],\n",
      "        [-2.0069],\n",
      "        [-2.0055],\n",
      "        [-2.0064],\n",
      "        [-2.0057],\n",
      "        [-2.0072],\n",
      "        [-2.0060],\n",
      "        [-2.0059],\n",
      "        [-2.0061],\n",
      "        [-2.0064],\n",
      "        [-2.0060],\n",
      "        [-2.0055],\n",
      "        [-2.0069],\n",
      "        [-2.0062],\n",
      "        [-2.0058],\n",
      "        [-2.0057],\n",
      "        [-2.0064],\n",
      "        [-2.0066],\n",
      "        [-2.0060],\n",
      "        [-2.0068],\n",
      "        [-2.0062],\n",
      "        [-2.0059],\n",
      "        [-2.0064],\n",
      "        [-2.0061],\n",
      "        [-2.0067],\n",
      "        [-2.0060],\n",
      "        [-2.0060],\n",
      "        [-2.0064],\n",
      "        [-2.0071],\n",
      "        [-2.0060],\n",
      "        [-2.0060],\n",
      "        [-2.0056],\n",
      "        [-2.0071],\n",
      "        [-2.0061],\n",
      "        [-2.0063],\n",
      "        [-2.0066],\n",
      "        [-2.0062],\n",
      "        [-2.0064],\n",
      "        [-2.0064],\n",
      "        [-2.0060],\n",
      "        [-2.0066],\n",
      "        [-2.0066],\n",
      "        [-2.0060],\n",
      "        [-2.0061],\n",
      "        [-2.0060],\n",
      "        [-2.0058],\n",
      "        [-2.0058],\n",
      "        [-2.0062],\n",
      "        [-2.0058],\n",
      "        [-2.0060],\n",
      "        [-2.0060],\n",
      "        [-2.0078],\n",
      "        [-2.0060],\n",
      "        [-2.0060],\n",
      "        [-2.0068],\n",
      "        [-2.0062],\n",
      "        [-2.0064],\n",
      "        [-2.0057],\n",
      "        [-2.0062],\n",
      "        [-2.0069],\n",
      "        [-2.0063],\n",
      "        [-2.0063],\n",
      "        [-2.0066],\n",
      "        [-2.0067],\n",
      "        [-2.0061],\n",
      "        [-2.0060],\n",
      "        [-2.0069],\n",
      "        [-2.0061],\n",
      "        [-2.0064],\n",
      "        [-2.0060],\n",
      "        [-2.0070],\n",
      "        [-2.0061],\n",
      "        [-2.0059],\n",
      "        [-2.0060],\n",
      "        [-2.0059],\n",
      "        [-2.0057],\n",
      "        [-2.0056],\n",
      "        [-2.0063],\n",
      "        [-2.0055],\n",
      "        [-2.0060],\n",
      "        [-2.0063],\n",
      "        [-2.0062],\n",
      "        [-2.0058],\n",
      "        [-2.0062],\n",
      "        [-2.0053],\n",
      "        [-2.0060],\n",
      "        [-2.0059],\n",
      "        [-2.0060],\n",
      "        [-2.0060],\n",
      "        [-2.0056],\n",
      "        [-2.0062],\n",
      "        [-2.0054],\n",
      "        [-2.0058],\n",
      "        [-2.0062],\n",
      "        [-2.0061],\n",
      "        [-2.0060],\n",
      "        [-2.0060],\n",
      "        [-2.0074],\n",
      "        [-2.0062],\n",
      "        [-2.0060],\n",
      "        [-2.0075],\n",
      "        [-2.0054],\n",
      "        [-2.0061],\n",
      "        [-2.0058],\n",
      "        [-2.0074],\n",
      "        [-2.0059],\n",
      "        [-2.0061],\n",
      "        [-2.0058],\n",
      "        [-2.0061],\n",
      "        [-2.0063],\n",
      "        [-2.0063],\n",
      "        [-2.0060],\n",
      "        [-2.0062],\n",
      "        [-2.0065],\n",
      "        [-2.0055],\n",
      "        [-2.0060],\n",
      "        [-2.0060],\n",
      "        [-2.0061],\n",
      "        [-2.0054],\n",
      "        [-2.0060],\n",
      "        [-2.0063],\n",
      "        [-2.0060],\n",
      "        [-2.0056],\n",
      "        [-2.0070],\n",
      "        [-2.0061],\n",
      "        [-2.0052],\n",
      "        [-2.0066],\n",
      "        [-2.0068],\n",
      "        [-2.0060],\n",
      "        [-2.0045],\n",
      "        [-2.0069],\n",
      "        [-2.0060],\n",
      "        [-2.0059],\n",
      "        [-2.0059],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        [-2.0059]], requires_grad=True))\n",
      "('out_layers_w_mean.0', Parameter containing:\n",
      "tensor([[-0.1903],\n",
      "        [ 0.0262],\n",
      "        [-0.0187],\n",
      "        [ 0.0391],\n",
      "        [-0.0975],\n",
      "        [ 0.1049],\n",
      "        [-0.0994],\n",
      "        [ 0.0796],\n",
      "        [ 0.1510],\n",
      "        [-0.2374],\n",
      "        [ 0.1518],\n",
      "        [ 0.1402],\n",
      "        [ 0.1292],\n",
      "        [ 0.0045],\n",
      "        [ 0.0320],\n",
      "        [-0.0529],\n",
      "        [-0.1723],\n",
      "        [-0.0487],\n",
      "        [-0.0101],\n",
      "        [-0.1039],\n",
      "        [ 0.0980],\n",
      "        [-0.0898],\n",
      "        [ 0.0149],\n",
      "        [-0.2111],\n",
      "        [-0.2005],\n",
      "        [-0.0352],\n",
      "        [-0.0243],\n",
      "        [ 0.0324],\n",
      "        [ 0.1276],\n",
      "        [-0.0008],\n",
      "        [-0.0485],\n",
      "        [ 0.0467],\n",
      "        [-0.1710],\n",
      "        [-0.0421],\n",
      "        [ 0.1007],\n",
      "        [-0.0359],\n",
      "        [ 0.0170],\n",
      "        [-0.0017],\n",
      "        [ 0.0028],\n",
      "        [ 0.1358],\n",
      "        [ 0.1168],\n",
      "        [ 0.0400],\n",
      "        [-0.1053],\n",
      "        [ 0.1915],\n",
      "        [-0.0676],\n",
      "        [-0.1115],\n",
      "        [ 0.0798],\n",
      "        [-0.1054],\n",
      "        [-0.0671],\n",
      "        [ 0.1163],\n",
      "        [-0.0285],\n",
      "        [ 0.0332],\n",
      "        [ 0.0678],\n",
      "        [-0.1401],\n",
      "        [-0.0023],\n",
      "        [ 0.1802],\n",
      "        [-0.0314],\n",
      "        [ 0.1172],\n",
      "        [ 0.0535],\n",
      "        [ 0.0785],\n",
      "        [-0.0885],\n",
      "        [ 0.1050],\n",
      "        [ 0.1351],\n",
      "        [-0.0167],\n",
      "        [ 0.1122],\n",
      "        [-0.0486],\n",
      "        [ 0.0296],\n",
      "        [ 0.1171],\n",
      "        [-0.1351],\n",
      "        [-0.0918],\n",
      "        [-0.0133],\n",
      "        [-0.0442],\n",
      "        [ 0.1277],\n",
      "        [ 0.0197],\n",
      "        [ 0.1515],\n",
      "        [-0.1221],\n",
      "        [-0.0461],\n",
      "        [-0.0314],\n",
      "        [-0.1424],\n",
      "        [ 0.1186],\n",
      "        [ 0.0110],\n",
      "        [ 0.1450],\n",
      "        [-0.0545],\n",
      "        [ 0.1462],\n",
      "        [-0.1460],\n",
      "        [ 0.0074],\n",
      "        [-0.1978],\n",
      "        [-0.0649],\n",
      "        [-0.2025],\n",
      "        [-0.0720],\n",
      "        [ 0.0533],\n",
      "        [-0.1790],\n",
      "        [ 0.1728],\n",
      "        [ 0.0539],\n",
      "        [-0.1613],\n",
      "        [ 0.1614],\n",
      "        [-0.0549],\n",
      "        [ 0.2185],\n",
      "        [ 0.1172],\n",
      "        [-0.1163],\n",
      "        [ 0.0862],\n",
      "        [ 0.0622],\n",
      "        [ 0.0773],\n",
      "        [-0.0410],\n",
      "        [-0.1419],\n",
      "        [ 0.0584],\n",
      "        [-0.1348],\n",
      "        [ 0.0312],\n",
      "        [-0.0038],\n",
      "        [ 0.2127],\n",
      "        [ 0.1029],\n",
      "        [ 0.0215],\n",
      "        [ 0.1136],\n",
      "        [ 0.0414],\n",
      "        [ 0.0475],\n",
      "        [ 0.0827],\n",
      "        [-0.1627],\n",
      "        [-0.0519],\n",
      "        [ 0.1619],\n",
      "        [-0.2396],\n",
      "        [-0.0534],\n",
      "        [-0.0651],\n",
      "        [ 0.0792],\n",
      "        [-0.0251],\n",
      "        [ 0.1267],\n",
      "        [ 0.0574],\n",
      "        [ 0.2329],\n",
      "        [-0.0465],\n",
      "        [-0.0632],\n",
      "        [-0.0525],\n",
      "        [-0.1053],\n",
      "        [-0.0538],\n",
      "        [-0.0968],\n",
      "        [ 0.1452],\n",
      "        [ 0.0578],\n",
      "        [ 0.0832],\n",
      "        [-0.1313],\n",
      "        [ 0.0547],\n",
      "        [-0.1050],\n",
      "        [ 0.0265],\n",
      "        [ 0.0327],\n",
      "        [ 0.0266],\n",
      "        [-0.1569],\n",
      "        [-0.1224],\n",
      "        [ 0.1686],\n",
      "        [-0.1877],\n",
      "        [ 0.0422],\n",
      "        [-0.0204],\n",
      "        [ 0.0697],\n",
      "        [-0.1029],\n",
      "        [ 0.0396],\n",
      "        [-0.0398],\n",
      "        [-0.0204],\n",
      "        [ 0.1085],\n",
      "        [-0.1440],\n",
      "        [-0.0699],\n",
      "        [ 0.0198],\n",
      "        [ 0.2530],\n",
      "        [-0.1037],\n",
      "        [-0.0459],\n",
      "        [ 0.0216],\n",
      "        [ 0.1995],\n",
      "        [-0.0495],\n",
      "        [ 0.1277],\n",
      "        [-0.0535],\n",
      "        [-0.0426],\n",
      "        [-0.0938],\n",
      "        [ 0.0283],\n",
      "        [-0.0525],\n",
      "        [-0.0584],\n",
      "        [ 0.0382],\n",
      "        [ 0.0835],\n",
      "        [ 0.0363],\n",
      "        [-0.1242],\n",
      "        [ 0.2544],\n",
      "        [-0.1829],\n",
      "        [ 0.2067],\n",
      "        [ 0.0278],\n",
      "        [ 0.0611],\n",
      "        [-0.1123],\n",
      "        [-0.0398],\n",
      "        [-0.1421],\n",
      "        [ 0.1490],\n",
      "        [-0.0761],\n",
      "        [-0.2486],\n",
      "        [ 0.0662],\n",
      "        [-0.1427],\n",
      "        [-0.0423],\n",
      "        [-0.1042],\n",
      "        [ 0.0082],\n",
      "        [ 0.1954],\n",
      "        [-0.0098],\n",
      "        [-0.0691],\n",
      "        [-0.0991],\n",
      "        [-0.0357],\n",
      "        [ 0.0610],\n",
      "        [-0.1260],\n",
      "        [ 0.0281],\n",
      "        [-0.1012],\n",
      "        [-0.0977],\n",
      "        [-0.0024],\n",
      "        [-0.0953],\n",
      "        [ 0.1132],\n",
      "        [-0.0343],\n",
      "        [ 0.1176],\n",
      "        [ 0.0745],\n",
      "        [-0.1688],\n",
      "        [ 0.0394],\n",
      "        [-0.0161],\n",
      "        [ 0.1912],\n",
      "        [ 0.0105],\n",
      "        [-0.2989],\n",
      "        [-0.1098],\n",
      "        [ 0.0101],\n",
      "        [-0.0427],\n",
      "        [ 0.1688],\n",
      "        [-0.0034],\n",
      "        [-0.1422],\n",
      "        [-0.1134],\n",
      "        [ 0.0763],\n",
      "        [-0.0176],\n",
      "        [-0.0466],\n",
      "        [-0.0147],\n",
      "        [ 0.1515],\n",
      "        [ 0.1368],\n",
      "        [-0.0672],\n",
      "        [-0.0848],\n",
      "        [ 0.0688],\n",
      "        [ 0.0496],\n",
      "        [-0.1586],\n",
      "        [-0.0080],\n",
      "        [ 0.1623],\n",
      "        [ 0.1054],\n",
      "        [-0.1195],\n",
      "        [ 0.1287],\n",
      "        [-0.1423],\n",
      "        [-0.0843],\n",
      "        [-0.1362],\n",
      "        [-0.0705],\n",
      "        [ 0.0331],\n",
      "        [-0.2247],\n",
      "        [ 0.0587],\n",
      "        [ 0.1032],\n",
      "        [ 0.0142],\n",
      "        [-0.0516],\n",
      "        [-0.1659],\n",
      "        [ 0.0073],\n",
      "        [ 0.1867],\n",
      "        [-0.0581],\n",
      "        [ 0.1499],\n",
      "        [-0.0184],\n",
      "        [-0.0625],\n",
      "        [ 0.0140],\n",
      "        [ 0.0201],\n",
      "        [ 0.0906],\n",
      "        [-0.0815]], requires_grad=True))\n",
      "('out_layers_w_mean.1', Parameter containing:\n",
      "tensor([[-0.1903],\n",
      "        [ 0.0262],\n",
      "        [-0.0187],\n",
      "        [ 0.0391],\n",
      "        [-0.0975],\n",
      "        [ 0.1049],\n",
      "        [-0.0994],\n",
      "        [ 0.0796],\n",
      "        [ 0.1510],\n",
      "        [-0.2374],\n",
      "        [ 0.1518],\n",
      "        [ 0.1402],\n",
      "        [ 0.1292],\n",
      "        [ 0.0045],\n",
      "        [ 0.0320],\n",
      "        [-0.0529],\n",
      "        [-0.1723],\n",
      "        [-0.0487],\n",
      "        [-0.0101],\n",
      "        [-0.1039],\n",
      "        [ 0.0980],\n",
      "        [-0.0898],\n",
      "        [ 0.0149],\n",
      "        [-0.2111],\n",
      "        [-0.2005],\n",
      "        [-0.0352],\n",
      "        [-0.0243],\n",
      "        [ 0.0324],\n",
      "        [ 0.1276],\n",
      "        [-0.0008],\n",
      "        [-0.0485],\n",
      "        [ 0.0467],\n",
      "        [-0.1710],\n",
      "        [-0.0421],\n",
      "        [ 0.1007],\n",
      "        [-0.0359],\n",
      "        [ 0.0170],\n",
      "        [-0.0017],\n",
      "        [ 0.0028],\n",
      "        [ 0.1358],\n",
      "        [ 0.1168],\n",
      "        [ 0.0400],\n",
      "        [-0.1053],\n",
      "        [ 0.1915],\n",
      "        [-0.0676],\n",
      "        [-0.1115],\n",
      "        [ 0.0798],\n",
      "        [-0.1054],\n",
      "        [-0.0671],\n",
      "        [ 0.1163],\n",
      "        [-0.0285],\n",
      "        [ 0.0332],\n",
      "        [ 0.0678],\n",
      "        [-0.1401],\n",
      "        [-0.0023],\n",
      "        [ 0.1802],\n",
      "        [-0.0314],\n",
      "        [ 0.1172],\n",
      "        [ 0.0535],\n",
      "        [ 0.0785],\n",
      "        [-0.0885],\n",
      "        [ 0.1050],\n",
      "        [ 0.1351],\n",
      "        [-0.0167],\n",
      "        [ 0.1122],\n",
      "        [-0.0486],\n",
      "        [ 0.0296],\n",
      "        [ 0.1171],\n",
      "        [-0.1351],\n",
      "        [-0.0918],\n",
      "        [-0.0133],\n",
      "        [-0.0442],\n",
      "        [ 0.1277],\n",
      "        [ 0.0197],\n",
      "        [ 0.1515],\n",
      "        [-0.1221],\n",
      "        [-0.0461],\n",
      "        [-0.0314],\n",
      "        [-0.1424],\n",
      "        [ 0.1186],\n",
      "        [ 0.0110],\n",
      "        [ 0.1450],\n",
      "        [-0.0545],\n",
      "        [ 0.1462],\n",
      "        [-0.1460],\n",
      "        [ 0.0074],\n",
      "        [-0.1978],\n",
      "        [-0.0649],\n",
      "        [-0.2025],\n",
      "        [-0.0720],\n",
      "        [ 0.0533],\n",
      "        [-0.1790],\n",
      "        [ 0.1728],\n",
      "        [ 0.0539],\n",
      "        [-0.1613],\n",
      "        [ 0.1614],\n",
      "        [-0.0549],\n",
      "        [ 0.2185],\n",
      "        [ 0.1172],\n",
      "        [-0.1163],\n",
      "        [ 0.0862],\n",
      "        [ 0.0622],\n",
      "        [ 0.0773],\n",
      "        [-0.0410],\n",
      "        [-0.1419],\n",
      "        [ 0.0584],\n",
      "        [-0.1348],\n",
      "        [ 0.0312],\n",
      "        [-0.0038],\n",
      "        [ 0.2127],\n",
      "        [ 0.1029],\n",
      "        [ 0.0215],\n",
      "        [ 0.1136],\n",
      "        [ 0.0414],\n",
      "        [ 0.0475],\n",
      "        [ 0.0827],\n",
      "        [-0.1627],\n",
      "        [-0.0519],\n",
      "        [ 0.1619],\n",
      "        [-0.2396],\n",
      "        [-0.0534],\n",
      "        [-0.0651],\n",
      "        [ 0.0792],\n",
      "        [-0.0251],\n",
      "        [ 0.1267],\n",
      "        [ 0.0574],\n",
      "        [ 0.2329],\n",
      "        [-0.0465],\n",
      "        [-0.0632],\n",
      "        [-0.0525],\n",
      "        [-0.1053],\n",
      "        [-0.0538],\n",
      "        [-0.0968],\n",
      "        [ 0.1452],\n",
      "        [ 0.0578],\n",
      "        [ 0.0832],\n",
      "        [-0.1313],\n",
      "        [ 0.0547],\n",
      "        [-0.1050],\n",
      "        [ 0.0265],\n",
      "        [ 0.0327],\n",
      "        [ 0.0266],\n",
      "        [-0.1569],\n",
      "        [-0.1224],\n",
      "        [ 0.1686],\n",
      "        [-0.1877],\n",
      "        [ 0.0422],\n",
      "        [-0.0204],\n",
      "        [ 0.0697],\n",
      "        [-0.1029],\n",
      "        [ 0.0396],\n",
      "        [-0.0398],\n",
      "        [-0.0204],\n",
      "        [ 0.1085],\n",
      "        [-0.1440],\n",
      "        [-0.0699],\n",
      "        [ 0.0198],\n",
      "        [ 0.2530],\n",
      "        [-0.1037],\n",
      "        [-0.0459],\n",
      "        [ 0.0216],\n",
      "        [ 0.1995],\n",
      "        [-0.0495],\n",
      "        [ 0.1277],\n",
      "        [-0.0535],\n",
      "        [-0.0426],\n",
      "        [-0.0938],\n",
      "        [ 0.0283],\n",
      "        [-0.0525],\n",
      "        [-0.0584],\n",
      "        [ 0.0382],\n",
      "        [ 0.0835],\n",
      "        [ 0.0363],\n",
      "        [-0.1242],\n",
      "        [ 0.2544],\n",
      "        [-0.1829],\n",
      "        [ 0.2067],\n",
      "        [ 0.0278],\n",
      "        [ 0.0611],\n",
      "        [-0.1123],\n",
      "        [-0.0398],\n",
      "        [-0.1421],\n",
      "        [ 0.1490],\n",
      "        [-0.0761],\n",
      "        [-0.2486],\n",
      "        [ 0.0662],\n",
      "        [-0.1427],\n",
      "        [-0.0423],\n",
      "        [-0.1042],\n",
      "        [ 0.0082],\n",
      "        [ 0.1954],\n",
      "        [-0.0098],\n",
      "        [-0.0691],\n",
      "        [-0.0991],\n",
      "        [-0.0357],\n",
      "        [ 0.0610],\n",
      "        [-0.1260],\n",
      "        [ 0.0281],\n",
      "        [-0.1012],\n",
      "        [-0.0977],\n",
      "        [-0.0024],\n",
      "        [-0.0953],\n",
      "        [ 0.1132],\n",
      "        [-0.0343],\n",
      "        [ 0.1176],\n",
      "        [ 0.0745],\n",
      "        [-0.1688],\n",
      "        [ 0.0394],\n",
      "        [-0.0161],\n",
      "        [ 0.1912],\n",
      "        [ 0.0105],\n",
      "        [-0.2989],\n",
      "        [-0.1098],\n",
      "        [ 0.0101],\n",
      "        [-0.0427],\n",
      "        [ 0.1688],\n",
      "        [-0.0034],\n",
      "        [-0.1422],\n",
      "        [-0.1134],\n",
      "        [ 0.0763],\n",
      "        [-0.0176],\n",
      "        [-0.0466],\n",
      "        [-0.0147],\n",
      "        [ 0.1515],\n",
      "        [ 0.1368],\n",
      "        [-0.0672],\n",
      "        [-0.0848],\n",
      "        [ 0.0688],\n",
      "        [ 0.0496],\n",
      "        [-0.1586],\n",
      "        [-0.0080],\n",
      "        [ 0.1623],\n",
      "        [ 0.1054],\n",
      "        [-0.1195],\n",
      "        [ 0.1287],\n",
      "        [-0.1423],\n",
      "        [-0.0843],\n",
      "        [-0.1362],\n",
      "        [-0.0705],\n",
      "        [ 0.0331],\n",
      "        [-0.2247],\n",
      "        [ 0.0587],\n",
      "        [ 0.1032],\n",
      "        [ 0.0142],\n",
      "        [-0.0516],\n",
      "        [-0.1659],\n",
      "        [ 0.0073],\n",
      "        [ 0.1867],\n",
      "        [-0.0581],\n",
      "        [ 0.1499],\n",
      "        [-0.0184],\n",
      "        [-0.0625],\n",
      "        [ 0.0140],\n",
      "        [ 0.0201],\n",
      "        [ 0.0906],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        [-0.0815]], requires_grad=True))\n",
      "('out_layers_w_mean.2', Parameter containing:\n",
      "tensor([[-0.1903],\n",
      "        [ 0.0262],\n",
      "        [-0.0187],\n",
      "        [ 0.0391],\n",
      "        [-0.0975],\n",
      "        [ 0.1049],\n",
      "        [-0.0994],\n",
      "        [ 0.0796],\n",
      "        [ 0.1510],\n",
      "        [-0.2374],\n",
      "        [ 0.1518],\n",
      "        [ 0.1402],\n",
      "        [ 0.1292],\n",
      "        [ 0.0045],\n",
      "        [ 0.0320],\n",
      "        [-0.0529],\n",
      "        [-0.1723],\n",
      "        [-0.0487],\n",
      "        [-0.0101],\n",
      "        [-0.1039],\n",
      "        [ 0.0980],\n",
      "        [-0.0898],\n",
      "        [ 0.0149],\n",
      "        [-0.2111],\n",
      "        [-0.2005],\n",
      "        [-0.0352],\n",
      "        [-0.0243],\n",
      "        [ 0.0324],\n",
      "        [ 0.1276],\n",
      "        [-0.0008],\n",
      "        [-0.0485],\n",
      "        [ 0.0467],\n",
      "        [-0.1710],\n",
      "        [-0.0421],\n",
      "        [ 0.1007],\n",
      "        [-0.0359],\n",
      "        [ 0.0170],\n",
      "        [-0.0017],\n",
      "        [ 0.0028],\n",
      "        [ 0.1358],\n",
      "        [ 0.1168],\n",
      "        [ 0.0400],\n",
      "        [-0.1053],\n",
      "        [ 0.1915],\n",
      "        [-0.0676],\n",
      "        [-0.1115],\n",
      "        [ 0.0798],\n",
      "        [-0.1054],\n",
      "        [-0.0671],\n",
      "        [ 0.1163],\n",
      "        [-0.0285],\n",
      "        [ 0.0332],\n",
      "        [ 0.0678],\n",
      "        [-0.1401],\n",
      "        [-0.0023],\n",
      "        [ 0.1802],\n",
      "        [-0.0314],\n",
      "        [ 0.1172],\n",
      "        [ 0.0535],\n",
      "        [ 0.0785],\n",
      "        [-0.0885],\n",
      "        [ 0.1050],\n",
      "        [ 0.1351],\n",
      "        [-0.0167],\n",
      "        [ 0.1122],\n",
      "        [-0.0486],\n",
      "        [ 0.0296],\n",
      "        [ 0.1171],\n",
      "        [-0.1351],\n",
      "        [-0.0918],\n",
      "        [-0.0133],\n",
      "        [-0.0442],\n",
      "        [ 0.1277],\n",
      "        [ 0.0197],\n",
      "        [ 0.1515],\n",
      "        [-0.1221],\n",
      "        [-0.0461],\n",
      "        [-0.0314],\n",
      "        [-0.1424],\n",
      "        [ 0.1186],\n",
      "        [ 0.0110],\n",
      "        [ 0.1450],\n",
      "        [-0.0545],\n",
      "        [ 0.1462],\n",
      "        [-0.1460],\n",
      "        [ 0.0074],\n",
      "        [-0.1978],\n",
      "        [-0.0649],\n",
      "        [-0.2025],\n",
      "        [-0.0720],\n",
      "        [ 0.0533],\n",
      "        [-0.1790],\n",
      "        [ 0.1728],\n",
      "        [ 0.0539],\n",
      "        [-0.1613],\n",
      "        [ 0.1614],\n",
      "        [-0.0549],\n",
      "        [ 0.2185],\n",
      "        [ 0.1172],\n",
      "        [-0.1163],\n",
      "        [ 0.0862],\n",
      "        [ 0.0622],\n",
      "        [ 0.0773],\n",
      "        [-0.0410],\n",
      "        [-0.1419],\n",
      "        [ 0.0584],\n",
      "        [-0.1348],\n",
      "        [ 0.0312],\n",
      "        [-0.0038],\n",
      "        [ 0.2127],\n",
      "        [ 0.1029],\n",
      "        [ 0.0215],\n",
      "        [ 0.1136],\n",
      "        [ 0.0414],\n",
      "        [ 0.0475],\n",
      "        [ 0.0827],\n",
      "        [-0.1627],\n",
      "        [-0.0519],\n",
      "        [ 0.1619],\n",
      "        [-0.2396],\n",
      "        [-0.0534],\n",
      "        [-0.0651],\n",
      "        [ 0.0792],\n",
      "        [-0.0251],\n",
      "        [ 0.1267],\n",
      "        [ 0.0574],\n",
      "        [ 0.2329],\n",
      "        [-0.0465],\n",
      "        [-0.0632],\n",
      "        [-0.0525],\n",
      "        [-0.1053],\n",
      "        [-0.0538],\n",
      "        [-0.0968],\n",
      "        [ 0.1452],\n",
      "        [ 0.0578],\n",
      "        [ 0.0832],\n",
      "        [-0.1313],\n",
      "        [ 0.0547],\n",
      "        [-0.1050],\n",
      "        [ 0.0265],\n",
      "        [ 0.0327],\n",
      "        [ 0.0266],\n",
      "        [-0.1569],\n",
      "        [-0.1224],\n",
      "        [ 0.1686],\n",
      "        [-0.1877],\n",
      "        [ 0.0422],\n",
      "        [-0.0204],\n",
      "        [ 0.0697],\n",
      "        [-0.1029],\n",
      "        [ 0.0396],\n",
      "        [-0.0398],\n",
      "        [-0.0204],\n",
      "        [ 0.1085],\n",
      "        [-0.1440],\n",
      "        [-0.0699],\n",
      "        [ 0.0198],\n",
      "        [ 0.2530],\n",
      "        [-0.1037],\n",
      "        [-0.0459],\n",
      "        [ 0.0216],\n",
      "        [ 0.1995],\n",
      "        [-0.0495],\n",
      "        [ 0.1277],\n",
      "        [-0.0535],\n",
      "        [-0.0426],\n",
      "        [-0.0938],\n",
      "        [ 0.0283],\n",
      "        [-0.0525],\n",
      "        [-0.0584],\n",
      "        [ 0.0382],\n",
      "        [ 0.0835],\n",
      "        [ 0.0363],\n",
      "        [-0.1242],\n",
      "        [ 0.2544],\n",
      "        [-0.1829],\n",
      "        [ 0.2067],\n",
      "        [ 0.0278],\n",
      "        [ 0.0611],\n",
      "        [-0.1123],\n",
      "        [-0.0398],\n",
      "        [-0.1421],\n",
      "        [ 0.1490],\n",
      "        [-0.0761],\n",
      "        [-0.2486],\n",
      "        [ 0.0662],\n",
      "        [-0.1427],\n",
      "        [-0.0423],\n",
      "        [-0.1042],\n",
      "        [ 0.0082],\n",
      "        [ 0.1954],\n",
      "        [-0.0098],\n",
      "        [-0.0691],\n",
      "        [-0.0991],\n",
      "        [-0.0357],\n",
      "        [ 0.0610],\n",
      "        [-0.1260],\n",
      "        [ 0.0281],\n",
      "        [-0.1012],\n",
      "        [-0.0977],\n",
      "        [-0.0024],\n",
      "        [-0.0953],\n",
      "        [ 0.1132],\n",
      "        [-0.0343],\n",
      "        [ 0.1176],\n",
      "        [ 0.0745],\n",
      "        [-0.1688],\n",
      "        [ 0.0394],\n",
      "        [-0.0161],\n",
      "        [ 0.1912],\n",
      "        [ 0.0105],\n",
      "        [-0.2989],\n",
      "        [-0.1098],\n",
      "        [ 0.0101],\n",
      "        [-0.0427],\n",
      "        [ 0.1688],\n",
      "        [-0.0034],\n",
      "        [-0.1422],\n",
      "        [-0.1134],\n",
      "        [ 0.0763],\n",
      "        [-0.0176],\n",
      "        [-0.0466],\n",
      "        [-0.0147],\n",
      "        [ 0.1515],\n",
      "        [ 0.1368],\n",
      "        [-0.0672],\n",
      "        [-0.0848],\n",
      "        [ 0.0688],\n",
      "        [ 0.0496],\n",
      "        [-0.1586],\n",
      "        [-0.0080],\n",
      "        [ 0.1623],\n",
      "        [ 0.1054],\n",
      "        [-0.1195],\n",
      "        [ 0.1287],\n",
      "        [-0.1423],\n",
      "        [-0.0843],\n",
      "        [-0.1362],\n",
      "        [-0.0705],\n",
      "        [ 0.0331],\n",
      "        [-0.2247],\n",
      "        [ 0.0587],\n",
      "        [ 0.1032],\n",
      "        [ 0.0142],\n",
      "        [-0.0516],\n",
      "        [-0.1659],\n",
      "        [ 0.0073],\n",
      "        [ 0.1867],\n",
      "        [-0.0581],\n",
      "        [ 0.1499],\n",
      "        [-0.0184],\n",
      "        [-0.0625],\n",
      "        [ 0.0140],\n",
      "        [ 0.0201],\n",
      "        [ 0.0906],\n",
      "        [-0.0815]], requires_grad=True))\n",
      "('out_layers_b_mean.0', Parameter containing:\n",
      "tensor([[-0.0843]], requires_grad=True))\n",
      "('out_layers_b_mean.1', Parameter containing:\n",
      "tensor([[-0.0843]], requires_grad=True))\n",
      "('out_layers_b_mean.2', Parameter containing:\n",
      "tensor([[-0.0843]], requires_grad=True))\n",
      "('out_layers_w_var.0', Parameter containing:\n",
      "tensor([[-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        [-2.]], requires_grad=True))\n",
      "('out_layers_w_var.1', Parameter containing:\n",
      "tensor([[-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.]], requires_grad=True))\n",
      "('out_layers_w_var.2', Parameter containing:\n",
      "tensor([[-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.]], requires_grad=True))\n",
      "('out_layers_b_var.0', Parameter containing:\n",
      "tensor([[-2.]], requires_grad=True))\n",
      "('out_layers_b_var.1', Parameter containing:\n",
      "tensor([[-2.]], requires_grad=True))\n",
      "('out_layers_b_var.2', Parameter containing:\n",
      "tensor([[-2.]], requires_grad=True))\n"
     ]
    }
   ],
   "source": [
    "for i in net.named_parameters():\n",
    "    print(i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1992, 0.6211, 0.9883,\n",
      "         0.6211, 0.1953, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1875, 0.9297, 0.9844, 0.9844,\n",
      "         0.9844, 0.9258, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.2109, 0.8867, 0.9883, 0.9844, 0.9336,\n",
      "         0.9102, 0.9844, 0.2227, 0.0234, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0391, 0.2344, 0.8750, 0.9844, 0.9883, 0.9844, 0.7891,\n",
      "         0.3281, 0.9844, 0.9883, 0.4766, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.6367, 0.9844, 0.9844, 0.9844, 0.9883, 0.9844, 0.9844,\n",
      "         0.3750, 0.7383, 0.9883, 0.6523, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.1992, 0.9297, 0.9883, 0.9883, 0.7422, 0.4453, 0.9883, 0.8906,\n",
      "         0.1836, 0.3086, 0.9961, 0.6562, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.1875, 0.9297, 0.9844, 0.9844, 0.6992, 0.0469, 0.2930, 0.4727, 0.0820,\n",
      "         0.0000, 0.0000, 0.9883, 0.9492, 0.1953, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1484,\n",
      "         0.6445, 0.9883, 0.9102, 0.8125, 0.3281, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.9883, 0.9844, 0.6445, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0273, 0.6953,\n",
      "         0.9844, 0.9375, 0.2773, 0.0742, 0.1094, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.9883, 0.9844, 0.7617, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2227, 0.9844,\n",
      "         0.9844, 0.2461, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.9883, 0.9844, 0.7617, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7734, 0.9883,\n",
      "         0.7422, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.9961, 0.9883, 0.7656, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2969, 0.9609, 0.9844,\n",
      "         0.4375, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.9883, 0.9844, 0.5781, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3320, 0.9844, 0.8984,\n",
      "         0.0977, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0273, 0.5273, 0.9883, 0.7266, 0.0469, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3320, 0.9844, 0.8711,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0273,\n",
      "         0.5117, 0.9844, 0.8789, 0.2773, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3320, 0.9844, 0.5664,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1875, 0.6445,\n",
      "         0.9844, 0.6758, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3359, 0.9883, 0.8789,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4453, 0.9297, 0.9883,\n",
      "         0.6328, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3320, 0.9844, 0.9727,\n",
      "         0.5703, 0.1875, 0.1133, 0.3320, 0.6953, 0.8789, 0.9883, 0.8711, 0.6523,\n",
      "         0.2188, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3320, 0.9844, 0.9844,\n",
      "         0.9844, 0.8945, 0.8398, 0.9844, 0.9844, 0.9844, 0.7656, 0.5078, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1094, 0.7773, 0.9844,\n",
      "         0.9844, 0.9883, 0.9844, 0.9844, 0.9102, 0.5664, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0977, 0.5000,\n",
      "         0.9844, 0.9883, 0.9844, 0.5508, 0.1445, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000]])\n"
     ]
    }
   ],
   "source": [
    "print(X_batch_tr.type(torch.FloatTensor)[0]/256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(len(net.out_layers_b_mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
