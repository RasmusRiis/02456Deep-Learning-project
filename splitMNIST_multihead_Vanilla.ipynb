{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VCL PYTORCH IMPL \n",
    "import torch\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------This class extends the Dataset class and basicly takes a Dataset [images,labels] and sort out the indexes with a specified sub_labels\n",
    "#------see how it it used later to make more sense of it.\n",
    "class SubDataset(Dataset): #FROM https://github.com/GMvandeVen/continual-learning/blob/master/data.py\n",
    "    '''To sub-sample a dataset, taking only those samples with label in [sub_labels].\n",
    "    After this selection of samples has been made, it is possible to transform the target-labels,\n",
    "    which can be useful when doing continual learning with fixed number of output units.'''\n",
    "\n",
    "    def __init__(self, original_dataset, sub_labels, target_transform=None):\n",
    "        super().__init__()\n",
    "        self.dataset = original_dataset\n",
    "        self.sub_indeces = []\n",
    "        for index in range(len(self.dataset)):\n",
    "            if hasattr(original_dataset, \"train_labels\"):\n",
    "                if self.dataset.target_transform is None:\n",
    "                    label = self.dataset.train_labels[index]\n",
    "                else:\n",
    "                    label = self.dataset.target_transform(self.dataset.train_labels[index])\n",
    "            elif hasattr(self.dataset, \"test_labels\"):\n",
    "                if self.dataset.target_transform is None:\n",
    "                    label = self.dataset.test_labels[index]\n",
    "                else:\n",
    "                    label = self.dataset.target_transform(self.dataset.test_labels[index])\n",
    "            else:\n",
    "                label = self.dataset[index][1]\n",
    "            if label in sub_labels:\n",
    "                self.sub_indeces.append(index)\n",
    "        self.target_transform = target_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ralle\\Anaconda3\\lib\\site-packages\\torchvision\\datasets\\mnist.py:43: UserWarning: train_labels has been renamed targets\n",
      "  warnings.warn(\"train_labels has been renamed targets\")\n"
     ]
    }
   ],
   "source": [
    "n_tasks = 5 #one for each number\n",
    "classes_per_task = 2 #because n_task = 10\n",
    "\n",
    "\n",
    "# prepare permutation to shuffle label-ids (to create different class batches for each random seed)\n",
    "#permutation = np.random.permutation(list(range(10)))\n",
    "#--------WE ACTUALLY DONT DO THE ABOVE. we always want the tasks to be the same so no permutation.\n",
    "permutation = list(range(10))\n",
    "#print(\"random permutation of labels\",permutation)\n",
    "#Lambda transform is a user defined transform.\n",
    "#-------nothhing really happens here\n",
    "target_transform = transforms.Lambda(lambda y, x=permutation: int(permutation[y]))\n",
    "\n",
    "dataset_transform = transforms.Compose([transforms.ToTensor()])#A lambda transform to random permutation of pixels can be added here in the permutetMNIST\n",
    "\n",
    "#-------Here the entire MNIST dataset is loaded\n",
    "mnist_train = torchvision.datasets.FashionMNIST(\"/\", train=True,\n",
    "                            download=True, transform=dataset_transform, target_transform=target_transform)\n",
    "mnist_test = torchvision.datasets.FashionMNIST(\"/\", train=False,\n",
    "                            download=True, transform=dataset_transform, target_transform=target_transform)\n",
    "\n",
    "#---------- generate labels-per-task. \n",
    "labels_per_task = [list(np.array(range(classes_per_task)) + classes_per_task * task_id) for task_id in range(n_tasks)]\n",
    "#print(labels_per_task)# [[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]]\n",
    "\n",
    "#----------- split them up into sub-tasks\n",
    "#-------we make a list where all sub dataset get put in.\n",
    "#HERE IS HOW WE take out one task data and labels. this is used in the training loop:\n",
    "#xtrain_set = train_datasets[task_no].dataset.data[train_datasets[task_no].sub_indeces]\n",
    "#ytrain_set = train_datasets[task_no].dataset.targets[train_datasets[task_no].sub_indeces]\n",
    "\n",
    "train_datasets = []\n",
    "test_datasets = []\n",
    "for labels in labels_per_task:\n",
    "    #target_transform = transforms.Lambda(lambda y, x=labels[0]: y - x) if scenario=='domain' else None #We are Task-IL: task is given, is it 1 or not\n",
    "    target_transform = None\n",
    "    train_datasets.append(SubDataset(mnist_train, labels, target_transform=target_transform))\n",
    "    test_datasets.append(SubDataset(mnist_test, labels, target_transform=target_transform))\n",
    "    \n",
    "#train_datasets is a list of 5 tasks with the entire dataset in each.\n",
    "#Each task has the sub_indicies that is the indicies where the target value fits the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Gotta do some coresets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset FashionMNIST\n",
      "    Number of datapoints: 60000\n",
      "    Root location: /\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               ToTensor()\n",
      "           )\n",
      "Target transform: Lambda()\n",
      "12000\n",
      "tensor(3)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(3)\n",
      "tensor(3)\n",
      "tensor(2)\n",
      "tensor(3)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(3)\n",
      "tensor(3)\n",
      "tensor(3)\n",
      "tensor(3)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(3)\n",
      "tensor(3)\n",
      "tensor(2)\n",
      "tensor(3)\n",
      "tensor(3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x1000 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# this is just BS\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(train_datasets[1].dataset)\n",
    "print(len(train_datasets[1].sub_indeces))\n",
    "\n",
    "for i in range(20):\n",
    "    print(train_datasets[1].dataset.targets[train_datasets[1].sub_indeces[i]])\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(1,5):\n",
    "    for j in range(1,5):\n",
    "        plt.subplot(4,4,(j-1)*4+i)\n",
    "        plt.imshow(train_datasets[1].dataset.data[train_datasets[1].sub_indeces[i*j]])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_true(pred,y):\n",
    "    n = len(y)\n",
    "    prec=0\n",
    "    for i in range(n):\n",
    "        pred_index = torch.argmax(pred[i])\n",
    "        true_index = torch.argmax(y[i])\n",
    "        if  pred_index == true_index:\n",
    "            prec +=1\n",
    "    return prec\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (criterion): BCEWithLogitsLoss()\n",
      "  (hidden_w_mean): ParameterList(\n",
      "      (0): Parameter containing: [torch.FloatTensor of size 784x256]\n",
      "      (1): Parameter containing: [torch.FloatTensor of size 256x256]\n",
      "  )\n",
      "  (hidden_b_mean): ParameterList(\n",
      "      (0): Parameter containing: [torch.FloatTensor of size 256x1]\n",
      "      (1): Parameter containing: [torch.FloatTensor of size 256x1]\n",
      "  )\n",
      "  (out_layers_w_mean): ParameterList()\n",
      "  (out_layers_b_mean): ParameterList()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as tdist\n",
    "import copy\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# FOR THE SPLITMNSIST MULTIHEAD PROBLEM\n",
    "input_size = 28*28 #size of images\n",
    "hidden_size = 256 #hidden layers size\n",
    "n_hidden_layers = 2\n",
    "output_size = 2\n",
    "n_tasks = n_tasks # also the number of heads\n",
    "init_var = 0   #This is the log(var) we use when we initiate layers. 0 works well with the other hyper parameters.\n",
    "init_var_hid = 0\n",
    "init_var_pri = 0 #Priors are initiated with this log(var)\n",
    "n_samples = 10 #The number of samples we take from the posterior distribution to sample a new weight to use when training\n",
    "\n",
    "torch.manual_seed(30)\n",
    "np.random.seed(30)\n",
    "\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.criterion = torch.nn.BCEWithLogitsLoss()\n",
    "        #Empty parameters to hold layers when these are added\n",
    "        \n",
    "        #notice that posteriors is Parameter list, so they get updated and priors is just a list.\n",
    "        #i may fuck this up when i add layers or something, but as long as the priors are not updated by the optimizer we are good\n",
    "        self.hidden_w_mean = torch.nn.ParameterList([])\n",
    "        self.hidden_b_mean = torch.nn.ParameterList([])\n",
    "#         self.hidden_w_var = torch.nn.ParameterList([])\n",
    "#         self.hidden_b_var = torch.nn.ParameterList([])\n",
    "\n",
    "#         #And the priors\n",
    "#         self.pri_hidden_w_mean = []\n",
    "#         self.pri_hidden_b_mean = []\n",
    "#         self.pri_hidden_w_var = []\n",
    "#         self.pri_hidden_b_var = []\n",
    "\n",
    "        #initiating hidden layers. weights are initiatet with N(0,0.1)\n",
    "        for i in range(n_hidden_layers):\n",
    "            if i == 0:\n",
    "                inp_size = input_size\n",
    "            else:\n",
    "                inp_size = hidden_size\n",
    "            self.hidden_w_mean.append(torch.nn.Parameter(torch.randn(inp_size,hidden_size)*0.1, requires_grad=True))\n",
    "            self.hidden_b_mean.append(torch.nn.Parameter(torch.randn(hidden_size,1)*0.1, requires_grad=True))\n",
    "#             self.hidden_w_var.append(torch.nn.Parameter(torch.ones(inp_size,hidden_size)*init_var_hid, requires_grad=True))\n",
    "#             self.hidden_b_var.append(torch.nn.Parameter(torch.ones(hidden_size,1)*init_var_hid, requires_grad=True))\n",
    "\n",
    "#             #And the priors\n",
    "#             self.pri_hidden_w_mean.append(torch.zeros(inp_size,hidden_size))\n",
    "#             self.pri_hidden_b_mean.append(torch.zeros(hidden_size,1))\n",
    "#             self.pri_hidden_w_var.append(torch.ones(inp_size,hidden_size)*init_var_pri)\n",
    "#             self.pri_hidden_b_var.append(torch.ones(hidden_size,1)*init_var_pri)\n",
    "\n",
    "       #Empty parameters to hold output layers when these are added in the add_task function. again \n",
    "        #priors will just be a list(or maybe not when we addtask() but it does not matter as long as they are not updated by optimizer)\n",
    "        self.out_layers_w_mean = torch.nn.ParameterList([])\n",
    "        self.out_layers_b_mean = torch.nn.ParameterList([])\n",
    "#         self.out_layers_w_var = torch.nn.ParameterList([])\n",
    "#         self.out_layers_b_var = torch.nn.ParameterList([])\n",
    "        \n",
    "#         self.pri_out_layers_w_mean = None\n",
    "#         self.pri_out_layers_b_mean = None\n",
    "#         self.pri_out_layers_w_var = None\n",
    "#         self.pri_out_layers_b_var = None\n",
    "        \n",
    "           \n",
    "    def forward(self, x, y, task_no, train=True):#y must be 1 of k encoded\n",
    "        #sample from posterior using the reparameterization trick\n",
    "        #from hidden layer\n",
    "        batch_size = x.shape[0] \n",
    "        \n",
    "        x = x.view(-1,input_size) #make it a vector\n",
    "        x = x/256\n",
    "        \n",
    "        for i in range(n_hidden_layers):\n",
    "            if train == False:   #if we are validating the network we just use the current mean of the weights. no need to sample\n",
    "                hidden_w = self.hidden_w_mean[i]\n",
    "                hidden_b = self.hidden_b_mean[i]\n",
    "#             else:\n",
    "#                 with torch.no_grad(): #sample from N(0,1). nograd because of reparameterization trick. SEE TAHT WE take n_samples!\n",
    "#                     inp_size,out_size = self.hidden_w_mean[i].size()\n",
    "#                     hidden_w = torch.nn.Parameter(torch.randn(n_samples,inp_size,out_size))\n",
    "#                     hidden_b = torch.nn.Parameter(torch.randn(n_samples,out_size,1))\n",
    "                \n",
    "#                 #WE MEAN THE SAMPLES AND generate weights from them\n",
    "#                 hidden_w = torch.add(self.hidden_w_mean[i],torch.mul(torch.mean(hidden_w,dim=0),torch.exp(0.5*self.hidden_w_var[i])))\n",
    "#                 hidden_b = torch.add(self.hidden_b_mean[i],torch.mul(torch.mean(hidden_b,dim=0),torch.exp(0.5*self.hidden_b_var[i])))\n",
    "#             #then we put data through the layer. ReLU is shit when network is randomly initiatet, but sigmoid works well.(paper uses relu)\n",
    "            x = F.relu(torch.tensordot(torch.transpose(hidden_w,0,1),x,([1],[1]))+hidden_b)\n",
    "            x = torch.transpose(x,0,1)\n",
    "        \n",
    "        # WE DO the same thing as above with the output layer.\n",
    "        #here we chose the layer that maches the task_no\n",
    "        if train == False:\n",
    "            out_w = self.out_layers_w_mean[task_no]\n",
    "            out_b = self.out_layers_b_mean[task_no]\n",
    "#         else:        \n",
    "#             with torch.no_grad():\n",
    "#                 inp_size,out_size = self.out_layers_w_mean[task_no].size()\n",
    "#                 out_w = torch.nn.Parameter(torch.randn(n_samples,inp_size,out_size))\n",
    "#                 out_b = torch.nn.Parameter(torch.randn(n_samples,out_size,1))\n",
    "#             out_w = torch.add(self.out_layers_w_mean[task_no],torch.mul(torch.mean(out_w,dim=0),torch.exp(0.5*self.out_layers_w_var[task_no])))\n",
    "#             out_b = torch.add(self.out_layers_b_mean[task_no],torch.mul(torch.mean(out_b,dim=0),torch.exp(0.5*self.out_layers_b_var[task_no])))\n",
    "           \n",
    "        #predict\n",
    "        #print(\"x shape input\",x.shape)\n",
    "        \n",
    "        #print(\"x shape input after view\",x.shape)\n",
    "        #print(\"x shape after unsqueeze\",x.shape)\n",
    "        #print(\"x type\",x.type())\n",
    "        \n",
    "        #print(hidden_w.shape,x.shape,hidden_b.shape)\n",
    "        \n",
    "        x = torch.tensordot(torch.transpose(out_w,0,1),x,([1],[1]))+out_b\n",
    "        \n",
    "        #print(x.shape) # [2xbatch]\n",
    "        #x = torch.transpose(x,0,1)\n",
    "        #print(\"before softmax: \",x)\n",
    "        pred = x\n",
    "        #print(\"after softmax: \",pred)\n",
    "        pred = torch.transpose(pred, 0, 1)\n",
    "        #print(\"after softmax: \",pred)\n",
    "        #Eq*log(likelyhood)-Eq*KL(estimated_prior/prior)\n",
    "        \n",
    "        \n",
    "        # SEE THE loss function\n",
    "        loss, likelihood, kl = self.calc_loss(pred,y,task_no,batch_size)\n",
    "        #print(likelihood, kl)\n",
    "        \n",
    "        return pred, loss, kl\n",
    "            \n",
    "    def calc_loss(self,y,t,task_no,batch_size):#FROM EX 7.2:\n",
    "        # \n",
    "        likelihood = self.criterion(y, t)\n",
    "        \n",
    "        kl = torch.tensor(0.)\n",
    "        \n",
    "        #For the KL loss i use the formula for two multivariate gaussians(but ours is just a single gaussian for each weight)\n",
    "        #for i in range(n_hidden_layers):\n",
    "        #    kl += torch.sum(0.5*(torch.exp(self.hidden_w_var[i]-self.pri_hidden_w_var[i]) + (self.pri_hidden_w_mean[i]-self.hidden_w_mean[i])**2/(torch.exp(self.pri_hidden_w_var[i]))-1+self.pri_hidden_w_var[i]-self.hidden_w_var[i]))\n",
    "        #    kl += torch.sum(0.5*(torch.exp(self.hidden_b_var[i]-self.pri_hidden_b_var[i]) + (self.pri_hidden_b_mean[i]-self.hidden_b_mean[i])**2/(torch.exp(self.pri_hidden_b_var[i]))-1+self.pri_hidden_b_var[i]-self.hidden_b_var[i]))\n",
    "        \n",
    "        ##also for output layers\n",
    "        #kl += torch.sum(0.5*(torch.exp(self.out_layers_w_var[task_no]-self.pri_out_layers_w_var[task_no]) + (self.pri_out_layers_w_mean[task_no]-self.out_layers_w_mean[task_no])**2/(torch.exp(self.pri_out_layers_w_var[task_no]))-1+self.pri_out_layers_w_var[task_no]-self.out_layers_w_var[task_no]))\n",
    "        #kl += torch.sum(0.5*(torch.exp(self.out_layers_b_var[task_no]-self.pri_out_layers_b_var[task_no]) + (self.pri_out_layers_b_mean[task_no]-self.out_layers_b_mean[task_no])**2/(torch.exp(self.pri_out_layers_b_var[task_no]))-1+self.pri_out_layers_b_var[task_no]-self.out_layers_b_var[task_no]))\n",
    "        #print(\"kl\",kl)\n",
    "        \n",
    "        ################THEIR KL ALGO\n",
    "#         for i in range(n_hidden_layers):\n",
    "#             if i == 0:\n",
    "#                 inp_size = input_size\n",
    "#             else:\n",
    "#                 inp_size = hidden_size\n",
    "#             const_term = -0.5 * hidden_size * inp_size\n",
    "#             log_std_diff = 0.5 * torch.sum(self.pri_hidden_w_var[i] - self.hidden_w_var[i])\n",
    "#             mu_diff_term = 0.5 * torch.sum((torch.exp(self.hidden_w_var[i]) + (self.pri_hidden_w_mean[i] - self.hidden_w_mean[i])**2) / torch.exp(self.pri_hidden_w_var[i]))\n",
    "#             kl += const_term + log_std_diff + mu_diff_term\n",
    "            \n",
    "#             const_term = -0.5 * hidden_size\n",
    "#             log_std_diff = 0.5 * torch.sum(self.pri_hidden_b_var[i] - self.hidden_b_var[i])\n",
    "#             mu_diff_term = 0.5 * torch.sum((torch.exp(self.hidden_b_var[i]) + (self.pri_hidden_b_mean[i] - self.hidden_b_mean[i])**2) / torch.exp(self.pri_hidden_b_var[i]))\n",
    "#             kl += const_term + log_std_diff + mu_diff_term\n",
    "        \n",
    "        \n",
    "#         #for i in range(len(self.out_layers_b_mean)):\n",
    "#         const_term = -0.5 * hidden_size * output_size\n",
    "#         log_std_diff = 0.5 * torch.sum(self.pri_out_layers_w_var[task_no] - self.out_layers_w_var[task_no])\n",
    "#         mu_diff_term = 0.5 * torch.sum((torch.exp(self.out_layers_w_var[task_no]) + (self.pri_out_layers_w_mean[task_no] - self.out_layers_w_mean[task_no])**2) / torch.exp(self.pri_out_layers_w_var[task_no]))\n",
    "#         kl += const_term + log_std_diff + mu_diff_term\n",
    "        \n",
    "#         const_term = -0.5  * output_size\n",
    "#         log_std_diff = 0.5 * torch.sum(self.pri_out_layers_b_var[task_no] - self.out_layers_b_var[task_no])\n",
    "#         mu_diff_term = 0.5 * torch.sum((torch.exp(self.out_layers_b_var[task_no]) + (self.pri_out_layers_b_mean[task_no] - self.out_layers_b_mean[task_no])**2) / torch.exp(self.pri_out_layers_b_var[task_no]))\n",
    "#         kl += const_term + log_std_diff + mu_diff_term\n",
    "        \n",
    "#         kl = torch.div(kl,batch_size)\n",
    "        ############################\n",
    "\n",
    "        # Combining the two terms in the evidence lower bound objective (ELBO) \n",
    "        #I DONT KNOW IF THE KL needs some kind of scaling since we just added up for every weight in the network\n",
    "        ELBO = torch.add(likelihood, 0) ############################\n",
    "        \n",
    "        #print(\"elbo: \",likelihood, \"kl\", kl)\n",
    "        return ELBO, likelihood, kl\n",
    "    \n",
    "    def add_task(self):\n",
    "        \n",
    "        if len(self.out_layers_w_mean) is 0: #if we need to add the first output layer (initiates as the hidden layers)\n",
    "            self.out_layers_w_mean.append(torch.nn.Parameter(torch.randn(hidden_size,output_size)*0.1, requires_grad=True))\n",
    "            self.out_layers_b_mean.append(torch.nn.Parameter(torch.randn(output_size,1)*0.1, requires_grad=True))\n",
    "#             self.out_layers_w_var.append(torch.nn.Parameter(torch.ones(hidden_size,output_size)*init_var, requires_grad=True))\n",
    "#             self.out_layers_b_var.append(torch.nn.Parameter(torch.ones(output_size,1)*init_var, requires_grad=True))\n",
    "            \n",
    "#             self.pri_out_layers_w_mean = [torch.nn.Parameter(torch.zeros(hidden_size,output_size), requires_grad=False)]\n",
    "#             self.pri_out_layers_b_mean = [torch.nn.Parameter(torch.zeros(output_size,1), requires_grad=False)]\n",
    "#             self.pri_out_layers_w_var = [torch.nn.Parameter(torch.ones(hidden_size,output_size)*init_var_pri, requires_grad=False)]\n",
    "#             self.pri_out_layers_b_var = [torch.nn.Parameter(torch.ones(output_size,1)*init_var_pri, requires_grad=False)]\n",
    "            \n",
    "        else:#if it is not the first we initiate the means as the means for the prev layer, but the variance as we defined so we allow it to change more\n",
    "            self.out_layers_w_mean.append(torch.nn.Parameter(torch.randn(hidden_size,output_size)*0.1, requires_grad=True))\n",
    "            self.out_layers_b_mean.append(torch.nn.Parameter(torch.randn(output_size,1)*0.1, requires_grad=True))\n",
    "#             self.out_layers_w_var.append(torch.nn.Parameter(torch.ones(hidden_size,output_size)*init_var, requires_grad=True))#initialize new head as the same as previous\n",
    "#             self.out_layers_b_var.append(torch.nn.Parameter(torch.ones(output_size,1)*init_var, requires_grad=True))#initialize new head as the same as previous\n",
    "            \n",
    "            #self.out_layers_w_mean.append(torch.nn.Parameter(self.out_layers_w_mean[-1].clone().detach()))#initialize new head as the same as previous\n",
    "            #self.out_layers_b_mean.append(torch.nn.Parameter(self.out_layers_b_mean[-1].clone().detach()))#initialize new head as the same as previous\n",
    "            #self.out_layers_w_var.append(torch.nn.Parameter(self.out_layers_w_var[-1].clone().detach()))#initialize new head as the same as previous\n",
    "            #self.out_layers_b_var.append(torch.nn.Parameter(self.out_layers_b_var[-1].clone().detach()))#initialize new head as the same as previous\n",
    "            \n",
    "            #self.out_layers_w_var.append(torch.nn.Parameter(torch.ones(hidden_size,output_size)*init_var, requires_grad=True))\n",
    "            #self.out_layers_b_var.append(torch.nn.Parameter(torch.ones(output_size,1)*init_var, requires_grad=True))\n",
    "            \n",
    "            #AND THE priors as always:\n",
    "#             self.pri_out_layers_w_mean.append(torch.nn.Parameter(torch.zeros(hidden_size,output_size), requires_grad=False))\n",
    "#             self.pri_out_layers_b_mean.append(torch.nn.Parameter(torch.zeros(output_size,1), requires_grad=False))\n",
    "#             self.pri_out_layers_w_var.append(torch.nn.Parameter(torch.ones(hidden_size,output_size)*init_var_pri, requires_grad=False))\n",
    "#             self.pri_out_layers_b_var.append(torch.nn.Parameter(torch.ones(output_size,1)*init_var_pri, requires_grad=False))\n",
    "            \n",
    "            #self.pri_out_layers_w_mean.append(self.pri_out_layers_w_mean[-1])\n",
    "            #self.pri_out_layers_b_mean.append(self.pri_out_layers_b_mean[-1])\n",
    "            #self.pri_out_layers_w_var.append(self.pri_out_layers_w_var[-1])\n",
    "            #self.pri_out_layers_b_var.append(self.pri_out_layers_b_var[-1])\n",
    "            \n",
    "#     def update_prior(self,task_no):#call before backward and after forward so we have used the old ones to get loss but copy the old posterior before it is updated in backward\n",
    "#         for i in range(n_hidden_layers):# just copies the current mean and var into the priors in a weird way to make shure they requires grad =False\n",
    "#             self.pri_hidden_w_mean[i] = self.hidden_w_mean[i].clone().detach()\n",
    "#             self.pri_hidden_b_mean[i] = self.hidden_b_mean[i].clone().detach()\n",
    "#             self.pri_hidden_w_var[i] = self.hidden_w_var[i].clone().detach()\n",
    "#             self.pri_hidden_b_var[i] = self.hidden_b_var[i].clone().detach()\n",
    "\n",
    "#             self.pri_hidden_w_mean[i].requires_grad = False\n",
    "#             self.pri_hidden_b_mean[i].requires_grad = False\n",
    "#             self.pri_hidden_w_var[i].requires_grad = False\n",
    "#             self.pri_hidden_b_var[i].requires_grad = False\n",
    "\n",
    "#         self.pri_out_layers_w_mean[task_no] = self.out_layers_w_mean[task_no].clone().detach()\n",
    "#         self.pri_out_layers_b_mean[task_no] = self.out_layers_b_mean[task_no].clone().detach()\n",
    "#         self.pri_out_layers_w_var[task_no] = self.out_layers_w_var[task_no].clone().detach()\n",
    "#         self.pri_out_layers_b_var[task_no] = self.out_layers_b_var[task_no].clone().detach()\n",
    "            \n",
    "#         self.pri_out_layers_w_mean[task_no].requires_grad = False\n",
    "#         self.pri_out_layers_b_mean[task_no].requires_grad = False\n",
    "#         self.pri_out_layers_w_var[task_no].requires_grad = False\n",
    "#         self.pri_out_layers_b_var[task_no].requires_grad = False\n",
    "    \n",
    "#     def reset_var_and_priors(self): #This only resets the first output layer\n",
    "#         print(init_var_hid,init_var,init_var_pri)\n",
    "#         for i in range(n_hidden_layers):\n",
    "#             if i == 0:\n",
    "#                 inp_size = input_size\n",
    "#             else:\n",
    "#                 inp_size = hidden_size\n",
    "#             self.hidden_w_var[i] = torch.nn.Parameter(torch.ones(inp_size,hidden_size)*init_var_hid, requires_grad=True)\n",
    "#             self.hidden_b_var[i] = torch.nn.Parameter(torch.ones(hidden_size,1)*init_var_hid, requires_grad=True)\n",
    "            \n",
    "#             self.pri_hidden_w_mean[i] = self.hidden_w_mean[i].clone().detach()\n",
    "#             self.pri_hidden_b_mean[i]= self.hidden_b_mean[i].clone().detach()\n",
    "#             self.pri_hidden_w_mean[i].requires_grad = False\n",
    "#             self.pri_hidden_b_mean[i].requires_grad = False\n",
    "            \n",
    "#             self.pri_hidden_w_var[i] = torch.ones(inp_size,hidden_size)*init_var_pri\n",
    "#             self.pri_hidden_b_var[i] = torch.ones(hidden_size,1)*init_var_pri\n",
    "\n",
    "            \n",
    "        \n",
    "#         self.out_layers_w_var[0] = torch.nn.Parameter(torch.ones(hidden_size,output_size)*init_var, requires_grad=True)\n",
    "#         self.out_layers_b_var[0] = torch.nn.Parameter(torch.ones(output_size,1)*init_var, requires_grad=True)\n",
    "        \n",
    "#         self.pri_out_layers_w_mean[0] = self.out_layers_w_mean[0].clone().detach()\n",
    "#         self.pri_out_layers_b_mean[0] = self.out_layers_b_mean[0].clone().detach()\n",
    "#         self.pri_out_layers_w_mean[0].requires_grad = False\n",
    "#         self.pri_out_layers_b_mean[0].requires_grad = False\n",
    "        \n",
    "#         self.pri_out_layers_w_var[0] = torch.nn.Parameter(torch.ones(hidden_size,output_size)*init_var_pri, requires_grad=False)\n",
    "#         self.pri_out_layers_b_var[0] = torch.nn.Parameter(torch.ones(output_size,1)*init_var_pri, requires_grad=False)\n",
    "            \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#when resetting i set prior mean to current mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task:  4 \tloss:  tensor(0.0004)\n",
      "ex out m:  tensor([-0.0098]) \tex hidmean:  tensor([-0.0566])\n",
      "validation prec:  0.8545\n",
      "kl:  tensor(0.)\n",
      "8545 10000\n"
     ]
    }
   ],
   "source": [
    "#THE TRAINING AND VALIDATION PART\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "\n",
    "no_epochs = 120\n",
    "batch_size = 60000\n",
    "lr = 0.001\n",
    "\n",
    "xtest_set = [test_datasets[0].dataset.data[test_datasets[0].sub_indeces]]\n",
    "xtrain_set = torch.tensor([])\n",
    "ytest_set = [test_datasets[0].dataset.targets[test_datasets[0].sub_indeces]]\n",
    "ytrain_set = torch.tensor([])\n",
    "\n",
    "batch_loss_list = []\n",
    "\n",
    "#########PRE TRAINING\n",
    "print(\"pretraining\")\n",
    "pre_batch_size = 64\n",
    "net.add_task()\n",
    "optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "xtrain_set = train_datasets[0].dataset.data[train_datasets[0].sub_indeces]\n",
    "ytrain_set = train_datasets[0].dataset.targets[train_datasets[0].sub_indeces]\n",
    "    \n",
    "for epoch in range(1):\n",
    "    print(\"pretraing epoch\", epoch)\n",
    "    num_samples = len(xtrain_set)\n",
    "    num_batches = int(np.ceil(num_samples / float(pre_batch_size)))\n",
    "    for batch in range(num_batches):\n",
    "        net.train()\n",
    "        #get indexes for current batch\n",
    "        idx = range(batch*pre_batch_size, np.minimum((batch+1)*pre_batch_size, num_samples))#indexes to use from batch\n",
    "        X_batch_tr = xtrain_set[idx]\n",
    "        y_batch_tr_raw = ytrain_set[idx]\n",
    "        #TRANSFORM Y:\n",
    "        y_batch_tr = torch.zeros((len(idx),2))\n",
    "        for i in range(len(y_batch_tr_raw)):\n",
    "            if y_batch_tr_raw[i] in [0,2,4,6,8]:\n",
    "                y_batch_tr[i,0] = 1\n",
    "            else: #i used this when i had 2 output neurons\n",
    "                y_batch_tr[i,1] = 1\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output, batch_loss, _ = net(X_batch_tr.type(torch.FloatTensor),y_batch_tr.type(torch.FloatTensor),0,train=False)#task no so we choose correct head. calls forward function.\n",
    "        batch_loss.backward() #Update weights and biases to new posterior\n",
    "        optimizer.step()            \n",
    "#DONE PRE TRAINING\n",
    "\n",
    "init_var = -6   #This is the log(var) we use when we initiate layers. 0 works well with the other hyper parameters.\n",
    "init_var_hid = -6\n",
    "init_var_pri = 0\n",
    "\n",
    "\n",
    "#net.reset_var_and_priors()\n",
    "\n",
    "\n",
    "\n",
    "#for plotting\n",
    "test_vali = []\n",
    "test_iter = []\n",
    "test_i = 0\n",
    "test_vali_seperate = [[],[],[],[],[]]\n",
    "\n",
    "for task_no in range(n_tasks):\n",
    "    print(\"TASK NUM: \",task_no)\n",
    "    #Get the train and test \n",
    "    xtrain_set = train_datasets[task_no].dataset.data[train_datasets[task_no].sub_indeces]\n",
    "    ytrain_set = train_datasets[task_no].dataset.targets[train_datasets[task_no].sub_indeces]\n",
    "    \n",
    "    \n",
    "    if task_no != 0: #Also whe a new taks comes we append this to the test set\n",
    "        xtest_set.append(test_datasets[task_no].dataset.data[test_datasets[task_no].sub_indeces])\n",
    "        ytest_set.append(test_datasets[task_no].dataset.targets[test_datasets[task_no].sub_indeces])\n",
    "        \n",
    "        net.add_task()#initialize a new head for the network\n",
    "\n",
    "    #TRAIN\n",
    "    optimizer = optim.Adam(net.parameters(), lr=lr) #REDEFINE OPTIMIZER AFTER LAYERS ARE ADDED!\n",
    "\n",
    "    for epoch in range(no_epochs):\n",
    "        net.train()\n",
    "        num_samples = len(xtrain_set)\n",
    "        num_batches = int(np.ceil(num_samples / float(batch_size)))\n",
    "        print(\"num batches: \",num_batches)\n",
    "        \n",
    "        indexes = list(range(num_samples))\n",
    "        np.random.shuffle(list(indexes))\n",
    "        \n",
    "        for batch in range(num_batches):\n",
    "            net.train()\n",
    "            #get indexes for current batch\n",
    "            idx = indexes[slice(batch*batch_size, np.minimum((batch+1)*batch_size, num_samples))]#indexes to use from batch\n",
    "            X_batch_tr = xtrain_set[idx]\n",
    "            y_batch_tr_raw = ytrain_set[idx]\n",
    "            #TRANSFORM Y:\n",
    "            y_batch_tr = torch.zeros((len(idx),2))\n",
    "            for i in range(len(y_batch_tr_raw)):\n",
    "                if y_batch_tr_raw[i] in [0,2,4,6,8]:\n",
    "                    y_batch_tr[i,0] = 1\n",
    "                else: #i used this when i had 2 output neurons\n",
    "                    y_batch_tr[i,1] = 1\n",
    "  \n",
    "            optimizer.zero_grad()\n",
    "            net.zero_grad()\n",
    "            output, batch_loss, kl= net(X_batch_tr.type(torch.FloatTensor),y_batch_tr.type(torch.FloatTensor),task_no, train=False)#task no so we choose correct head. calls forward function.\n",
    "            batch_loss.backward(retain_graph = True) #Update weights and biases to new posterior\n",
    "            optimizer.step()            \n",
    "            \n",
    "            if(batch%100 == 0):#THis is just for evaluation and plotting\n",
    "                #print(net)\n",
    "                net.eval()\n",
    "                vali_accu = 0\n",
    "                batch_loss_list.append(batch_loss.data)\n",
    "                n_test_samples = 0\n",
    "                for i in range(len(xtest_set)):\n",
    "                    task_test_no = i\n",
    "                    \n",
    "                    ytest_set_onehot = torch.zeros((len(ytest_set[i]),2))\n",
    "                    for j in range(len(ytest_set[i])):\n",
    "                        if ytest_set[i][j] in [0,2,4,6,8]:\n",
    "                            ytest_set_onehot[j,0] = 1\n",
    "                        else:\n",
    "                            ytest_set_onehot[j,1] = 1       \n",
    "                    output, _, _ = net(xtest_set[task_test_no].type(torch.FloatTensor),ytest_set_onehot.type(torch.FloatTensor),task_test_no, False)#task no so we choose correct head. calls forward function.\n",
    "                    vali_accu += get_n_true(output,ytest_set_onehot)\n",
    "                    n_test_samples += len(ytest_set_onehot)\n",
    "                    \n",
    "                #for plotting\n",
    "                clear_output(wait=True)\n",
    "                test_vali.append(vali_accu/n_test_samples)#i is task_no\n",
    "                test_iter.append(test_i)\n",
    "                test_i += 1\n",
    "                                \n",
    "                fig = plt.figure(figsize=(12,4))\n",
    "                plt.plot(test_iter,test_vali, label='valid_accu')\n",
    "                #plt.plot(test_iter,batch_loss_list)\n",
    "                plt.legend()\n",
    "                plt.show()\n",
    "                    \n",
    "                print(\"task: \",task_no,\"\\tloss: \",batch_loss.data)\n",
    "                #print(\"ex outvar: \", net.out_layers_b_var[task_no][0].data, \"\\tex hidvar: \", net.hidden_b_var[0][0].data)\n",
    "                #print(\"ex pri outvar: \", net.pri_out_layers_b_var[task_no][0].data, \"\\tex pri hidvar: \", net.pri_hidden_b_var[0][0].data)\n",
    "                print(\"ex out m: \", net.out_layers_b_mean[task_no][0].data, \"\\tex hidmean: \", net.hidden_b_mean[0][0].data)\n",
    "                #print(\"ex pri out m: \", net.pri_out_layers_b_mean[task_no][0].data, \"\\tex pri hidmean: \", net.pri_hidden_b_mean[0][0].data)\n",
    "                print(\"validation prec: \",vali_accu/n_test_samples)\n",
    "                print(\"kl: \",kl)\n",
    "                print(vali_accu,n_test_samples)\n",
    "    \n",
    "    #net.update_prior(task_no)\n",
    "    \n",
    "    net.eval()\n",
    "    #THIS is used when we make the last plot of the end results for each taks\n",
    "    for i in range(task_no+1):\n",
    "        y_batch_test_raw = test_datasets[i].dataset.targets[test_datasets[i].sub_indeces]\n",
    "        #TRANSFORM Y TO one hot:\n",
    "        y_batch_test = torch.zeros((len(y_batch_test_raw),2))\n",
    "        for j in range(len(y_batch_test_raw)):\n",
    "            if y_batch_test_raw[j] in [0,2,4,6,8]:\n",
    "                y_batch_test[j,0] = 1\n",
    "            else:\n",
    "                y_batch_test[j,1] = 1\n",
    "        output, _, _ = net(test_datasets[i].dataset.data[test_datasets[i].sub_indeces].type(torch.FloatTensor), y_batch_test.type(torch.FloatTensor), i, False)\n",
    "        test_vali_seperate[i].append([task_no, get_n_true(output,y_batch_test)/len(y_batch_test)])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "for i in range(5):\n",
    "    plt.plot(np.transpose(test_vali_seperate[i])[0],np.transpose(test_vali_seperate[i])[1])\n",
    "    plt.plot(np.transpose(test_vali_seperate[i])[0],np.transpose(test_vali_seperate[i])[1],\"x\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('splitfashion_EM_4.pickle', 'wb') as f:\n",
    "    pickle.dump(test_vali_seperate, f)\n",
    "\n",
    "\n",
    "with open('splitfashion_EM_4.pickle', 'rb') as f:\n",
    "   test_vali_seperate_1 = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "for i in range(5):\n",
    "    plt.plot(np.transpose(test_vali_seperate_1[i])[0],np.transpose(test_vali_seperate_1[i])[1])\n",
    "    plt.plot(np.transpose(test_vali_seperate_1[i])[0],np.transpose(test_vali_seperate_1[i])[1],\"x\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in net.named_parameters():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(net.named_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.log(torch.tensor(0.0024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(net.out_layers_b_var[0].requires_grad)\n",
    "print(net.pri_out_layers_b_var[0].requires_grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('hidden_w_mean.0', Parameter containing:\n",
      "tensor([[ 2.1353, -2.3715,  2.3291,  ..., -2.3068,  2.3694,  2.3030],\n",
      "        [ 2.3663, -2.2922,  2.3625,  ..., -2.3557, -2.3699, -2.3070],\n",
      "        [ 2.3397,  2.3608,  2.2949,  ..., -2.1888,  2.3516,  2.3501],\n",
      "        ...,\n",
      "        [ 2.3077, -2.3618, -2.2659,  ..., -2.3371, -2.3140,  2.2923],\n",
      "        [ 2.3638, -2.3181, -2.3490,  ...,  2.3543,  2.3608, -2.3579],\n",
      "        [-2.3075, -2.3575, -2.3455,  ...,  2.3712, -2.3460,  2.3696]],\n",
      "       requires_grad=True))\n",
      "('hidden_w_mean.1', Parameter containing:\n",
      "tensor([[-0.2620,  0.3067, -0.0851,  ..., -0.1688,  0.1854,  0.1521],\n",
      "        [ 1.0431,  2.3428,  0.9990,  ...,  2.3702,  2.2967,  2.1707],\n",
      "        [ 0.1489,  0.1878, -0.1783,  ...,  0.3391,  0.3604,  0.5605],\n",
      "        ...,\n",
      "        [ 2.0066,  2.3320, -0.0949,  ..., -2.3012, -2.3646, -2.3490],\n",
      "        [ 1.0625,  0.3373, -0.0715,  ..., -0.3154,  1.1491,  0.8978],\n",
      "        [ 1.0238,  0.9340, -0.1178,  ..., -1.0098, -1.0133, -0.9456]],\n",
      "       requires_grad=True))\n",
      "('hidden_b_mean.0', Parameter containing:\n",
      "tensor([[ 7.3184e-02],\n",
      "        [ 2.3095e+00],\n",
      "        [ 2.3928e-01],\n",
      "        [-6.4068e-01],\n",
      "        [-1.7990e+00],\n",
      "        [-1.4562e-01],\n",
      "        [-1.9122e+00],\n",
      "        [-2.1879e+00],\n",
      "        [-3.2464e-01],\n",
      "        [ 2.1377e+00],\n",
      "        [-1.0339e+00],\n",
      "        [-2.1257e+00],\n",
      "        [ 1.5887e-01],\n",
      "        [-2.2501e+00],\n",
      "        [ 1.8045e+00],\n",
      "        [-1.4799e-01],\n",
      "        [-2.0567e-01],\n",
      "        [ 1.8289e+00],\n",
      "        [ 3.6023e-01],\n",
      "        [-1.6240e+00],\n",
      "        [ 4.3147e-01],\n",
      "        [-4.6853e-01],\n",
      "        [ 1.6872e+00],\n",
      "        [ 1.2820e+00],\n",
      "        [-1.9373e+00],\n",
      "        [-4.3383e-02],\n",
      "        [-9.8448e-01],\n",
      "        [ 1.9939e+00],\n",
      "        [ 2.0662e+00],\n",
      "        [-1.8107e-01],\n",
      "        [-1.5909e-01],\n",
      "        [ 1.9802e-01],\n",
      "        [ 1.0824e+00],\n",
      "        [-2.9198e-01],\n",
      "        [ 3.6427e-01],\n",
      "        [ 1.4874e+00],\n",
      "        [ 6.5592e-01],\n",
      "        [ 2.2273e+00],\n",
      "        [ 1.6841e-01],\n",
      "        [-1.2515e+00],\n",
      "        [ 2.1709e+00],\n",
      "        [ 1.5919e+00],\n",
      "        [-1.5683e+00],\n",
      "        [ 2.0946e+00],\n",
      "        [-1.4337e-01],\n",
      "        [-4.7994e-01],\n",
      "        [ 2.1111e+00],\n",
      "        [-2.3243e+00],\n",
      "        [-1.6241e+00],\n",
      "        [ 1.4818e+00],\n",
      "        [-1.1841e+00],\n",
      "        [-2.3649e+00],\n",
      "        [ 8.9621e-01],\n",
      "        [ 2.2648e+00],\n",
      "        [-1.0091e+00],\n",
      "        [-2.2968e-02],\n",
      "        [-1.9315e+00],\n",
      "        [ 6.4050e-01],\n",
      "        [-1.0873e+00],\n",
      "        [-3.4036e-01],\n",
      "        [ 2.1877e-01],\n",
      "        [ 1.6753e+00],\n",
      "        [ 1.0929e-01],\n",
      "        [ 7.7575e-01],\n",
      "        [ 1.9257e+00],\n",
      "        [-1.9215e+00],\n",
      "        [-1.5054e+00],\n",
      "        [-2.1113e+00],\n",
      "        [ 6.9371e-01],\n",
      "        [ 8.2430e-01],\n",
      "        [ 2.0484e+00],\n",
      "        [-2.0822e+00],\n",
      "        [ 2.2434e+00],\n",
      "        [ 3.8886e-01],\n",
      "        [ 1.8353e+00],\n",
      "        [ 2.3222e+00],\n",
      "        [-9.8905e-01],\n",
      "        [ 1.9898e-01],\n",
      "        [ 1.7899e+00],\n",
      "        [-2.2631e+00],\n",
      "        [-2.9900e-01],\n",
      "        [ 1.3400e+00],\n",
      "        [-1.4398e+00],\n",
      "        [-4.5716e-01],\n",
      "        [-1.8784e+00],\n",
      "        [ 1.5705e+00],\n",
      "        [ 1.4737e+00],\n",
      "        [ 2.0819e+00],\n",
      "        [ 1.5742e+00],\n",
      "        [ 1.7253e+00],\n",
      "        [-4.8422e-04],\n",
      "        [ 2.2888e+00],\n",
      "        [ 1.7451e+00],\n",
      "        [-2.3041e+00],\n",
      "        [ 1.9469e+00],\n",
      "        [ 1.5928e-02],\n",
      "        [-1.6836e+00],\n",
      "        [ 3.2561e-02],\n",
      "        [-2.4873e-01],\n",
      "        [-9.2043e-02],\n",
      "        [ 1.0977e+00],\n",
      "        [-5.3692e-01],\n",
      "        [ 2.2932e+00],\n",
      "        [ 2.2232e+00],\n",
      "        [-8.9688e-01],\n",
      "        [-1.8646e+00],\n",
      "        [-1.7328e+00],\n",
      "        [-2.3480e+00],\n",
      "        [-2.3502e+00],\n",
      "        [-2.2532e+00],\n",
      "        [-5.4669e-02],\n",
      "        [ 2.3352e+00],\n",
      "        [-1.6414e+00],\n",
      "        [-2.3347e+00],\n",
      "        [-3.9569e-01],\n",
      "        [ 5.7118e-01],\n",
      "        [-2.1611e+00],\n",
      "        [ 3.3158e-01],\n",
      "        [-2.2303e+00],\n",
      "        [ 1.5624e+00],\n",
      "        [ 6.3964e-01],\n",
      "        [ 2.1244e+00],\n",
      "        [ 1.1079e+00],\n",
      "        [ 2.3239e+00],\n",
      "        [-2.0749e+00],\n",
      "        [-2.3692e+00],\n",
      "        [-6.4252e-01],\n",
      "        [-1.2143e-01],\n",
      "        [-3.6193e-01],\n",
      "        [-1.7455e+00],\n",
      "        [ 5.9657e-01],\n",
      "        [ 1.9306e+00],\n",
      "        [-1.2006e+00],\n",
      "        [-1.7680e+00],\n",
      "        [-4.0732e-01],\n",
      "        [ 1.6800e+00],\n",
      "        [ 1.9934e+00],\n",
      "        [ 1.8685e+00],\n",
      "        [-9.0322e-01],\n",
      "        [ 7.2160e-01],\n",
      "        [ 9.7510e-01],\n",
      "        [-1.6433e+00],\n",
      "        [ 1.6456e+00],\n",
      "        [ 2.2558e+00],\n",
      "        [-2.5056e-01],\n",
      "        [ 1.5971e+00],\n",
      "        [ 1.8624e+00],\n",
      "        [-1.3201e+00],\n",
      "        [-2.2802e+00],\n",
      "        [-9.4493e-01],\n",
      "        [ 2.2291e+00],\n",
      "        [ 2.0318e+00],\n",
      "        [-1.4662e+00],\n",
      "        [ 1.7874e+00],\n",
      "        [ 5.5809e-01],\n",
      "        [ 3.2492e-01],\n",
      "        [ 9.5593e-01],\n",
      "        [ 1.5520e+00],\n",
      "        [ 1.1289e+00],\n",
      "        [-1.9133e+00],\n",
      "        [-1.5007e+00],\n",
      "        [-5.1735e-01],\n",
      "        [-1.3164e+00],\n",
      "        [-1.1719e+00],\n",
      "        [-8.8193e-02],\n",
      "        [-2.7175e-01],\n",
      "        [ 8.1259e-01],\n",
      "        [ 1.5646e+00],\n",
      "        [-1.3514e+00],\n",
      "        [ 1.4241e+00],\n",
      "        [-2.1133e+00],\n",
      "        [ 4.6230e-01],\n",
      "        [ 8.1658e-01],\n",
      "        [ 1.9523e+00],\n",
      "        [ 3.3370e-01],\n",
      "        [-7.5962e-01],\n",
      "        [-6.6080e-01],\n",
      "        [ 1.0954e+00],\n",
      "        [ 5.7147e-01],\n",
      "        [ 2.2426e+00],\n",
      "        [ 2.0248e+00],\n",
      "        [-3.1514e-01],\n",
      "        [ 1.2594e+00],\n",
      "        [ 1.2579e+00],\n",
      "        [-1.4259e+00],\n",
      "        [-2.3306e+00],\n",
      "        [ 3.7031e-01],\n",
      "        [ 2.9298e-01],\n",
      "        [-6.0299e-01],\n",
      "        [ 4.9932e-03],\n",
      "        [ 1.6673e+00],\n",
      "        [ 1.3413e-01],\n",
      "        [ 1.8450e-01],\n",
      "        [-1.8123e+00],\n",
      "        [ 1.1755e+00],\n",
      "        [ 1.8019e+00],\n",
      "        [ 2.3171e+00],\n",
      "        [ 2.1262e+00],\n",
      "        [-1.4032e+00],\n",
      "        [ 9.0120e-03],\n",
      "        [-2.3621e+00],\n",
      "        [ 1.4306e+00],\n",
      "        [ 4.9074e-01],\n",
      "        [-2.1489e+00],\n",
      "        [-5.8219e-01],\n",
      "        [-1.9163e-01],\n",
      "        [-1.3347e+00],\n",
      "        [-1.9459e+00],\n",
      "        [ 1.8615e+00],\n",
      "        [ 2.2288e+00],\n",
      "        [-1.7391e+00],\n",
      "        [ 1.5195e+00],\n",
      "        [-2.2377e+00],\n",
      "        [-1.3816e+00],\n",
      "        [ 2.2324e+00],\n",
      "        [-1.6699e+00],\n",
      "        [ 1.6949e+00],\n",
      "        [ 1.4249e-01],\n",
      "        [ 2.2246e+00],\n",
      "        [-1.1422e+00],\n",
      "        [-7.6349e-02],\n",
      "        [-2.2832e+00],\n",
      "        [ 2.0721e+00],\n",
      "        [ 2.1739e+00],\n",
      "        [-5.3654e-01],\n",
      "        [ 1.3899e+00],\n",
      "        [-2.2327e+00],\n",
      "        [-2.2353e+00],\n",
      "        [-1.3775e+00],\n",
      "        [-2.0349e+00],\n",
      "        [-1.9381e-02],\n",
      "        [-2.2919e+00],\n",
      "        [-1.9251e+00],\n",
      "        [ 6.7566e-02],\n",
      "        [ 2.3295e+00],\n",
      "        [-5.6011e-01],\n",
      "        [-2.0987e+00],\n",
      "        [-4.4961e-02],\n",
      "        [ 2.1857e+00],\n",
      "        [-1.7167e+00],\n",
      "        [ 1.6171e+00],\n",
      "        [ 1.1269e+00],\n",
      "        [ 3.8000e-01],\n",
      "        [ 2.8544e-02],\n",
      "        [-1.7354e+00],\n",
      "        [-1.9645e-01],\n",
      "        [ 9.8288e-01],\n",
      "        [-1.9034e+00],\n",
      "        [ 1.1573e-01],\n",
      "        [-6.8918e-01],\n",
      "        [-1.2816e+00],\n",
      "        [ 1.0396e+00],\n",
      "        [-1.2297e+00],\n",
      "        [ 2.3051e+00],\n",
      "        [-1.7230e+00],\n",
      "        [ 9.5795e-01]], requires_grad=True))\n",
      "('hidden_b_mean.1', Parameter containing:\n",
      "tensor([[ 2.3284],\n",
      "        [-2.2878],\n",
      "        [ 2.3243],\n",
      "        [ 2.2949],\n",
      "        [ 2.2987],\n",
      "        [-2.3647],\n",
      "        [ 2.2774],\n",
      "        [-2.3026],\n",
      "        [-2.3524],\n",
      "        [-2.3574],\n",
      "        [-1.9553],\n",
      "        [ 2.2866],\n",
      "        [ 2.3241],\n",
      "        [ 2.2968],\n",
      "        [-2.3202],\n",
      "        [ 2.3536],\n",
      "        [-2.3653],\n",
      "        [-2.3072],\n",
      "        [ 2.2792],\n",
      "        [-2.3388],\n",
      "        [ 2.3295],\n",
      "        [ 2.3564],\n",
      "        [ 2.3659],\n",
      "        [ 2.2979],\n",
      "        [ 2.3445],\n",
      "        [-2.2872],\n",
      "        [ 2.3401],\n",
      "        [-2.3069],\n",
      "        [ 2.3704],\n",
      "        [ 2.3466],\n",
      "        [ 2.3628],\n",
      "        [-2.2957],\n",
      "        [-2.3082],\n",
      "        [-2.2802],\n",
      "        [-2.3298],\n",
      "        [ 2.3469],\n",
      "        [ 2.3365],\n",
      "        [-2.3269],\n",
      "        [-2.3694],\n",
      "        [-2.3457],\n",
      "        [ 2.2776],\n",
      "        [ 2.2621],\n",
      "        [-2.3373],\n",
      "        [-2.2931],\n",
      "        [-2.3598],\n",
      "        [-2.3182],\n",
      "        [ 2.3262],\n",
      "        [-2.3710],\n",
      "        [ 2.3650],\n",
      "        [-2.3577],\n",
      "        [-1.8887],\n",
      "        [-2.3508],\n",
      "        [ 2.3511],\n",
      "        [ 2.3332],\n",
      "        [ 2.3098],\n",
      "        [-2.3287],\n",
      "        [ 2.3653],\n",
      "        [ 2.3656],\n",
      "        [ 2.2948],\n",
      "        [-2.3042],\n",
      "        [-2.3792],\n",
      "        [ 2.3684],\n",
      "        [ 2.3656],\n",
      "        [ 2.3088],\n",
      "        [ 2.3282],\n",
      "        [ 2.3370],\n",
      "        [-2.3430],\n",
      "        [-2.3169],\n",
      "        [ 2.2968],\n",
      "        [ 2.3291],\n",
      "        [-2.3328],\n",
      "        [ 2.3664],\n",
      "        [-2.3319],\n",
      "        [ 2.3057],\n",
      "        [ 2.3442],\n",
      "        [-2.2979],\n",
      "        [-2.3433],\n",
      "        [-2.3009],\n",
      "        [ 2.3592],\n",
      "        [ 2.3472],\n",
      "        [-2.3461],\n",
      "        [ 2.3012],\n",
      "        [ 2.3420],\n",
      "        [-2.3672],\n",
      "        [-2.3662],\n",
      "        [ 2.3282],\n",
      "        [ 2.2819],\n",
      "        [ 2.3574],\n",
      "        [-2.3191],\n",
      "        [ 2.3138],\n",
      "        [ 2.3170],\n",
      "        [-2.3093],\n",
      "        [ 2.3199],\n",
      "        [-2.2916],\n",
      "        [-2.3305],\n",
      "        [-2.3099],\n",
      "        [-2.2948],\n",
      "        [-2.3504],\n",
      "        [-2.2695],\n",
      "        [-2.3451],\n",
      "        [ 2.3058],\n",
      "        [-2.3656],\n",
      "        [-2.2801],\n",
      "        [-2.3643],\n",
      "        [ 1.8750],\n",
      "        [-2.2265],\n",
      "        [ 2.2886],\n",
      "        [-2.3628],\n",
      "        [ 2.2371],\n",
      "        [-2.3357],\n",
      "        [-2.3137],\n",
      "        [-2.3420],\n",
      "        [-2.2875],\n",
      "        [-2.3271],\n",
      "        [-2.2890],\n",
      "        [-2.3204],\n",
      "        [ 2.3380],\n",
      "        [ 1.8184],\n",
      "        [-2.3558],\n",
      "        [-2.3475],\n",
      "        [ 2.3649],\n",
      "        [-2.3241],\n",
      "        [-2.3274],\n",
      "        [ 2.3552],\n",
      "        [-2.2963],\n",
      "        [ 2.3693],\n",
      "        [-2.3324],\n",
      "        [-2.3361],\n",
      "        [-2.3443],\n",
      "        [-2.3695],\n",
      "        [-2.3119],\n",
      "        [-2.3477],\n",
      "        [-2.3212],\n",
      "        [-2.3517],\n",
      "        [-2.2951],\n",
      "        [-2.3110],\n",
      "        [ 2.3643],\n",
      "        [-2.3010],\n",
      "        [-2.3569],\n",
      "        [ 2.3329],\n",
      "        [-2.3310],\n",
      "        [-1.7785],\n",
      "        [-2.3209],\n",
      "        [ 2.2924],\n",
      "        [ 2.3718],\n",
      "        [ 2.3123],\n",
      "        [-2.3400],\n",
      "        [-2.3383],\n",
      "        [ 2.3310],\n",
      "        [-2.3521],\n",
      "        [ 2.3703],\n",
      "        [-2.3221],\n",
      "        [ 2.3252],\n",
      "        [-2.1616],\n",
      "        [ 2.2350],\n",
      "        [-2.3053],\n",
      "        [ 2.3177],\n",
      "        [ 2.3522],\n",
      "        [-2.3342],\n",
      "        [-2.3005],\n",
      "        [-2.2968],\n",
      "        [ 2.3373],\n",
      "        [ 2.2686],\n",
      "        [ 2.3101],\n",
      "        [ 2.3499],\n",
      "        [-2.3389],\n",
      "        [-2.2929],\n",
      "        [ 2.3472],\n",
      "        [-2.3122],\n",
      "        [-2.3185],\n",
      "        [ 2.3090],\n",
      "        [ 2.3157],\n",
      "        [-2.2910],\n",
      "        [-2.3282],\n",
      "        [-2.2993],\n",
      "        [ 2.3680],\n",
      "        [-2.3551],\n",
      "        [ 2.3056],\n",
      "        [-2.3530],\n",
      "        [-2.3505],\n",
      "        [ 2.3072],\n",
      "        [ 2.3607],\n",
      "        [-2.2717],\n",
      "        [-2.3275],\n",
      "        [ 2.2858],\n",
      "        [-2.3661],\n",
      "        [-2.3214],\n",
      "        [ 2.3426],\n",
      "        [ 2.3242],\n",
      "        [-2.3657],\n",
      "        [-2.3042],\n",
      "        [ 2.3584],\n",
      "        [ 2.3047],\n",
      "        [-2.2786],\n",
      "        [-2.3598],\n",
      "        [-2.3631],\n",
      "        [ 2.3590],\n",
      "        [-2.3500],\n",
      "        [ 2.3580],\n",
      "        [-2.3660],\n",
      "        [ 2.3535],\n",
      "        [ 2.3250],\n",
      "        [-2.3521],\n",
      "        [ 2.2906],\n",
      "        [ 2.3656],\n",
      "        [ 2.3079],\n",
      "        [-2.3331],\n",
      "        [ 2.3408],\n",
      "        [-2.3617],\n",
      "        [-2.2947],\n",
      "        [-2.2921],\n",
      "        [ 2.3076],\n",
      "        [-2.3033],\n",
      "        [ 2.3467],\n",
      "        [-2.2843],\n",
      "        [ 2.3641],\n",
      "        [-2.3158],\n",
      "        [-2.3615],\n",
      "        [ 2.3284],\n",
      "        [-2.3645],\n",
      "        [ 2.3521],\n",
      "        [ 1.8501],\n",
      "        [ 2.3185],\n",
      "        [ 2.3118],\n",
      "        [-2.3156],\n",
      "        [-2.3615],\n",
      "        [ 2.3002],\n",
      "        [ 2.3662],\n",
      "        [-2.3595],\n",
      "        [-2.3262],\n",
      "        [-2.2898],\n",
      "        [-2.2942],\n",
      "        [-2.3319],\n",
      "        [-2.2954],\n",
      "        [-2.3178],\n",
      "        [-2.3587],\n",
      "        [-2.2946],\n",
      "        [ 2.3545],\n",
      "        [ 2.3711],\n",
      "        [ 2.3465],\n",
      "        [-2.3293],\n",
      "        [ 2.3500],\n",
      "        [ 2.3704],\n",
      "        [-2.3585],\n",
      "        [-2.3441],\n",
      "        [ 2.3054],\n",
      "        [ 2.2893],\n",
      "        [ 2.3242],\n",
      "        [-2.3352],\n",
      "        [ 2.3248],\n",
      "        [ 2.2629],\n",
      "        [ 2.3529],\n",
      "        [-2.3582],\n",
      "        [ 2.3660],\n",
      "        [-2.2939],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        [ 1.7611]], requires_grad=True))\n",
      "('hidden_w_var.0', Parameter containing:\n",
      "tensor([[-2.0060, -2.0060, -2.0060,  ..., -2.0060, -2.0060, -2.0060],\n",
      "        [-2.0060, -2.0060, -2.0060,  ..., -2.0060, -2.0060, -2.0060],\n",
      "        [-2.0060, -2.0060, -2.0060,  ..., -2.0060, -2.0060, -2.0060],\n",
      "        ...,\n",
      "        [-2.0060, -2.0060, -2.0060,  ..., -2.0060, -2.0060, -2.0060],\n",
      "        [-2.0060, -2.0060, -2.0060,  ..., -2.0060, -2.0060, -2.0060],\n",
      "        [-2.0060, -2.0060, -2.0060,  ..., -2.0060, -2.0060, -2.0060]],\n",
      "       requires_grad=True))\n",
      "('hidden_w_var.1', Parameter containing:\n",
      "tensor([[-2.0043, -1.9662, -1.9985,  ..., -2.0133, -2.0021, -2.0196],\n",
      "        [-1.9822, -2.0060, -2.0309,  ..., -2.0060, -2.0060, -2.0060],\n",
      "        [-1.9854, -2.0167, -1.9416,  ..., -2.0111, -2.0151, -1.9843],\n",
      "        ...,\n",
      "        [-2.0060, -2.0061, -2.0152,  ..., -2.0060, -2.0060, -2.0062],\n",
      "        [-2.0036, -2.0177, -1.9933,  ..., -2.0207, -1.9812, -2.0193],\n",
      "        [-1.9610, -2.0431, -1.9964,  ..., -2.0184, -1.9940, -2.0399]],\n",
      "       requires_grad=True))\n",
      "('hidden_b_var.0', Parameter containing:\n",
      "tensor([[-2.0101],\n",
      "        [-2.0063],\n",
      "        [-2.0104],\n",
      "        [-2.0159],\n",
      "        [-2.0087],\n",
      "        [-2.0054],\n",
      "        [-2.0009],\n",
      "        [-2.0058],\n",
      "        [-2.0135],\n",
      "        [-2.0087],\n",
      "        [-2.0133],\n",
      "        [-2.0069],\n",
      "        [-2.0025],\n",
      "        [-2.0082],\n",
      "        [-2.0030],\n",
      "        [-2.0068],\n",
      "        [-2.0014],\n",
      "        [-2.0048],\n",
      "        [-1.9935],\n",
      "        [-2.0063],\n",
      "        [-2.0007],\n",
      "        [-2.0073],\n",
      "        [-2.0175],\n",
      "        [-2.0107],\n",
      "        [-2.0053],\n",
      "        [-2.0074],\n",
      "        [-2.0087],\n",
      "        [-2.0094],\n",
      "        [-2.0048],\n",
      "        [-2.0108],\n",
      "        [-2.0042],\n",
      "        [-2.0062],\n",
      "        [-2.0028],\n",
      "        [-2.0072],\n",
      "        [-2.0056],\n",
      "        [-2.0032],\n",
      "        [-2.0057],\n",
      "        [-2.0073],\n",
      "        [-2.0074],\n",
      "        [-2.0134],\n",
      "        [-2.0010],\n",
      "        [-2.0171],\n",
      "        [-2.0095],\n",
      "        [-2.0109],\n",
      "        [-2.0014],\n",
      "        [-2.0016],\n",
      "        [-2.0065],\n",
      "        [-2.0059],\n",
      "        [-2.0039],\n",
      "        [-1.9952],\n",
      "        [-2.0125],\n",
      "        [-2.0060],\n",
      "        [-2.0204],\n",
      "        [-2.0062],\n",
      "        [-2.0043],\n",
      "        [-2.0124],\n",
      "        [-2.0060],\n",
      "        [-2.0061],\n",
      "        [-2.0111],\n",
      "        [-2.0034],\n",
      "        [-2.0058],\n",
      "        [-1.9966],\n",
      "        [-2.0069],\n",
      "        [-1.9872],\n",
      "        [-2.0090],\n",
      "        [-2.0001],\n",
      "        [-2.0036],\n",
      "        [-2.0069],\n",
      "        [-2.0111],\n",
      "        [-2.0021],\n",
      "        [-2.0064],\n",
      "        [-2.0044],\n",
      "        [-2.0053],\n",
      "        [-2.0086],\n",
      "        [-2.0017],\n",
      "        [-2.0079],\n",
      "        [-2.0048],\n",
      "        [-2.0113],\n",
      "        [-2.0017],\n",
      "        [-2.0064],\n",
      "        [-1.9941],\n",
      "        [-2.0079],\n",
      "        [-2.0107],\n",
      "        [-2.0097],\n",
      "        [-2.0072],\n",
      "        [-2.0078],\n",
      "        [-2.0086],\n",
      "        [-2.0065],\n",
      "        [-2.0031],\n",
      "        [-2.0183],\n",
      "        [-2.0065],\n",
      "        [-2.0055],\n",
      "        [-2.0117],\n",
      "        [-2.0094],\n",
      "        [-2.0049],\n",
      "        [-2.0131],\n",
      "        [-1.9951],\n",
      "        [-2.0027],\n",
      "        [-2.0082],\n",
      "        [-2.0073],\n",
      "        [-2.0050],\n",
      "        [-2.0063],\n",
      "        [-2.0050],\n",
      "        [-2.0026],\n",
      "        [-2.0010],\n",
      "        [-2.0096],\n",
      "        [-2.0064],\n",
      "        [-2.0066],\n",
      "        [-2.0060],\n",
      "        [-2.0066],\n",
      "        [-2.0013],\n",
      "        [-2.0063],\n",
      "        [-2.0059],\n",
      "        [-2.0061],\n",
      "        [-2.0084],\n",
      "        [-2.0078],\n",
      "        [-2.0081],\n",
      "        [-2.0111],\n",
      "        [-2.0061],\n",
      "        [-1.9988],\n",
      "        [-2.0034],\n",
      "        [-2.0077],\n",
      "        [-1.9921],\n",
      "        [-2.0059],\n",
      "        [-2.0113],\n",
      "        [-2.0060],\n",
      "        [-2.0018],\n",
      "        [-2.0096],\n",
      "        [-2.0079],\n",
      "        [-2.0130],\n",
      "        [-2.0019],\n",
      "        [-2.0062],\n",
      "        [-2.0035],\n",
      "        [-2.0042],\n",
      "        [-1.9977],\n",
      "        [-2.0091],\n",
      "        [-2.0061],\n",
      "        [-1.9996],\n",
      "        [-2.0078],\n",
      "        [-1.9990],\n",
      "        [-1.9877],\n",
      "        [-1.9953],\n",
      "        [-2.0069],\n",
      "        [-2.0092],\n",
      "        [-2.0102],\n",
      "        [-2.0072],\n",
      "        [-2.0144],\n",
      "        [-2.0274],\n",
      "        [-2.0061],\n",
      "        [-2.0083],\n",
      "        [-2.0063],\n",
      "        [-2.0097],\n",
      "        [-2.0142],\n",
      "        [-2.0083],\n",
      "        [-1.9908],\n",
      "        [-1.9995],\n",
      "        [-2.0051],\n",
      "        [-2.0081],\n",
      "        [-2.0121],\n",
      "        [-2.0018],\n",
      "        [-2.0018],\n",
      "        [-2.0002],\n",
      "        [-2.0078],\n",
      "        [-2.0035],\n",
      "        [-1.9985],\n",
      "        [-2.0131],\n",
      "        [-2.0095],\n",
      "        [-2.0092],\n",
      "        [-2.0184],\n",
      "        [-2.0050],\n",
      "        [-2.0073],\n",
      "        [-2.0098],\n",
      "        [-2.0039],\n",
      "        [-2.0061],\n",
      "        [-2.0031],\n",
      "        [-2.0078],\n",
      "        [-2.0055],\n",
      "        [-2.0065],\n",
      "        [-2.0126],\n",
      "        [-2.0057],\n",
      "        [-2.0040],\n",
      "        [-2.0015],\n",
      "        [-2.0131],\n",
      "        [-2.0094],\n",
      "        [-2.0109],\n",
      "        [-2.0060],\n",
      "        [-2.0096],\n",
      "        [-2.0061],\n",
      "        [-2.0048],\n",
      "        [-2.0158],\n",
      "        [-2.0007],\n",
      "        [-2.0118],\n",
      "        [-2.0074],\n",
      "        [-2.0099],\n",
      "        [-2.0048],\n",
      "        [-1.9970],\n",
      "        [-2.0060],\n",
      "        [-2.0047],\n",
      "        [-2.0082],\n",
      "        [-2.0048],\n",
      "        [-2.0060],\n",
      "        [-2.0051],\n",
      "        [-2.0054],\n",
      "        [-2.0029],\n",
      "        [-1.9978],\n",
      "        [-2.0140],\n",
      "        [-2.0060],\n",
      "        [-2.0056],\n",
      "        [-2.0139],\n",
      "        [-2.0053],\n",
      "        [-2.0061],\n",
      "        [-2.0106],\n",
      "        [-2.0060],\n",
      "        [-2.0056],\n",
      "        [-2.0066],\n",
      "        [-2.0058],\n",
      "        [-2.0053],\n",
      "        [-2.0129],\n",
      "        [-2.0085],\n",
      "        [-2.0033],\n",
      "        [-2.0090],\n",
      "        [-2.0069],\n",
      "        [-2.0084],\n",
      "        [-2.0064],\n",
      "        [-2.0051],\n",
      "        [-2.0069],\n",
      "        [-2.0088],\n",
      "        [-2.0052],\n",
      "        [-2.0102],\n",
      "        [-2.0066],\n",
      "        [-2.0023],\n",
      "        [-2.0064],\n",
      "        [-2.0062],\n",
      "        [-2.0010],\n",
      "        [-2.0060],\n",
      "        [-2.0003],\n",
      "        [-2.0064],\n",
      "        [-2.0046],\n",
      "        [-2.0054],\n",
      "        [-2.0075],\n",
      "        [-2.0028],\n",
      "        [-2.0136],\n",
      "        [-2.0021],\n",
      "        [-2.0050],\n",
      "        [-2.0121],\n",
      "        [-2.0113],\n",
      "        [-2.0069],\n",
      "        [-2.0112],\n",
      "        [-2.0068],\n",
      "        [-2.0152],\n",
      "        [-2.0085],\n",
      "        [-2.0128],\n",
      "        [-2.0059],\n",
      "        [-2.0060],\n",
      "        [-2.0033],\n",
      "        [-2.0102]], requires_grad=True))\n",
      "('hidden_b_var.1', Parameter containing:\n",
      "tensor([[-2.0057],\n",
      "        [-2.0049],\n",
      "        [-2.0053],\n",
      "        [-2.0057],\n",
      "        [-2.0065],\n",
      "        [-2.0060],\n",
      "        [-2.0062],\n",
      "        [-2.0066],\n",
      "        [-2.0066],\n",
      "        [-2.0061],\n",
      "        [-2.0048],\n",
      "        [-2.0061],\n",
      "        [-2.0057],\n",
      "        [-2.0067],\n",
      "        [-2.0048],\n",
      "        [-2.0060],\n",
      "        [-2.0061],\n",
      "        [-2.0062],\n",
      "        [-2.0061],\n",
      "        [-2.0061],\n",
      "        [-2.0057],\n",
      "        [-2.0052],\n",
      "        [-2.0065],\n",
      "        [-2.0063],\n",
      "        [-2.0058],\n",
      "        [-2.0056],\n",
      "        [-2.0070],\n",
      "        [-2.0065],\n",
      "        [-2.0058],\n",
      "        [-2.0061],\n",
      "        [-2.0061],\n",
      "        [-2.0052],\n",
      "        [-2.0060],\n",
      "        [-2.0064],\n",
      "        [-2.0061],\n",
      "        [-2.0056],\n",
      "        [-2.0060],\n",
      "        [-2.0061],\n",
      "        [-2.0060],\n",
      "        [-2.0062],\n",
      "        [-2.0053],\n",
      "        [-2.0061],\n",
      "        [-2.0063],\n",
      "        [-2.0055],\n",
      "        [-2.0061],\n",
      "        [-2.0054],\n",
      "        [-2.0059],\n",
      "        [-2.0061],\n",
      "        [-2.0060],\n",
      "        [-2.0068],\n",
      "        [-2.0059],\n",
      "        [-2.0045],\n",
      "        [-2.0058],\n",
      "        [-2.0056],\n",
      "        [-2.0061],\n",
      "        [-2.0066],\n",
      "        [-2.0052],\n",
      "        [-2.0065],\n",
      "        [-2.0051],\n",
      "        [-2.0064],\n",
      "        [-2.0060],\n",
      "        [-2.0058],\n",
      "        [-2.0069],\n",
      "        [-2.0074],\n",
      "        [-2.0070],\n",
      "        [-2.0055],\n",
      "        [-2.0061],\n",
      "        [-2.0060],\n",
      "        [-2.0066],\n",
      "        [-2.0078],\n",
      "        [-2.0059],\n",
      "        [-2.0060],\n",
      "        [-2.0069],\n",
      "        [-2.0049],\n",
      "        [-2.0072],\n",
      "        [-2.0064],\n",
      "        [-2.0054],\n",
      "        [-2.0061],\n",
      "        [-2.0053],\n",
      "        [-2.0063],\n",
      "        [-2.0052],\n",
      "        [-2.0064],\n",
      "        [-2.0062],\n",
      "        [-2.0060],\n",
      "        [-2.0070],\n",
      "        [-2.0048],\n",
      "        [-2.0068],\n",
      "        [-2.0060],\n",
      "        [-2.0062],\n",
      "        [-2.0057],\n",
      "        [-2.0053],\n",
      "        [-2.0060],\n",
      "        [-2.0051],\n",
      "        [-2.0072],\n",
      "        [-2.0060],\n",
      "        [-2.0051],\n",
      "        [-2.0052],\n",
      "        [-2.0067],\n",
      "        [-2.0048],\n",
      "        [-2.0047],\n",
      "        [-2.0055],\n",
      "        [-2.0060],\n",
      "        [-2.0063],\n",
      "        [-2.0061],\n",
      "        [-2.0057],\n",
      "        [-2.0044],\n",
      "        [-2.0059],\n",
      "        [-2.0059],\n",
      "        [-2.0042],\n",
      "        [-2.0063],\n",
      "        [-2.0062],\n",
      "        [-2.0061],\n",
      "        [-2.0066],\n",
      "        [-2.0058],\n",
      "        [-2.0054],\n",
      "        [-2.0054],\n",
      "        [-2.0061],\n",
      "        [-2.0057],\n",
      "        [-2.0070],\n",
      "        [-2.0062],\n",
      "        [-2.0065],\n",
      "        [-2.0070],\n",
      "        [-2.0069],\n",
      "        [-2.0055],\n",
      "        [-2.0064],\n",
      "        [-2.0057],\n",
      "        [-2.0072],\n",
      "        [-2.0060],\n",
      "        [-2.0059],\n",
      "        [-2.0061],\n",
      "        [-2.0064],\n",
      "        [-2.0060],\n",
      "        [-2.0055],\n",
      "        [-2.0069],\n",
      "        [-2.0062],\n",
      "        [-2.0058],\n",
      "        [-2.0057],\n",
      "        [-2.0064],\n",
      "        [-2.0066],\n",
      "        [-2.0060],\n",
      "        [-2.0068],\n",
      "        [-2.0062],\n",
      "        [-2.0059],\n",
      "        [-2.0064],\n",
      "        [-2.0061],\n",
      "        [-2.0067],\n",
      "        [-2.0060],\n",
      "        [-2.0060],\n",
      "        [-2.0064],\n",
      "        [-2.0071],\n",
      "        [-2.0060],\n",
      "        [-2.0060],\n",
      "        [-2.0056],\n",
      "        [-2.0071],\n",
      "        [-2.0061],\n",
      "        [-2.0063],\n",
      "        [-2.0066],\n",
      "        [-2.0062],\n",
      "        [-2.0064],\n",
      "        [-2.0064],\n",
      "        [-2.0060],\n",
      "        [-2.0066],\n",
      "        [-2.0066],\n",
      "        [-2.0060],\n",
      "        [-2.0061],\n",
      "        [-2.0060],\n",
      "        [-2.0058],\n",
      "        [-2.0058],\n",
      "        [-2.0062],\n",
      "        [-2.0058],\n",
      "        [-2.0060],\n",
      "        [-2.0060],\n",
      "        [-2.0078],\n",
      "        [-2.0060],\n",
      "        [-2.0060],\n",
      "        [-2.0068],\n",
      "        [-2.0062],\n",
      "        [-2.0064],\n",
      "        [-2.0057],\n",
      "        [-2.0062],\n",
      "        [-2.0069],\n",
      "        [-2.0063],\n",
      "        [-2.0063],\n",
      "        [-2.0066],\n",
      "        [-2.0067],\n",
      "        [-2.0061],\n",
      "        [-2.0060],\n",
      "        [-2.0069],\n",
      "        [-2.0061],\n",
      "        [-2.0064],\n",
      "        [-2.0060],\n",
      "        [-2.0070],\n",
      "        [-2.0061],\n",
      "        [-2.0059],\n",
      "        [-2.0060],\n",
      "        [-2.0059],\n",
      "        [-2.0057],\n",
      "        [-2.0056],\n",
      "        [-2.0063],\n",
      "        [-2.0055],\n",
      "        [-2.0060],\n",
      "        [-2.0063],\n",
      "        [-2.0062],\n",
      "        [-2.0058],\n",
      "        [-2.0062],\n",
      "        [-2.0053],\n",
      "        [-2.0060],\n",
      "        [-2.0059],\n",
      "        [-2.0060],\n",
      "        [-2.0060],\n",
      "        [-2.0056],\n",
      "        [-2.0062],\n",
      "        [-2.0054],\n",
      "        [-2.0058],\n",
      "        [-2.0062],\n",
      "        [-2.0061],\n",
      "        [-2.0060],\n",
      "        [-2.0060],\n",
      "        [-2.0074],\n",
      "        [-2.0062],\n",
      "        [-2.0060],\n",
      "        [-2.0075],\n",
      "        [-2.0054],\n",
      "        [-2.0061],\n",
      "        [-2.0058],\n",
      "        [-2.0074],\n",
      "        [-2.0059],\n",
      "        [-2.0061],\n",
      "        [-2.0058],\n",
      "        [-2.0061],\n",
      "        [-2.0063],\n",
      "        [-2.0063],\n",
      "        [-2.0060],\n",
      "        [-2.0062],\n",
      "        [-2.0065],\n",
      "        [-2.0055],\n",
      "        [-2.0060],\n",
      "        [-2.0060],\n",
      "        [-2.0061],\n",
      "        [-2.0054],\n",
      "        [-2.0060],\n",
      "        [-2.0063],\n",
      "        [-2.0060],\n",
      "        [-2.0056],\n",
      "        [-2.0070],\n",
      "        [-2.0061],\n",
      "        [-2.0052],\n",
      "        [-2.0066],\n",
      "        [-2.0068],\n",
      "        [-2.0060],\n",
      "        [-2.0045],\n",
      "        [-2.0069],\n",
      "        [-2.0060],\n",
      "        [-2.0059],\n",
      "        [-2.0059],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        [-2.0059]], requires_grad=True))\n",
      "('out_layers_w_mean.0', Parameter containing:\n",
      "tensor([[-0.1903],\n",
      "        [ 0.0262],\n",
      "        [-0.0187],\n",
      "        [ 0.0391],\n",
      "        [-0.0975],\n",
      "        [ 0.1049],\n",
      "        [-0.0994],\n",
      "        [ 0.0796],\n",
      "        [ 0.1510],\n",
      "        [-0.2374],\n",
      "        [ 0.1518],\n",
      "        [ 0.1402],\n",
      "        [ 0.1292],\n",
      "        [ 0.0045],\n",
      "        [ 0.0320],\n",
      "        [-0.0529],\n",
      "        [-0.1723],\n",
      "        [-0.0487],\n",
      "        [-0.0101],\n",
      "        [-0.1039],\n",
      "        [ 0.0980],\n",
      "        [-0.0898],\n",
      "        [ 0.0149],\n",
      "        [-0.2111],\n",
      "        [-0.2005],\n",
      "        [-0.0352],\n",
      "        [-0.0243],\n",
      "        [ 0.0324],\n",
      "        [ 0.1276],\n",
      "        [-0.0008],\n",
      "        [-0.0485],\n",
      "        [ 0.0467],\n",
      "        [-0.1710],\n",
      "        [-0.0421],\n",
      "        [ 0.1007],\n",
      "        [-0.0359],\n",
      "        [ 0.0170],\n",
      "        [-0.0017],\n",
      "        [ 0.0028],\n",
      "        [ 0.1358],\n",
      "        [ 0.1168],\n",
      "        [ 0.0400],\n",
      "        [-0.1053],\n",
      "        [ 0.1915],\n",
      "        [-0.0676],\n",
      "        [-0.1115],\n",
      "        [ 0.0798],\n",
      "        [-0.1054],\n",
      "        [-0.0671],\n",
      "        [ 0.1163],\n",
      "        [-0.0285],\n",
      "        [ 0.0332],\n",
      "        [ 0.0678],\n",
      "        [-0.1401],\n",
      "        [-0.0023],\n",
      "        [ 0.1802],\n",
      "        [-0.0314],\n",
      "        [ 0.1172],\n",
      "        [ 0.0535],\n",
      "        [ 0.0785],\n",
      "        [-0.0885],\n",
      "        [ 0.1050],\n",
      "        [ 0.1351],\n",
      "        [-0.0167],\n",
      "        [ 0.1122],\n",
      "        [-0.0486],\n",
      "        [ 0.0296],\n",
      "        [ 0.1171],\n",
      "        [-0.1351],\n",
      "        [-0.0918],\n",
      "        [-0.0133],\n",
      "        [-0.0442],\n",
      "        [ 0.1277],\n",
      "        [ 0.0197],\n",
      "        [ 0.1515],\n",
      "        [-0.1221],\n",
      "        [-0.0461],\n",
      "        [-0.0314],\n",
      "        [-0.1424],\n",
      "        [ 0.1186],\n",
      "        [ 0.0110],\n",
      "        [ 0.1450],\n",
      "        [-0.0545],\n",
      "        [ 0.1462],\n",
      "        [-0.1460],\n",
      "        [ 0.0074],\n",
      "        [-0.1978],\n",
      "        [-0.0649],\n",
      "        [-0.2025],\n",
      "        [-0.0720],\n",
      "        [ 0.0533],\n",
      "        [-0.1790],\n",
      "        [ 0.1728],\n",
      "        [ 0.0539],\n",
      "        [-0.1613],\n",
      "        [ 0.1614],\n",
      "        [-0.0549],\n",
      "        [ 0.2185],\n",
      "        [ 0.1172],\n",
      "        [-0.1163],\n",
      "        [ 0.0862],\n",
      "        [ 0.0622],\n",
      "        [ 0.0773],\n",
      "        [-0.0410],\n",
      "        [-0.1419],\n",
      "        [ 0.0584],\n",
      "        [-0.1348],\n",
      "        [ 0.0312],\n",
      "        [-0.0038],\n",
      "        [ 0.2127],\n",
      "        [ 0.1029],\n",
      "        [ 0.0215],\n",
      "        [ 0.1136],\n",
      "        [ 0.0414],\n",
      "        [ 0.0475],\n",
      "        [ 0.0827],\n",
      "        [-0.1627],\n",
      "        [-0.0519],\n",
      "        [ 0.1619],\n",
      "        [-0.2396],\n",
      "        [-0.0534],\n",
      "        [-0.0651],\n",
      "        [ 0.0792],\n",
      "        [-0.0251],\n",
      "        [ 0.1267],\n",
      "        [ 0.0574],\n",
      "        [ 0.2329],\n",
      "        [-0.0465],\n",
      "        [-0.0632],\n",
      "        [-0.0525],\n",
      "        [-0.1053],\n",
      "        [-0.0538],\n",
      "        [-0.0968],\n",
      "        [ 0.1452],\n",
      "        [ 0.0578],\n",
      "        [ 0.0832],\n",
      "        [-0.1313],\n",
      "        [ 0.0547],\n",
      "        [-0.1050],\n",
      "        [ 0.0265],\n",
      "        [ 0.0327],\n",
      "        [ 0.0266],\n",
      "        [-0.1569],\n",
      "        [-0.1224],\n",
      "        [ 0.1686],\n",
      "        [-0.1877],\n",
      "        [ 0.0422],\n",
      "        [-0.0204],\n",
      "        [ 0.0697],\n",
      "        [-0.1029],\n",
      "        [ 0.0396],\n",
      "        [-0.0398],\n",
      "        [-0.0204],\n",
      "        [ 0.1085],\n",
      "        [-0.1440],\n",
      "        [-0.0699],\n",
      "        [ 0.0198],\n",
      "        [ 0.2530],\n",
      "        [-0.1037],\n",
      "        [-0.0459],\n",
      "        [ 0.0216],\n",
      "        [ 0.1995],\n",
      "        [-0.0495],\n",
      "        [ 0.1277],\n",
      "        [-0.0535],\n",
      "        [-0.0426],\n",
      "        [-0.0938],\n",
      "        [ 0.0283],\n",
      "        [-0.0525],\n",
      "        [-0.0584],\n",
      "        [ 0.0382],\n",
      "        [ 0.0835],\n",
      "        [ 0.0363],\n",
      "        [-0.1242],\n",
      "        [ 0.2544],\n",
      "        [-0.1829],\n",
      "        [ 0.2067],\n",
      "        [ 0.0278],\n",
      "        [ 0.0611],\n",
      "        [-0.1123],\n",
      "        [-0.0398],\n",
      "        [-0.1421],\n",
      "        [ 0.1490],\n",
      "        [-0.0761],\n",
      "        [-0.2486],\n",
      "        [ 0.0662],\n",
      "        [-0.1427],\n",
      "        [-0.0423],\n",
      "        [-0.1042],\n",
      "        [ 0.0082],\n",
      "        [ 0.1954],\n",
      "        [-0.0098],\n",
      "        [-0.0691],\n",
      "        [-0.0991],\n",
      "        [-0.0357],\n",
      "        [ 0.0610],\n",
      "        [-0.1260],\n",
      "        [ 0.0281],\n",
      "        [-0.1012],\n",
      "        [-0.0977],\n",
      "        [-0.0024],\n",
      "        [-0.0953],\n",
      "        [ 0.1132],\n",
      "        [-0.0343],\n",
      "        [ 0.1176],\n",
      "        [ 0.0745],\n",
      "        [-0.1688],\n",
      "        [ 0.0394],\n",
      "        [-0.0161],\n",
      "        [ 0.1912],\n",
      "        [ 0.0105],\n",
      "        [-0.2989],\n",
      "        [-0.1098],\n",
      "        [ 0.0101],\n",
      "        [-0.0427],\n",
      "        [ 0.1688],\n",
      "        [-0.0034],\n",
      "        [-0.1422],\n",
      "        [-0.1134],\n",
      "        [ 0.0763],\n",
      "        [-0.0176],\n",
      "        [-0.0466],\n",
      "        [-0.0147],\n",
      "        [ 0.1515],\n",
      "        [ 0.1368],\n",
      "        [-0.0672],\n",
      "        [-0.0848],\n",
      "        [ 0.0688],\n",
      "        [ 0.0496],\n",
      "        [-0.1586],\n",
      "        [-0.0080],\n",
      "        [ 0.1623],\n",
      "        [ 0.1054],\n",
      "        [-0.1195],\n",
      "        [ 0.1287],\n",
      "        [-0.1423],\n",
      "        [-0.0843],\n",
      "        [-0.1362],\n",
      "        [-0.0705],\n",
      "        [ 0.0331],\n",
      "        [-0.2247],\n",
      "        [ 0.0587],\n",
      "        [ 0.1032],\n",
      "        [ 0.0142],\n",
      "        [-0.0516],\n",
      "        [-0.1659],\n",
      "        [ 0.0073],\n",
      "        [ 0.1867],\n",
      "        [-0.0581],\n",
      "        [ 0.1499],\n",
      "        [-0.0184],\n",
      "        [-0.0625],\n",
      "        [ 0.0140],\n",
      "        [ 0.0201],\n",
      "        [ 0.0906],\n",
      "        [-0.0815]], requires_grad=True))\n",
      "('out_layers_w_mean.1', Parameter containing:\n",
      "tensor([[-0.1903],\n",
      "        [ 0.0262],\n",
      "        [-0.0187],\n",
      "        [ 0.0391],\n",
      "        [-0.0975],\n",
      "        [ 0.1049],\n",
      "        [-0.0994],\n",
      "        [ 0.0796],\n",
      "        [ 0.1510],\n",
      "        [-0.2374],\n",
      "        [ 0.1518],\n",
      "        [ 0.1402],\n",
      "        [ 0.1292],\n",
      "        [ 0.0045],\n",
      "        [ 0.0320],\n",
      "        [-0.0529],\n",
      "        [-0.1723],\n",
      "        [-0.0487],\n",
      "        [-0.0101],\n",
      "        [-0.1039],\n",
      "        [ 0.0980],\n",
      "        [-0.0898],\n",
      "        [ 0.0149],\n",
      "        [-0.2111],\n",
      "        [-0.2005],\n",
      "        [-0.0352],\n",
      "        [-0.0243],\n",
      "        [ 0.0324],\n",
      "        [ 0.1276],\n",
      "        [-0.0008],\n",
      "        [-0.0485],\n",
      "        [ 0.0467],\n",
      "        [-0.1710],\n",
      "        [-0.0421],\n",
      "        [ 0.1007],\n",
      "        [-0.0359],\n",
      "        [ 0.0170],\n",
      "        [-0.0017],\n",
      "        [ 0.0028],\n",
      "        [ 0.1358],\n",
      "        [ 0.1168],\n",
      "        [ 0.0400],\n",
      "        [-0.1053],\n",
      "        [ 0.1915],\n",
      "        [-0.0676],\n",
      "        [-0.1115],\n",
      "        [ 0.0798],\n",
      "        [-0.1054],\n",
      "        [-0.0671],\n",
      "        [ 0.1163],\n",
      "        [-0.0285],\n",
      "        [ 0.0332],\n",
      "        [ 0.0678],\n",
      "        [-0.1401],\n",
      "        [-0.0023],\n",
      "        [ 0.1802],\n",
      "        [-0.0314],\n",
      "        [ 0.1172],\n",
      "        [ 0.0535],\n",
      "        [ 0.0785],\n",
      "        [-0.0885],\n",
      "        [ 0.1050],\n",
      "        [ 0.1351],\n",
      "        [-0.0167],\n",
      "        [ 0.1122],\n",
      "        [-0.0486],\n",
      "        [ 0.0296],\n",
      "        [ 0.1171],\n",
      "        [-0.1351],\n",
      "        [-0.0918],\n",
      "        [-0.0133],\n",
      "        [-0.0442],\n",
      "        [ 0.1277],\n",
      "        [ 0.0197],\n",
      "        [ 0.1515],\n",
      "        [-0.1221],\n",
      "        [-0.0461],\n",
      "        [-0.0314],\n",
      "        [-0.1424],\n",
      "        [ 0.1186],\n",
      "        [ 0.0110],\n",
      "        [ 0.1450],\n",
      "        [-0.0545],\n",
      "        [ 0.1462],\n",
      "        [-0.1460],\n",
      "        [ 0.0074],\n",
      "        [-0.1978],\n",
      "        [-0.0649],\n",
      "        [-0.2025],\n",
      "        [-0.0720],\n",
      "        [ 0.0533],\n",
      "        [-0.1790],\n",
      "        [ 0.1728],\n",
      "        [ 0.0539],\n",
      "        [-0.1613],\n",
      "        [ 0.1614],\n",
      "        [-0.0549],\n",
      "        [ 0.2185],\n",
      "        [ 0.1172],\n",
      "        [-0.1163],\n",
      "        [ 0.0862],\n",
      "        [ 0.0622],\n",
      "        [ 0.0773],\n",
      "        [-0.0410],\n",
      "        [-0.1419],\n",
      "        [ 0.0584],\n",
      "        [-0.1348],\n",
      "        [ 0.0312],\n",
      "        [-0.0038],\n",
      "        [ 0.2127],\n",
      "        [ 0.1029],\n",
      "        [ 0.0215],\n",
      "        [ 0.1136],\n",
      "        [ 0.0414],\n",
      "        [ 0.0475],\n",
      "        [ 0.0827],\n",
      "        [-0.1627],\n",
      "        [-0.0519],\n",
      "        [ 0.1619],\n",
      "        [-0.2396],\n",
      "        [-0.0534],\n",
      "        [-0.0651],\n",
      "        [ 0.0792],\n",
      "        [-0.0251],\n",
      "        [ 0.1267],\n",
      "        [ 0.0574],\n",
      "        [ 0.2329],\n",
      "        [-0.0465],\n",
      "        [-0.0632],\n",
      "        [-0.0525],\n",
      "        [-0.1053],\n",
      "        [-0.0538],\n",
      "        [-0.0968],\n",
      "        [ 0.1452],\n",
      "        [ 0.0578],\n",
      "        [ 0.0832],\n",
      "        [-0.1313],\n",
      "        [ 0.0547],\n",
      "        [-0.1050],\n",
      "        [ 0.0265],\n",
      "        [ 0.0327],\n",
      "        [ 0.0266],\n",
      "        [-0.1569],\n",
      "        [-0.1224],\n",
      "        [ 0.1686],\n",
      "        [-0.1877],\n",
      "        [ 0.0422],\n",
      "        [-0.0204],\n",
      "        [ 0.0697],\n",
      "        [-0.1029],\n",
      "        [ 0.0396],\n",
      "        [-0.0398],\n",
      "        [-0.0204],\n",
      "        [ 0.1085],\n",
      "        [-0.1440],\n",
      "        [-0.0699],\n",
      "        [ 0.0198],\n",
      "        [ 0.2530],\n",
      "        [-0.1037],\n",
      "        [-0.0459],\n",
      "        [ 0.0216],\n",
      "        [ 0.1995],\n",
      "        [-0.0495],\n",
      "        [ 0.1277],\n",
      "        [-0.0535],\n",
      "        [-0.0426],\n",
      "        [-0.0938],\n",
      "        [ 0.0283],\n",
      "        [-0.0525],\n",
      "        [-0.0584],\n",
      "        [ 0.0382],\n",
      "        [ 0.0835],\n",
      "        [ 0.0363],\n",
      "        [-0.1242],\n",
      "        [ 0.2544],\n",
      "        [-0.1829],\n",
      "        [ 0.2067],\n",
      "        [ 0.0278],\n",
      "        [ 0.0611],\n",
      "        [-0.1123],\n",
      "        [-0.0398],\n",
      "        [-0.1421],\n",
      "        [ 0.1490],\n",
      "        [-0.0761],\n",
      "        [-0.2486],\n",
      "        [ 0.0662],\n",
      "        [-0.1427],\n",
      "        [-0.0423],\n",
      "        [-0.1042],\n",
      "        [ 0.0082],\n",
      "        [ 0.1954],\n",
      "        [-0.0098],\n",
      "        [-0.0691],\n",
      "        [-0.0991],\n",
      "        [-0.0357],\n",
      "        [ 0.0610],\n",
      "        [-0.1260],\n",
      "        [ 0.0281],\n",
      "        [-0.1012],\n",
      "        [-0.0977],\n",
      "        [-0.0024],\n",
      "        [-0.0953],\n",
      "        [ 0.1132],\n",
      "        [-0.0343],\n",
      "        [ 0.1176],\n",
      "        [ 0.0745],\n",
      "        [-0.1688],\n",
      "        [ 0.0394],\n",
      "        [-0.0161],\n",
      "        [ 0.1912],\n",
      "        [ 0.0105],\n",
      "        [-0.2989],\n",
      "        [-0.1098],\n",
      "        [ 0.0101],\n",
      "        [-0.0427],\n",
      "        [ 0.1688],\n",
      "        [-0.0034],\n",
      "        [-0.1422],\n",
      "        [-0.1134],\n",
      "        [ 0.0763],\n",
      "        [-0.0176],\n",
      "        [-0.0466],\n",
      "        [-0.0147],\n",
      "        [ 0.1515],\n",
      "        [ 0.1368],\n",
      "        [-0.0672],\n",
      "        [-0.0848],\n",
      "        [ 0.0688],\n",
      "        [ 0.0496],\n",
      "        [-0.1586],\n",
      "        [-0.0080],\n",
      "        [ 0.1623],\n",
      "        [ 0.1054],\n",
      "        [-0.1195],\n",
      "        [ 0.1287],\n",
      "        [-0.1423],\n",
      "        [-0.0843],\n",
      "        [-0.1362],\n",
      "        [-0.0705],\n",
      "        [ 0.0331],\n",
      "        [-0.2247],\n",
      "        [ 0.0587],\n",
      "        [ 0.1032],\n",
      "        [ 0.0142],\n",
      "        [-0.0516],\n",
      "        [-0.1659],\n",
      "        [ 0.0073],\n",
      "        [ 0.1867],\n",
      "        [-0.0581],\n",
      "        [ 0.1499],\n",
      "        [-0.0184],\n",
      "        [-0.0625],\n",
      "        [ 0.0140],\n",
      "        [ 0.0201],\n",
      "        [ 0.0906],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        [-0.0815]], requires_grad=True))\n",
      "('out_layers_w_mean.2', Parameter containing:\n",
      "tensor([[-0.1903],\n",
      "        [ 0.0262],\n",
      "        [-0.0187],\n",
      "        [ 0.0391],\n",
      "        [-0.0975],\n",
      "        [ 0.1049],\n",
      "        [-0.0994],\n",
      "        [ 0.0796],\n",
      "        [ 0.1510],\n",
      "        [-0.2374],\n",
      "        [ 0.1518],\n",
      "        [ 0.1402],\n",
      "        [ 0.1292],\n",
      "        [ 0.0045],\n",
      "        [ 0.0320],\n",
      "        [-0.0529],\n",
      "        [-0.1723],\n",
      "        [-0.0487],\n",
      "        [-0.0101],\n",
      "        [-0.1039],\n",
      "        [ 0.0980],\n",
      "        [-0.0898],\n",
      "        [ 0.0149],\n",
      "        [-0.2111],\n",
      "        [-0.2005],\n",
      "        [-0.0352],\n",
      "        [-0.0243],\n",
      "        [ 0.0324],\n",
      "        [ 0.1276],\n",
      "        [-0.0008],\n",
      "        [-0.0485],\n",
      "        [ 0.0467],\n",
      "        [-0.1710],\n",
      "        [-0.0421],\n",
      "        [ 0.1007],\n",
      "        [-0.0359],\n",
      "        [ 0.0170],\n",
      "        [-0.0017],\n",
      "        [ 0.0028],\n",
      "        [ 0.1358],\n",
      "        [ 0.1168],\n",
      "        [ 0.0400],\n",
      "        [-0.1053],\n",
      "        [ 0.1915],\n",
      "        [-0.0676],\n",
      "        [-0.1115],\n",
      "        [ 0.0798],\n",
      "        [-0.1054],\n",
      "        [-0.0671],\n",
      "        [ 0.1163],\n",
      "        [-0.0285],\n",
      "        [ 0.0332],\n",
      "        [ 0.0678],\n",
      "        [-0.1401],\n",
      "        [-0.0023],\n",
      "        [ 0.1802],\n",
      "        [-0.0314],\n",
      "        [ 0.1172],\n",
      "        [ 0.0535],\n",
      "        [ 0.0785],\n",
      "        [-0.0885],\n",
      "        [ 0.1050],\n",
      "        [ 0.1351],\n",
      "        [-0.0167],\n",
      "        [ 0.1122],\n",
      "        [-0.0486],\n",
      "        [ 0.0296],\n",
      "        [ 0.1171],\n",
      "        [-0.1351],\n",
      "        [-0.0918],\n",
      "        [-0.0133],\n",
      "        [-0.0442],\n",
      "        [ 0.1277],\n",
      "        [ 0.0197],\n",
      "        [ 0.1515],\n",
      "        [-0.1221],\n",
      "        [-0.0461],\n",
      "        [-0.0314],\n",
      "        [-0.1424],\n",
      "        [ 0.1186],\n",
      "        [ 0.0110],\n",
      "        [ 0.1450],\n",
      "        [-0.0545],\n",
      "        [ 0.1462],\n",
      "        [-0.1460],\n",
      "        [ 0.0074],\n",
      "        [-0.1978],\n",
      "        [-0.0649],\n",
      "        [-0.2025],\n",
      "        [-0.0720],\n",
      "        [ 0.0533],\n",
      "        [-0.1790],\n",
      "        [ 0.1728],\n",
      "        [ 0.0539],\n",
      "        [-0.1613],\n",
      "        [ 0.1614],\n",
      "        [-0.0549],\n",
      "        [ 0.2185],\n",
      "        [ 0.1172],\n",
      "        [-0.1163],\n",
      "        [ 0.0862],\n",
      "        [ 0.0622],\n",
      "        [ 0.0773],\n",
      "        [-0.0410],\n",
      "        [-0.1419],\n",
      "        [ 0.0584],\n",
      "        [-0.1348],\n",
      "        [ 0.0312],\n",
      "        [-0.0038],\n",
      "        [ 0.2127],\n",
      "        [ 0.1029],\n",
      "        [ 0.0215],\n",
      "        [ 0.1136],\n",
      "        [ 0.0414],\n",
      "        [ 0.0475],\n",
      "        [ 0.0827],\n",
      "        [-0.1627],\n",
      "        [-0.0519],\n",
      "        [ 0.1619],\n",
      "        [-0.2396],\n",
      "        [-0.0534],\n",
      "        [-0.0651],\n",
      "        [ 0.0792],\n",
      "        [-0.0251],\n",
      "        [ 0.1267],\n",
      "        [ 0.0574],\n",
      "        [ 0.2329],\n",
      "        [-0.0465],\n",
      "        [-0.0632],\n",
      "        [-0.0525],\n",
      "        [-0.1053],\n",
      "        [-0.0538],\n",
      "        [-0.0968],\n",
      "        [ 0.1452],\n",
      "        [ 0.0578],\n",
      "        [ 0.0832],\n",
      "        [-0.1313],\n",
      "        [ 0.0547],\n",
      "        [-0.1050],\n",
      "        [ 0.0265],\n",
      "        [ 0.0327],\n",
      "        [ 0.0266],\n",
      "        [-0.1569],\n",
      "        [-0.1224],\n",
      "        [ 0.1686],\n",
      "        [-0.1877],\n",
      "        [ 0.0422],\n",
      "        [-0.0204],\n",
      "        [ 0.0697],\n",
      "        [-0.1029],\n",
      "        [ 0.0396],\n",
      "        [-0.0398],\n",
      "        [-0.0204],\n",
      "        [ 0.1085],\n",
      "        [-0.1440],\n",
      "        [-0.0699],\n",
      "        [ 0.0198],\n",
      "        [ 0.2530],\n",
      "        [-0.1037],\n",
      "        [-0.0459],\n",
      "        [ 0.0216],\n",
      "        [ 0.1995],\n",
      "        [-0.0495],\n",
      "        [ 0.1277],\n",
      "        [-0.0535],\n",
      "        [-0.0426],\n",
      "        [-0.0938],\n",
      "        [ 0.0283],\n",
      "        [-0.0525],\n",
      "        [-0.0584],\n",
      "        [ 0.0382],\n",
      "        [ 0.0835],\n",
      "        [ 0.0363],\n",
      "        [-0.1242],\n",
      "        [ 0.2544],\n",
      "        [-0.1829],\n",
      "        [ 0.2067],\n",
      "        [ 0.0278],\n",
      "        [ 0.0611],\n",
      "        [-0.1123],\n",
      "        [-0.0398],\n",
      "        [-0.1421],\n",
      "        [ 0.1490],\n",
      "        [-0.0761],\n",
      "        [-0.2486],\n",
      "        [ 0.0662],\n",
      "        [-0.1427],\n",
      "        [-0.0423],\n",
      "        [-0.1042],\n",
      "        [ 0.0082],\n",
      "        [ 0.1954],\n",
      "        [-0.0098],\n",
      "        [-0.0691],\n",
      "        [-0.0991],\n",
      "        [-0.0357],\n",
      "        [ 0.0610],\n",
      "        [-0.1260],\n",
      "        [ 0.0281],\n",
      "        [-0.1012],\n",
      "        [-0.0977],\n",
      "        [-0.0024],\n",
      "        [-0.0953],\n",
      "        [ 0.1132],\n",
      "        [-0.0343],\n",
      "        [ 0.1176],\n",
      "        [ 0.0745],\n",
      "        [-0.1688],\n",
      "        [ 0.0394],\n",
      "        [-0.0161],\n",
      "        [ 0.1912],\n",
      "        [ 0.0105],\n",
      "        [-0.2989],\n",
      "        [-0.1098],\n",
      "        [ 0.0101],\n",
      "        [-0.0427],\n",
      "        [ 0.1688],\n",
      "        [-0.0034],\n",
      "        [-0.1422],\n",
      "        [-0.1134],\n",
      "        [ 0.0763],\n",
      "        [-0.0176],\n",
      "        [-0.0466],\n",
      "        [-0.0147],\n",
      "        [ 0.1515],\n",
      "        [ 0.1368],\n",
      "        [-0.0672],\n",
      "        [-0.0848],\n",
      "        [ 0.0688],\n",
      "        [ 0.0496],\n",
      "        [-0.1586],\n",
      "        [-0.0080],\n",
      "        [ 0.1623],\n",
      "        [ 0.1054],\n",
      "        [-0.1195],\n",
      "        [ 0.1287],\n",
      "        [-0.1423],\n",
      "        [-0.0843],\n",
      "        [-0.1362],\n",
      "        [-0.0705],\n",
      "        [ 0.0331],\n",
      "        [-0.2247],\n",
      "        [ 0.0587],\n",
      "        [ 0.1032],\n",
      "        [ 0.0142],\n",
      "        [-0.0516],\n",
      "        [-0.1659],\n",
      "        [ 0.0073],\n",
      "        [ 0.1867],\n",
      "        [-0.0581],\n",
      "        [ 0.1499],\n",
      "        [-0.0184],\n",
      "        [-0.0625],\n",
      "        [ 0.0140],\n",
      "        [ 0.0201],\n",
      "        [ 0.0906],\n",
      "        [-0.0815]], requires_grad=True))\n",
      "('out_layers_b_mean.0', Parameter containing:\n",
      "tensor([[-0.0843]], requires_grad=True))\n",
      "('out_layers_b_mean.1', Parameter containing:\n",
      "tensor([[-0.0843]], requires_grad=True))\n",
      "('out_layers_b_mean.2', Parameter containing:\n",
      "tensor([[-0.0843]], requires_grad=True))\n",
      "('out_layers_w_var.0', Parameter containing:\n",
      "tensor([[-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        [-2.]], requires_grad=True))\n",
      "('out_layers_w_var.1', Parameter containing:\n",
      "tensor([[-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.]], requires_grad=True))\n",
      "('out_layers_w_var.2', Parameter containing:\n",
      "tensor([[-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.]], requires_grad=True))\n",
      "('out_layers_b_var.0', Parameter containing:\n",
      "tensor([[-2.]], requires_grad=True))\n",
      "('out_layers_b_var.1', Parameter containing:\n",
      "tensor([[-2.]], requires_grad=True))\n",
      "('out_layers_b_var.2', Parameter containing:\n",
      "tensor([[-2.]], requires_grad=True))\n"
     ]
    }
   ],
   "source": [
    "for i in net.named_parameters():\n",
    "    print(i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(len(torch.nn.ParameterList([])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(len(net.out_layers_b_mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
