{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VCL PYTORCH IMPL \n",
    "import torch\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------This class extends the Dataset class and basicly takes a Dataset [images,labels] and sort out the indexes with a specified sub_labels\n",
    "#------see how it it used later to make more sense of it.\n",
    "class SubDataset(Dataset): #FROM https://github.com/GMvandeVen/continual-learning/blob/master/data.py\n",
    "    '''To sub-sample a dataset, taking only those samples with label in [sub_labels].\n",
    "    After this selection of samples has been made, it is possible to transform the target-labels,\n",
    "    which can be useful when doing continual learning with fixed number of output units.'''\n",
    "\n",
    "    def __init__(self, original_dataset, sub_labels, target_transform=None):\n",
    "        super().__init__()\n",
    "        self.dataset = original_dataset\n",
    "        self.sub_indeces = []\n",
    "        for index in range(len(self.dataset)):\n",
    "            if hasattr(original_dataset, \"train_labels\"):\n",
    "                if self.dataset.target_transform is None:\n",
    "                    label = self.dataset.train_labels[index]\n",
    "                else:\n",
    "                    label = self.dataset.target_transform(self.dataset.train_labels[index])\n",
    "            elif hasattr(self.dataset, \"test_labels\"):\n",
    "                if self.dataset.target_transform is None:\n",
    "                    label = self.dataset.test_labels[index]\n",
    "                else:\n",
    "                    label = self.dataset.target_transform(self.dataset.test_labels[index])\n",
    "            else:\n",
    "                label = self.dataset[index][1]\n",
    "            if label in sub_labels:\n",
    "                self.sub_indeces.append(index)\n",
    "        self.target_transform = target_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ralle\\Anaconda3\\lib\\site-packages\\torchvision\\datasets\\mnist.py:43: UserWarning: train_labels has been renamed targets\n",
      "  warnings.warn(\"train_labels has been renamed targets\")\n"
     ]
    }
   ],
   "source": [
    "n_tasks = 5 #one for each number\n",
    "classes_per_task = 2 #because n_task = 10\n",
    "\n",
    "\n",
    "# prepare permutation to shuffle label-ids (to create different class batches for each random seed)\n",
    "#permutation = np.random.permutation(list(range(10)))\n",
    "#--------WE ACTUALLY DONT DO THE ABOVE. we always want the tasks to be the same so no permutation.\n",
    "permutation = list(range(10))\n",
    "#print(\"random permutation of labels\",permutation)\n",
    "#Lambda transform is a user defined transform.\n",
    "#-------nothhing really happens here\n",
    "target_transform = transforms.Lambda(lambda y, x=permutation: int(permutation[y]))\n",
    "\n",
    "dataset_transform = transforms.Compose([transforms.ToTensor()])#A lambda transform to random permutation of pixels can be added here in the permutetMNIST\n",
    "\n",
    "#-------Here the entire MNIST dataset is loaded\n",
    "mnist_train = torchvision.datasets.MNIST(\"/\", train=True,\n",
    "                            download=True, transform=dataset_transform, target_transform=target_transform)\n",
    "mnist_test = torchvision.datasets.MNIST(\"/\", train=False,\n",
    "                            download=True, transform=dataset_transform, target_transform=target_transform)\n",
    "\n",
    "#---------- generate labels-per-task. \n",
    "labels_per_task = [list(np.array(range(classes_per_task)) + classes_per_task * task_id) for task_id in range(n_tasks)]\n",
    "#print(labels_per_task)# [[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]]\n",
    "\n",
    "#----------- split them up into sub-tasks\n",
    "#-------we make a list where all sub dataset get put in.\n",
    "#HERE IS HOW WE take out one task data and labels. this is used in the training loop:\n",
    "#xtrain_set = train_datasets[task_no].dataset.data[train_datasets[task_no].sub_indeces]\n",
    "#ytrain_set = train_datasets[task_no].dataset.targets[train_datasets[task_no].sub_indeces]\n",
    "\n",
    "train_datasets = []\n",
    "test_datasets = []\n",
    "for labels in labels_per_task:\n",
    "    #target_transform = transforms.Lambda(lambda y, x=labels[0]: y - x) if scenario=='domain' else None #We are Task-IL: task is given, is it 1 or not\n",
    "    target_transform = None\n",
    "    train_datasets.append(SubDataset(mnist_train, labels, target_transform=target_transform))\n",
    "    test_datasets.append(SubDataset(mnist_test, labels, target_transform=target_transform))\n",
    "    \n",
    "#train_datasets is a list of 5 tasks with the entire dataset in each.\n",
    "#Each task has the sub_indicies that is the indicies where the target value fits the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Gotta do some coresets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset MNIST\n",
      "    Number of datapoints: 60000\n",
      "    Root location: /\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               ToTensor()\n",
      "           )\n",
      "Target transform: Lambda()\n",
      "12089\n",
      "tensor(2)\n",
      "tensor(3)\n",
      "tensor(3)\n",
      "tensor(3)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(3)\n",
      "tensor(2)\n",
      "tensor(3)\n",
      "tensor(3)\n",
      "tensor(3)\n",
      "tensor(3)\n",
      "tensor(3)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(3)\n",
      "tensor(3)\n",
      "tensor(3)\n",
      "tensor(2)\n",
      "tensor(3)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 16 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# this is just BS\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(train_datasets[1].dataset)\n",
    "print(len(train_datasets[1].sub_indeces))\n",
    "\n",
    "for i in range(20):\n",
    "    print(train_datasets[1].dataset.targets[train_datasets[1].sub_indeces[i]])\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(1,5):\n",
    "    for j in range(1,5):\n",
    "        plt.subplot(4,4,(j-1)*4+i)\n",
    "        plt.imshow(train_datasets[1].dataset.data[train_datasets[1].sub_indeces[i*j]])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12665\n",
      "12089\n",
      "11263\n",
      "12183\n",
      "11800\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(len(train_datasets[i].dataset.targets[train_datasets[i].sub_indeces]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_true(pred,y):\n",
    "    n = len(y)\n",
    "    prec=0\n",
    "    for i in range(n):\n",
    "        pred_index = torch.argmax(pred[i])\n",
    "        true_index = torch.argmax(y[i])\n",
    "        if  pred_index == true_index:\n",
    "            prec +=1\n",
    "    return prec\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####THE CORESET THING  ##inspired by paper\n",
    "def k_center(x_coreset, y_coreset, x_train, y_train, coreset_size):\n",
    "    # Select K centers from (x_train, y_train) and add to current coreset (x_coreset, y_coreset)\n",
    "    dists = np.full(x_train.shape[0], np.inf) #all dists are inf\n",
    "    current_id = 0\n",
    "    \n",
    "    for j in range(x_train.shape[0]):\n",
    "            current_dist = np.linalg.norm(x_train[j,:]-x_train[current_id,:])\n",
    "            dists[j] = np.minimum(current_dist, dists[j])\n",
    "            \n",
    "    idx = [ current_id ]\n",
    "\n",
    "    for i in range(1, coreset_size):\n",
    "        current_id = np.argmax(dists)\n",
    "        \n",
    "        for j in range(x_train.shape[0]):\n",
    "            current_dist = np.linalg.norm(x_train[j,:]-x_train[current_id,:])\n",
    "            dists[j] = np.minimum(current_dist, dists[j])\n",
    "        \n",
    "        idx.append(current_id)\n",
    "\n",
    "    x_coreset.append(x_train[idx,:])\n",
    "    y_coreset.append(y_train[idx])\n",
    "    x_train = np.delete(x_train, idx, axis=0)\n",
    "    y_train = np.delete(y_train, idx, axis=0)\n",
    "\n",
    "    return x_coreset, y_coreset, x_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (criterion): BCEWithLogitsLoss()\n",
      "  (hidden_w_mean): ParameterList(\n",
      "      (0): Parameter containing: [torch.FloatTensor of size 784x256]\n",
      "      (1): Parameter containing: [torch.FloatTensor of size 256x256]\n",
      "  )\n",
      "  (hidden_b_mean): ParameterList(\n",
      "      (0): Parameter containing: [torch.FloatTensor of size 256x1]\n",
      "      (1): Parameter containing: [torch.FloatTensor of size 256x1]\n",
      "  )\n",
      "  (hidden_w_var): ParameterList(\n",
      "      (0): Parameter containing: [torch.FloatTensor of size 784x256]\n",
      "      (1): Parameter containing: [torch.FloatTensor of size 256x256]\n",
      "  )\n",
      "  (hidden_b_var): ParameterList(\n",
      "      (0): Parameter containing: [torch.FloatTensor of size 256x1]\n",
      "      (1): Parameter containing: [torch.FloatTensor of size 256x1]\n",
      "  )\n",
      "  (out_layers_w_mean): ParameterList()\n",
      "  (out_layers_b_mean): ParameterList()\n",
      "  (out_layers_w_var): ParameterList()\n",
      "  (out_layers_b_var): ParameterList()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as tdist\n",
    "import copy\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# FOR THE SPLITMNSIST MULTIHEAD PROBLEM\n",
    "input_size = 28*28 #size of images\n",
    "hidden_size = 256 #hidden layers size\n",
    "n_hidden_layers = 2\n",
    "output_size = 2\n",
    "n_tasks = n_tasks # also the number of heads\n",
    "init_var = 0   #This is the log(var) we use when we initiate layers. 0 works well with the other hyper parameters.\n",
    "init_var_hid = 0\n",
    "init_var_pri = 0 #Priors are initiated with this log(var)\n",
    "n_samples = 10 #The number of samples we take from the posterior distribution to sample a new weight to use when training\n",
    "\n",
    "\n",
    "torch.manual_seed(30)\n",
    "np.random.seed(30)\n",
    "\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.criterion = torch.nn.BCEWithLogitsLoss()\n",
    "        #Empty parameters to hold layers when these are added\n",
    "        \n",
    "        #notice that posteriors is Parameter list, so they get updated and priors is just a list.\n",
    "        #i may fuck this up when i add layers or something, but as long as the priors are not updated by the optimizer we are good\n",
    "        self.hidden_w_mean = torch.nn.ParameterList([])\n",
    "        self.hidden_b_mean = torch.nn.ParameterList([])\n",
    "        self.hidden_w_var = torch.nn.ParameterList([])\n",
    "        self.hidden_b_var = torch.nn.ParameterList([])\n",
    "\n",
    "        #And the priors\n",
    "        self.pri_hidden_w_mean = []\n",
    "        self.pri_hidden_b_mean = []\n",
    "        self.pri_hidden_w_var = []\n",
    "        self.pri_hidden_b_var = []\n",
    "\n",
    "        #initiating hidden layers. weights are initiatet with N(0,0.1)\n",
    "        for i in range(n_hidden_layers):\n",
    "            if i == 0:\n",
    "                inp_size = input_size\n",
    "            else:\n",
    "                inp_size = hidden_size\n",
    "            self.hidden_w_mean.append(torch.nn.Parameter(torch.randn(inp_size,hidden_size)*0.1, requires_grad=True))\n",
    "            self.hidden_b_mean.append(torch.nn.Parameter(torch.randn(hidden_size,1)*0.1, requires_grad=True))\n",
    "            self.hidden_w_var.append(torch.nn.Parameter(torch.ones(inp_size,hidden_size)*init_var_hid, requires_grad=True))\n",
    "            self.hidden_b_var.append(torch.nn.Parameter(torch.ones(hidden_size,1)*init_var_hid, requires_grad=True))\n",
    "\n",
    "            #And the priors\n",
    "            self.pri_hidden_w_mean.append(torch.zeros(inp_size,hidden_size))\n",
    "            self.pri_hidden_b_mean.append(torch.zeros(hidden_size,1))\n",
    "            self.pri_hidden_w_var.append(torch.ones(inp_size,hidden_size)*init_var_pri)\n",
    "            self.pri_hidden_b_var.append(torch.ones(hidden_size,1)*init_var_pri)\n",
    "\n",
    "       #Empty parameters to hold output layers when these are added in the add_task function. again \n",
    "        #priors will just be a list(or maybe not when we addtask() but it does not matter as long as they are not updated by optimizer)\n",
    "        self.out_layers_w_mean = torch.nn.ParameterList([])\n",
    "        self.out_layers_b_mean = torch.nn.ParameterList([])\n",
    "        self.out_layers_w_var = torch.nn.ParameterList([])\n",
    "        self.out_layers_b_var = torch.nn.ParameterList([])\n",
    "        \n",
    "        self.pri_out_layers_w_mean = None\n",
    "        self.pri_out_layers_b_mean = None\n",
    "        self.pri_out_layers_w_var = None\n",
    "        self.pri_out_layers_b_var = None\n",
    "        \n",
    "           \n",
    "    def forward(self, x, y, task_no, train=True):#y must be 1 of k encoded\n",
    "        #sample from posterior using the reparameterization trick\n",
    "        #from hidden layer\n",
    "        batch_size = x.shape[0] \n",
    "        \n",
    "        x = x.view(-1,input_size) #make it a vector\n",
    "        x = x/256\n",
    "        \n",
    "        for i in range(n_hidden_layers):\n",
    "            if train == False:   #if we are validating the network we just use the current mean of the weights. no need to sample\n",
    "                hidden_w = self.hidden_w_mean[i]\n",
    "                hidden_b = self.hidden_b_mean[i]\n",
    "            else:\n",
    "                with torch.no_grad(): #sample from N(0,1). nograd because of reparameterization trick. SEE TAHT WE take n_samples!\n",
    "                    inp_size,out_size = self.hidden_w_mean[i].size()\n",
    "                    hidden_w = torch.nn.Parameter(torch.randn(n_samples,inp_size,out_size))\n",
    "                    hidden_b = torch.nn.Parameter(torch.randn(n_samples,out_size,1))\n",
    "                \n",
    "                #WE MEAN THE SAMPLES AND generate weights from them\n",
    "                hidden_w = torch.add(self.hidden_w_mean[i],torch.mul(torch.mean(hidden_w,dim=0),torch.exp(0.5*self.hidden_w_var[i])))\n",
    "                hidden_b = torch.add(self.hidden_b_mean[i],torch.mul(torch.mean(hidden_b,dim=0),torch.exp(0.5*self.hidden_b_var[i])))\n",
    "            #then we put data through the layer. ReLU is shit when network is randomly initiatet, but sigmoid works well.(paper uses relu)\n",
    "            x = F.relu(torch.tensordot(torch.transpose(hidden_w,0,1),x,([1],[1]))+hidden_b)\n",
    "            x = torch.transpose(x,0,1)\n",
    "        \n",
    "        # WE DO the same thing as above with the output layer.\n",
    "        #here we chose the layer that maches the task_no\n",
    "        if train == False:\n",
    "            out_w = self.out_layers_w_mean[task_no]\n",
    "            out_b = self.out_layers_b_mean[task_no]\n",
    "        else:        \n",
    "            with torch.no_grad():\n",
    "                inp_size,out_size = self.out_layers_w_mean[task_no].size()\n",
    "                out_w = torch.nn.Parameter(torch.randn(n_samples,inp_size,out_size))\n",
    "                out_b = torch.nn.Parameter(torch.randn(n_samples,out_size,1))\n",
    "            out_w = torch.add(self.out_layers_w_mean[task_no],torch.mul(torch.mean(out_w,dim=0),torch.exp(0.5*self.out_layers_w_var[task_no])))\n",
    "            out_b = torch.add(self.out_layers_b_mean[task_no],torch.mul(torch.mean(out_b,dim=0),torch.exp(0.5*self.out_layers_b_var[task_no])))\n",
    "           \n",
    "        #predict\n",
    "        #print(\"x shape input\",x.shape)\n",
    "        \n",
    "        #print(\"x shape input after view\",x.shape)\n",
    "        #print(\"x shape after unsqueeze\",x.shape)\n",
    "        #print(\"x type\",x.type())\n",
    "        \n",
    "        #print(hidden_w.shape,x.shape,hidden_b.shape)\n",
    "        \n",
    "        x = torch.tensordot(torch.transpose(out_w,0,1),x,([1],[1]))+out_b\n",
    "        \n",
    "        #print(x.shape) # [2xbatch]\n",
    "        #x = torch.transpose(x,0,1)\n",
    "        #print(\"before softmax: \",x)\n",
    "        pred = x\n",
    "        #print(\"after softmax: \",pred)\n",
    "        pred = torch.transpose(pred, 0, 1)\n",
    "        #print(\"after softmax: \",pred)\n",
    "        #Eq*log(likelyhood)-Eq*KL(estimated_prior/prior)\n",
    "        \n",
    "        \n",
    "        # SEE THE loss function\n",
    "        loss, kl = self.calc_loss(pred,y,task_no,batch_size)\n",
    "        \n",
    "        \n",
    "        return pred, loss, kl\n",
    "            \n",
    "    def calc_loss(self,y,t,task_no,batch_size):#FROM EX 7.2:\n",
    "        # \n",
    "        \n",
    "        likelihood = self.criterion(y, t)\n",
    "        \n",
    "        kl = torch.tensor(0.)\n",
    "        \n",
    "        #For the KL loss i use the formula for two multivariate gaussians(but ours is just a single gaussian for each weight)\n",
    "        #for i in range(n_hidden_layers):\n",
    "        #    kl += torch.sum(0.5*(torch.exp(self.hidden_w_var[i]-self.pri_hidden_w_var[i]) + (self.pri_hidden_w_mean[i]-self.hidden_w_mean[i])**2/(torch.exp(self.pri_hidden_w_var[i]))-1+self.pri_hidden_w_var[i]-self.hidden_w_var[i]))\n",
    "        #    kl += torch.sum(0.5*(torch.exp(self.hidden_b_var[i]-self.pri_hidden_b_var[i]) + (self.pri_hidden_b_mean[i]-self.hidden_b_mean[i])**2/(torch.exp(self.pri_hidden_b_var[i]))-1+self.pri_hidden_b_var[i]-self.hidden_b_var[i]))\n",
    "        \n",
    "        ##also for output layers\n",
    "        #kl += torch.sum(0.5*(torch.exp(self.out_layers_w_var[task_no]-self.pri_out_layers_w_var[task_no]) + (self.pri_out_layers_w_mean[task_no]-self.out_layers_w_mean[task_no])**2/(torch.exp(self.pri_out_layers_w_var[task_no]))-1+self.pri_out_layers_w_var[task_no]-self.out_layers_w_var[task_no]))\n",
    "        #kl += torch.sum(0.5*(torch.exp(self.out_layers_b_var[task_no]-self.pri_out_layers_b_var[task_no]) + (self.pri_out_layers_b_mean[task_no]-self.out_layers_b_mean[task_no])**2/(torch.exp(self.pri_out_layers_b_var[task_no]))-1+self.pri_out_layers_b_var[task_no]-self.out_layers_b_var[task_no]))\n",
    "        #print(\"kl\",kl)\n",
    "        \n",
    "        ################THEIR KL ALGO\n",
    "        for i in range(n_hidden_layers):\n",
    "            if i == 0:\n",
    "                inp_size = input_size\n",
    "            else:\n",
    "                inp_size = hidden_size\n",
    "            const_term = -0.5 * hidden_size * inp_size\n",
    "            log_std_diff = 0.5 * torch.sum(self.pri_hidden_w_var[i] - self.hidden_w_var[i])\n",
    "            mu_diff_term = 0.5 * torch.sum((torch.exp(self.hidden_w_var[i]) + (self.pri_hidden_w_mean[i] - self.hidden_w_mean[i])**2) / torch.exp(self.pri_hidden_w_var[i]))\n",
    "            kl += const_term + log_std_diff + mu_diff_term\n",
    "            \n",
    "            const_term = -0.5 * hidden_size\n",
    "            log_std_diff = 0.5 * torch.sum(self.pri_hidden_b_var[i] - self.hidden_b_var[i])\n",
    "            mu_diff_term = 0.5 * torch.sum((torch.exp(self.hidden_b_var[i]) + (self.pri_hidden_b_mean[i] - self.hidden_b_mean[i])**2) / torch.exp(self.pri_hidden_b_var[i]))\n",
    "            kl += const_term + log_std_diff + mu_diff_term\n",
    "        \n",
    "        \n",
    "        const_term = -0.5 * hidden_size * output_size\n",
    "        log_std_diff = 0.5 * torch.sum(self.pri_out_layers_w_var[task_no] - self.out_layers_w_var[task_no])\n",
    "        mu_diff_term = 0.5 * torch.sum((torch.exp(self.out_layers_w_var[task_no]) + (self.pri_out_layers_w_mean[task_no] - self.out_layers_w_mean[task_no])**2) / torch.exp(self.pri_out_layers_w_var[task_no]))\n",
    "        kl += const_term + log_std_diff + mu_diff_term\n",
    "        \n",
    "        const_term = -0.5  * output_size\n",
    "        log_std_diff = 0.5 * torch.sum(self.pri_out_layers_b_var[task_no] - self.out_layers_b_var[task_no])\n",
    "        mu_diff_term = 0.5 * torch.sum((torch.exp(self.out_layers_b_var[task_no]) + (self.pri_out_layers_b_mean[task_no] - self.out_layers_b_mean[task_no])**2) / torch.exp(self.pri_out_layers_b_var[task_no]))\n",
    "        kl += const_term + log_std_diff + mu_diff_term\n",
    "        \n",
    "        kl = kl/batch_size\n",
    "        ############################\n",
    "\n",
    "        # Combining the two terms in the evidence lower bound objective (ELBO) \n",
    "        #I DONT KNOW IF THE KL needs some kind of scaling since we just added up for every weight in the network\n",
    "        ELBO = torch.add(likelihood, kl) ############################\n",
    "        \n",
    "        #print(\"elbo: \",likelihood, \"kl\", kl)\n",
    "        return ELBO, -kl\n",
    "    \n",
    "    def add_task(self):\n",
    "        if self.pri_out_layers_w_mean is None: #if we need to add the first output layer (initiates as the hidden layers)\n",
    "            self.out_layers_w_mean.append(torch.nn.Parameter(torch.randn(hidden_size,output_size)*0.1, requires_grad=True))\n",
    "            self.out_layers_b_mean.append(torch.nn.Parameter(torch.randn(output_size,1)*0.1, requires_grad=True))\n",
    "            self.out_layers_w_var.append(torch.nn.Parameter(torch.ones(hidden_size,output_size)*init_var, requires_grad=True))\n",
    "            self.out_layers_b_var.append(torch.nn.Parameter(torch.ones(output_size,1)*init_var, requires_grad=True))\n",
    "            \n",
    "            self.pri_out_layers_w_mean = [torch.nn.Parameter(torch.zeros(hidden_size,output_size), requires_grad=False)]\n",
    "            self.pri_out_layers_b_mean = [torch.nn.Parameter(torch.zeros(output_size,1), requires_grad=False)]\n",
    "            self.pri_out_layers_w_var = [torch.nn.Parameter(torch.ones(hidden_size,output_size)*init_var_pri, requires_grad=False)]\n",
    "            self.pri_out_layers_b_var = [torch.nn.Parameter(torch.ones(output_size,1)*init_var_pri, requires_grad=False)]\n",
    "            \n",
    "        else:#if it is not the first we initiate the means as the means for the prev layer, but the variance as we defined so we allow it to change more\n",
    "            self.out_layers_w_mean.append(torch.nn.Parameter(torch.randn(hidden_size,output_size)*0.1, requires_grad=True))\n",
    "            self.out_layers_b_mean.append(torch.nn.Parameter(torch.randn(output_size,1)*0.1, requires_grad=True))\n",
    "            self.out_layers_w_var.append(torch.nn.Parameter(torch.ones(hidden_size,output_size)*init_var, requires_grad=True))#initialize new head as the same as previous\n",
    "            self.out_layers_b_var.append(torch.nn.Parameter(torch.ones(output_size,1)*init_var, requires_grad=True))#initialize new head as the same as previous\n",
    "            \n",
    "            #self.out_layers_w_mean.append(torch.nn.Parameter(self.out_layers_w_mean[-1].clone().detach()))#initialize new head as the same as previous\n",
    "            #self.out_layers_b_mean.append(torch.nn.Parameter(self.out_layers_b_mean[-1].clone().detach()))#initialize new head as the same as previous\n",
    "            #self.out_layers_w_var.append(torch.nn.Parameter(self.out_layers_w_var[-1].clone().detach()))#initialize new head as the same as previous\n",
    "            #self.out_layers_b_var.append(torch.nn.Parameter(self.out_layers_b_var[-1].clone().detach()))#initialize new head as the same as previous\n",
    "            \n",
    "            #self.out_layers_w_var.append(torch.nn.Parameter(torch.ones(hidden_size,output_size)*init_var, requires_grad=True))\n",
    "            #self.out_layers_b_var.append(torch.nn.Parameter(torch.ones(output_size,1)*init_var, requires_grad=True))\n",
    "            \n",
    "            #AND THE priors as always:\n",
    "            self.pri_out_layers_w_mean.append(torch.nn.Parameter(torch.zeros(hidden_size,output_size), requires_grad=False))\n",
    "            self.pri_out_layers_b_mean.append(torch.nn.Parameter(torch.zeros(output_size,1), requires_grad=False))\n",
    "            self.pri_out_layers_w_var.append(torch.nn.Parameter(torch.ones(hidden_size,output_size)*init_var_pri, requires_grad=False))\n",
    "            self.pri_out_layers_b_var.append(torch.nn.Parameter(torch.ones(output_size,1)*init_var_pri, requires_grad=False))\n",
    "            \n",
    "            #self.pri_out_layers_w_mean.append(self.pri_out_layers_w_mean[-1])\n",
    "            #self.pri_out_layers_b_mean.append(self.pri_out_layers_b_mean[-1])\n",
    "            #self.pri_out_layers_w_var.append(self.pri_out_layers_w_var[-1])\n",
    "            #self.pri_out_layers_b_var.append(self.pri_out_layers_b_var[-1])\n",
    "            \n",
    "    def update_prior(self,task_no):#call before backward and after forward so we have used the old ones to get loss but copy the old posterior before it is updated in backward\n",
    "        for i in range(n_hidden_layers):# just copies the current mean and var into the priors in a weird way to make shure they requires grad =False\n",
    "            self.pri_hidden_w_mean[i] = self.hidden_w_mean[i].clone().detach()\n",
    "            self.pri_hidden_b_mean[i] = self.hidden_b_mean[i].clone().detach()\n",
    "            self.pri_hidden_w_var[i] = self.hidden_w_var[i].clone().detach()\n",
    "            self.pri_hidden_b_var[i] = self.hidden_b_var[i].clone().detach()\n",
    "\n",
    "            self.pri_hidden_w_mean[i].requires_grad = False\n",
    "            self.pri_hidden_b_mean[i].requires_grad = False\n",
    "            self.pri_hidden_w_var[i].requires_grad = False\n",
    "            self.pri_hidden_b_var[i].requires_grad = False\n",
    "\n",
    "        self.pri_out_layers_w_mean[task_no] = self.out_layers_w_mean[task_no].clone().detach()\n",
    "        self.pri_out_layers_b_mean[task_no] = self.out_layers_b_mean[task_no].clone().detach()\n",
    "        self.pri_out_layers_w_var[task_no] = self.out_layers_w_var[task_no].clone().detach()\n",
    "        self.pri_out_layers_b_var[task_no] = self.out_layers_b_var[task_no].clone().detach()\n",
    "            \n",
    "        self.pri_out_layers_w_mean[task_no].requires_grad = False\n",
    "        self.pri_out_layers_b_mean[task_no].requires_grad = False\n",
    "        self.pri_out_layers_w_var[task_no].requires_grad = False\n",
    "        self.pri_out_layers_b_var[task_no].requires_grad = False\n",
    "    \n",
    "    def reset_var_and_priors(self): #This only resets the first output layer\n",
    "        print(init_var_hid,init_var,init_var_pri)\n",
    "        for i in range(n_hidden_layers):\n",
    "            if i == 0:\n",
    "                inp_size = input_size\n",
    "            else:\n",
    "                inp_size = hidden_size\n",
    "            self.hidden_w_var[i] = torch.nn.Parameter(torch.ones(inp_size,hidden_size)*init_var_hid, requires_grad=True)\n",
    "            self.hidden_b_var[i] = torch.nn.Parameter(torch.ones(hidden_size,1)*init_var_hid, requires_grad=True)\n",
    "            \n",
    "            self.pri_hidden_w_mean[i] = self.hidden_w_mean[i].clone().detach()\n",
    "            self.pri_hidden_b_mean[i]= self.hidden_b_mean[i].clone().detach()\n",
    "            self.pri_hidden_w_mean[i].requires_grad = False\n",
    "            self.pri_hidden_b_mean[i].requires_grad = False\n",
    "            \n",
    "            self.pri_hidden_w_var[i] = torch.ones(inp_size,hidden_size)*init_var_pri\n",
    "            self.pri_hidden_b_var[i] = torch.ones(hidden_size,1)*init_var_pri\n",
    "\n",
    "            \n",
    "        \n",
    "        self.out_layers_w_var[0] = torch.nn.Parameter(torch.ones(hidden_size,output_size)*init_var, requires_grad=True)\n",
    "        self.out_layers_b_var[0] = torch.nn.Parameter(torch.ones(output_size,1)*init_var, requires_grad=True)\n",
    "        \n",
    "        self.pri_out_layers_w_mean[0] = self.out_layers_w_mean[0].clone().detach()\n",
    "        self.pri_out_layers_b_mean[0] = self.out_layers_b_mean[0].clone().detach()\n",
    "        self.pri_out_layers_w_mean[0].requires_grad = False\n",
    "        self.pri_out_layers_b_mean[0].requires_grad = False\n",
    "        \n",
    "        self.pri_out_layers_w_var[0] = torch.nn.Parameter(torch.ones(hidden_size,output_size)*init_var_pri, requires_grad=False)\n",
    "        self.pri_out_layers_b_var[0] = torch.nn.Parameter(torch.ones(output_size,1)*init_var_pri, requires_grad=False)\n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task:  0 \tloss:  tensor(55.8104)\n",
      "ex outvar:  tensor([-5.9917]) \tex hidvar:  tensor([-5.9630])\n",
      "ex pri outvar:  tensor([0.]) \tex pri hidvar:  tensor([0.])\n",
      "ex out m:  tensor([-0.1597]) \tex hidmean:  tensor([-0.1271])\n",
      "ex pri out m:  tensor([-0.1472]) \tex pri hidmean:  tensor([-0.1271])\n",
      "validation prec:  1974\n",
      "kl:  tensor(-331.7977, grad_fn=<NegBackward>)\n",
      "1974 2000\n",
      "num batches:  1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-0395750558f6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    121\u001b[0m                     \u001b[0mytest_set_onehot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mytest_set\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m                     \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mytest_set\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m                         \u001b[1;32mif\u001b[0m \u001b[0mytest_set\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m                             \u001b[0mytest_set_onehot\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m                         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#THE TRAINING AND VALIDATION PART\n",
    "import torch.optim as optim\n",
    "\n",
    "no_epochs = 120\n",
    "batch_size = 60000\n",
    "lr = 0.001\n",
    "\n",
    "xtest_set = [test_datasets[0].dataset.data[test_datasets[0].sub_indeces]]\n",
    "xtrain_set = torch.tensor([])\n",
    "ytest_set = [test_datasets[0].dataset.targets[test_datasets[0].sub_indeces]]\n",
    "ytrain_set = torch.tensor([])\n",
    "\n",
    "coreset_size = 100\n",
    "\n",
    "#########PRE TRAINING\n",
    "print(\"pretraining\")\n",
    "pre_batch_size = 64\n",
    "net.add_task()\n",
    "optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "xtrain_set = train_datasets[0].dataset.data[train_datasets[0].sub_indeces]\n",
    "ytrain_set = train_datasets[0].dataset.targets[train_datasets[0].sub_indeces]\n",
    "    \n",
    "for epoch in range(1):\n",
    "    print(\"pretraing epoch\", epoch)\n",
    "    num_samples = len(xtrain_set)\n",
    "    num_batches = int(np.ceil(num_samples / float(pre_batch_size)))\n",
    "    for batch in range(num_batches):\n",
    "        net.train()\n",
    "        #get indexes for current batch\n",
    "        idx = range(batch*pre_batch_size, np.minimum((batch+1)*pre_batch_size, num_samples))#indexes to use from batch\n",
    "        X_batch_tr = xtrain_set[idx]\n",
    "        y_batch_tr_raw = ytrain_set[idx]\n",
    "        #TRANSFORM Y:\n",
    "        y_batch_tr = torch.zeros((len(idx),2))\n",
    "        for i in range(len(y_batch_tr_raw)):\n",
    "            if y_batch_tr_raw[i] in [0,2,4,6,8]:\n",
    "                y_batch_tr[i,0] = 1\n",
    "            else: #i used this when i had 2 output neurons\n",
    "                y_batch_tr[i,1] = 1\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output, batch_loss, _ = net(X_batch_tr.type(torch.FloatTensor),y_batch_tr.type(torch.FloatTensor),0)#task no so we choose correct head. calls forward function.\n",
    "        batch_loss.backward() #Update weights and biases to new posterior\n",
    "        optimizer.step()            \n",
    "#DONE PRE TRAINING\n",
    "\n",
    "init_var = -6   #This is the log(var) we use when we initiate layers.\n",
    "init_var_hid = -6\n",
    "init_var_pri = 0\n",
    "\n",
    "net.reset_var_and_priors()\n",
    "\n",
    "x_coresets = []\n",
    "y_coresets = []\n",
    "\n",
    "#for plotting\n",
    "test_vali = []\n",
    "test_iter = []\n",
    "test_i = 0\n",
    "test_vali_seperate = [[],[],[],[],[]]\n",
    "\n",
    "for task_no in range(n_tasks):\n",
    "    print(\"TASK NUM: \",task_no)\n",
    "    #Get the train and test \n",
    "    xtrain_set = train_datasets[task_no].dataset.data[train_datasets[task_no].sub_indeces]\n",
    "    ytrain_set = train_datasets[task_no].dataset.targets[train_datasets[task_no].sub_indeces]\n",
    "    \n",
    "    \n",
    "    #####The coreset thing\n",
    "    if coreset_size > 0:  \n",
    "        x_coresets, y_coresets, xtrain_set, ytrain_set = k_center(x_coresets, y_coresets, xtrain_set, ytrain_set, coreset_size)\n",
    "    #####\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    if task_no != 0: #Also whe a new taks comes we append this to the test set\n",
    "        xtest_set.append(test_datasets[task_no].dataset.data[test_datasets[task_no].sub_indeces])\n",
    "        ytest_set.append(test_datasets[task_no].dataset.targets[test_datasets[task_no].sub_indeces])\n",
    "        \n",
    "        net.add_task()#initialize a new head for the network\n",
    "\n",
    "    #TRAIN\n",
    "    optimizer = optim.Adam(net.parameters(), lr=lr) #REDEFINE OPTIMIZER AFTER LAYERS ARE ADDED!\n",
    "\n",
    "    for epoch in range(no_epochs):\n",
    "        num_samples = len(xtrain_set)\n",
    "        num_batches = int(np.ceil(num_samples / float(batch_size)))\n",
    "        print(\"num batches: \",num_batches)\n",
    "        \n",
    "        indexes = list(range(num_samples))\n",
    "        np.random.shuffle(list(indexes))\n",
    "        \n",
    "        for batch in range(num_batches):\n",
    "            net.train()\n",
    "            #get indexes for current batch\n",
    "            idx = indexes[slice(batch*batch_size, np.minimum((batch+1)*batch_size, num_samples))]#indexes to use from batch\n",
    "            X_batch_tr = xtrain_set[idx]\n",
    "            y_batch_tr_raw = ytrain_set[idx]\n",
    "            #TRANSFORM Y:\n",
    "            y_batch_tr = torch.zeros((len(idx),2))\n",
    "            for i in range(len(y_batch_tr_raw)):\n",
    "                if y_batch_tr_raw[i] in [0,2,4,6,8]:\n",
    "                    y_batch_tr[i,0] = 1\n",
    "                else: #i used this when i had 2 output neurons\n",
    "                    y_batch_tr[i,1] = 1\n",
    "  \n",
    "            optimizer.zero_grad()\n",
    "            output, batch_loss, _= net(X_batch_tr.type(torch.FloatTensor),y_batch_tr.type(torch.FloatTensor),task_no)#task no so we choose correct head. calls forward function.\n",
    "            batch_loss.backward() #Update weights and biases to new posterior\n",
    "            optimizer.step()            \n",
    "            \n",
    "            if(batch%100 == 0):#THis is just for evaluation and plotting\n",
    "                #print(net)\n",
    "                net.eval()\n",
    "                vali_accu = 0\n",
    "                n_test_samples = 0\n",
    "                for i in range(len(xtest_set)):\n",
    "                    task_test_no = i\n",
    "                    \n",
    "                    ytest_set_onehot = torch.zeros((len(ytest_set[i]),2))\n",
    "                    for j in range(len(ytest_set[i])):\n",
    "                        if ytest_set[i][j] in [0,2,4,6,8]:\n",
    "                            ytest_set_onehot[j,0] = 1\n",
    "                        else:\n",
    "                            ytest_set_onehot[j,1] = 1       \n",
    "                    output, _, kl = net(xtest_set[task_test_no].type(torch.FloatTensor),ytest_set_onehot.type(torch.FloatTensor),task_test_no, False)#task no so we choose correct head. calls forward function.\n",
    "                    vali_accu += get_n_true(output,ytest_set_onehot)\n",
    "                    n_test_samples += len(ytest_set_onehot)\n",
    "                    \n",
    "                #for plotting\n",
    "                clear_output(wait=True)\n",
    "                test_vali.append(vali_accu/n_test_samples)#i is task_no\n",
    "                test_iter.append(test_i)\n",
    "                test_i += 1\n",
    "                                \n",
    "                fig = plt.figure(figsize=(12,4))\n",
    "                plt.plot(test_iter,test_vali, label='valid_accu')\n",
    "                #plt.plot(test_iter,batch_loss_list)\n",
    "                plt.legend()\n",
    "                plt.show()\n",
    "                    \n",
    "                print(\"task: \",task_no,\"\\tloss: \",batch_loss.data)\n",
    "                print(\"ex outvar: \", net.out_layers_b_var[task_no][0].data, \"\\tex hidvar: \", net.hidden_b_var[0][0].data)\n",
    "                print(\"ex pri outvar: \", net.pri_out_layers_b_var[task_no][0].data, \"\\tex pri hidvar: \", net.pri_hidden_b_var[0][0].data)\n",
    "                print(\"ex out m: \", net.out_layers_b_mean[task_no][0].data, \"\\tex hidmean: \", net.hidden_b_mean[0][0].data)\n",
    "                print(\"ex pri out m: \", net.pri_out_layers_b_mean[task_no][0].data, \"\\tex pri hidmean: \", net.pri_hidden_b_mean[0][0].data)\n",
    "                print(\"validation prec: \",vali_accu)\n",
    "                print(\"kl: \",kl)\n",
    "                print(vali_accu,n_test_samples)\n",
    "    \n",
    "    net.update_prior(task_no)\n",
    "\n",
    "                \n",
    "    #### The coreset thing: We only use the coresets when predicting and not when updating!\n",
    "    for i in range(task_no+1):\n",
    "        net_coreset = copy.deepcopy(net)\n",
    "        optimizer = optim.Adam(net_coreset.parameters(), lr=lr)\n",
    "        #Train on coreset \n",
    "        for j in range(no_epochs):\n",
    "            num_samples = len(x_coresets[i])\n",
    "            num_batches = int(np.ceil(num_samples / float(batch_size)))\n",
    "            \n",
    "            for batch in range(num_batches):\n",
    "                net_coreset.train()\n",
    "                #get indexes for current batch\n",
    "                idx = range(batch*batch_size, np.minimum((batch+1)*batch_size, num_samples))#indexes to use from batch\n",
    "                X_batch_tr = x_coresets[i][idx]\n",
    "                y_batch_tr_raw = y_coresets[i][idx]\n",
    "                #TRANSFORM Y:\n",
    "                y_batch_tr = torch.zeros((len(idx),2))\n",
    "                for k in range(len(y_batch_tr_raw)):\n",
    "                    if y_batch_tr_raw[k] in [0,2,4,6,8]:\n",
    "                        y_batch_tr[k,0] = 1\n",
    "                    else:\n",
    "                        y_batch_tr[k,1] = 1 \n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                output, batch_loss, _ = net_coreset(X_batch_tr.type(torch.FloatTensor),y_batch_tr.type(torch.FloatTensor),i)#task no so we choose correct head. calls forward function.\n",
    "                #net_coreset.update_prior(i)\n",
    "                batch_loss.backward() #Update weights and biases to new posterior\n",
    "                optimizer.step() \n",
    "        \n",
    "        #RUN MODEL ON TEST SET\n",
    "        ##Transform y\n",
    "        y_batch_test_raw = test_datasets[i].dataset.targets[test_datasets[i].sub_indeces]\n",
    "        y_batch_test = torch.zeros((len(y_batch_test_raw),2))\n",
    "        for j in range(len(y_batch_test_raw)):\n",
    "            if y_batch_test_raw[j] in [0,2,4,6,8]:\n",
    "                y_batch_test[j,0] = 1\n",
    "            else:\n",
    "                y_batch_test[j,1] = 1  \n",
    "                \n",
    "        output, _, _ = net_coreset(test_datasets[i].dataset.data[test_datasets[i].sub_indeces].type(torch.FloatTensor), y_batch_test.type(torch.FloatTensor), i, False)\n",
    "        test_vali_seperate[i].append([task_no, get_n_true(output,y_batch_test)/len(y_batch_test)])\n",
    "        \n",
    "        \n",
    "        \n",
    "    ####\n",
    "                \n",
    "\n",
    "#    #THIS is used when we make the last plot of the end results for each taks\n",
    "#    for i in range(task_no+1):\n",
    "#        y_batch_test_raw = test_datasets[i].dataset.targets[test_datasets[i].sub_indeces]\n",
    "#        #TRANSFORM Y TO one hot:\n",
    "#        y_batch_test = torch.zeros((len(y_batch_test_raw),1))\n",
    "#        for j in range(len(y_batch_test_raw)):\n",
    "#            if y_batch_test_raw[j] in [0,2,4,6,8]:\n",
    "#                y_batch_test[j,0] = 1\n",
    "#            #else:\n",
    "#            #    y_batch_test[j,1] = 1\n",
    "#        output, _, _ = net(test_datasets[i].dataset.data[test_datasets[i].sub_indeces].type(torch.FloatTensor), y_batch_test.type(torch.FloatTensor), i, False)\n",
    "#        test_vali_seperate[i].append([task_no, get_precision(output,y_batch_test)])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "for i in range(5):\n",
    "    plt.plot(np.transpose(test_vali_seperate[i])[0],np.transpose(test_vali_seperate[i])[1])\n",
    "    plt.plot(np.transpose(test_vali_seperate[i])[0],np.transpose(test_vali_seperate[i])[1],\"x\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('split_coreset100_fashion4.pickle', 'wb') as f:\n",
    "    pickle.dump(test_vali_seperate, f)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('split_coreset100_fashion4.pickle', 'rb') as f:\n",
    "   test_vali_seperate_1 = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "for i in range(5):\n",
    "    plt.plot(np.transpose(test_vali_seperate_1[i])[0],np.transpose(test_vali_seperate_1[i])[1])\n",
    "    plt.plot(np.transpose(test_vali_seperate_1[i])[0],np.transpose(test_vali_seperate_1[i])[1],\"x\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('hidden_w_mean.0', Parameter containing:\n",
      "tensor([[-7.7002e-07, -1.1125e-05,  3.5185e-05,  ..., -3.2222e-06,\n",
      "          7.7186e-06, -1.6301e-05],\n",
      "        [ 9.3675e-06,  7.3920e-07, -1.0126e-05,  ...,  2.3370e-06,\n",
      "          5.6567e-06,  8.9074e-06],\n",
      "        [-6.6875e-06, -7.5122e-06,  8.8762e-05,  ..., -3.8555e-06,\n",
      "          6.0347e-07,  1.0872e-05],\n",
      "        ...,\n",
      "        [ 1.1420e-05,  5.1225e-06, -3.0159e-05,  ...,  9.8955e-06,\n",
      "          8.8219e-06, -1.0427e-05],\n",
      "        [ 1.2787e-05,  2.9288e-05, -7.8276e-06,  ...,  6.2296e-06,\n",
      "          8.0720e-06,  1.0121e-06],\n",
      "        [ 1.3437e-06,  2.5653e-06, -9.3707e-06,  ..., -9.3047e-05,\n",
      "         -6.5004e-06, -4.8943e-08]], requires_grad=True))\n",
      "('hidden_w_mean.1', Parameter containing:\n",
      "tensor([[-4.8701e-02, -6.9072e-02,  8.6756e-04,  ..., -1.7249e-02,\n",
      "          3.9870e-05,  5.6056e-02],\n",
      "        [ 6.9138e-04, -1.3394e-02, -3.3830e-03,  ..., -4.2165e-02,\n",
      "         -5.0604e-02, -1.4167e-03],\n",
      "        [ 1.3550e-04, -4.9942e-02, -1.9222e-01,  ...,  6.1378e-02,\n",
      "         -7.5422e-02, -1.7152e-02],\n",
      "        ...,\n",
      "        [-5.6476e-03, -1.2876e-01, -9.4528e-05,  ...,  3.8299e-03,\n",
      "          3.7547e-02, -3.3125e-02],\n",
      "        [-1.9961e-04,  5.6011e-02,  1.2960e-02,  ..., -9.6351e-04,\n",
      "          3.4159e-02,  1.0245e-05],\n",
      "        [ 2.4933e-04,  6.2012e-03,  2.6712e-05,  ...,  3.8762e-06,\n",
      "         -3.8346e-02, -2.2988e-05]], requires_grad=True))\n",
      "('hidden_b_mean.0', Parameter containing:\n",
      "tensor([[ 1.2581e-01],\n",
      "        [-4.6803e-02],\n",
      "        [ 5.6033e-02],\n",
      "        [ 1.5679e-01],\n",
      "        [-7.3346e-02],\n",
      "        [-1.2936e-01],\n",
      "        [ 1.5654e-02],\n",
      "        [-3.8651e-02],\n",
      "        [-2.7798e-04],\n",
      "        [ 6.9272e-02],\n",
      "        [-3.0800e-02],\n",
      "        [ 1.2413e-01],\n",
      "        [-8.6817e-02],\n",
      "        [-1.4957e-02],\n",
      "        [ 8.1762e-02],\n",
      "        [ 1.9068e-02],\n",
      "        [-4.7071e-02],\n",
      "        [-8.2052e-02],\n",
      "        [ 6.5665e-02],\n",
      "        [ 1.6250e-03],\n",
      "        [ 3.0708e-03],\n",
      "        [ 2.1514e-02],\n",
      "        [ 3.8128e-02],\n",
      "        [ 1.2744e-01],\n",
      "        [ 3.0708e-02],\n",
      "        [ 8.2306e-02],\n",
      "        [-2.6654e-02],\n",
      "        [ 1.4618e-01],\n",
      "        [ 4.4752e-02],\n",
      "        [ 1.2021e-01],\n",
      "        [-3.9002e-02],\n",
      "        [ 1.1042e-01],\n",
      "        [ 3.0754e-02],\n",
      "        [ 4.9733e-02],\n",
      "        [ 1.2503e-02],\n",
      "        [-8.0179e-04],\n",
      "        [ 4.0790e-02],\n",
      "        [ 7.2042e-02],\n",
      "        [-5.6777e-02],\n",
      "        [-1.0622e-01],\n",
      "        [-4.9108e-02],\n",
      "        [ 5.7583e-02],\n",
      "        [ 6.4836e-02],\n",
      "        [ 4.1686e-02],\n",
      "        [-1.0524e-02],\n",
      "        [-5.8032e-02],\n",
      "        [ 1.8744e-02],\n",
      "        [ 1.9054e-01],\n",
      "        [-2.8677e-02],\n",
      "        [-7.9630e-02],\n",
      "        [-2.4808e-02],\n",
      "        [ 3.9043e-02],\n",
      "        [-9.1032e-02],\n",
      "        [ 9.8657e-02],\n",
      "        [ 5.0162e-02],\n",
      "        [-7.8749e-02],\n",
      "        [ 1.4007e-02],\n",
      "        [-1.6100e-02],\n",
      "        [ 4.6416e-02],\n",
      "        [ 8.3835e-02],\n",
      "        [-7.9778e-03],\n",
      "        [-9.5140e-04],\n",
      "        [ 2.7448e-02],\n",
      "        [-2.8122e-02],\n",
      "        [ 7.6317e-05],\n",
      "        [-3.8156e-03],\n",
      "        [-3.2740e-03],\n",
      "        [ 4.5665e-02],\n",
      "        [-8.0336e-02],\n",
      "        [-2.1350e-01],\n",
      "        [-5.4065e-02],\n",
      "        [ 1.4365e-01],\n",
      "        [ 1.3164e-02],\n",
      "        [-4.8595e-02],\n",
      "        [-2.7571e-02],\n",
      "        [-2.6803e-02],\n",
      "        [-3.4570e-02],\n",
      "        [ 2.1998e-02],\n",
      "        [ 8.6930e-02],\n",
      "        [-1.8477e-01],\n",
      "        [ 4.1033e-02],\n",
      "        [-1.1637e-02],\n",
      "        [-1.0337e-01],\n",
      "        [-5.3908e-02],\n",
      "        [ 9.6991e-02],\n",
      "        [ 2.2368e-03],\n",
      "        [ 2.4192e-02],\n",
      "        [-4.4512e-02],\n",
      "        [-1.0486e-02],\n",
      "        [-5.2991e-03],\n",
      "        [-2.3581e-02],\n",
      "        [ 5.5778e-02],\n",
      "        [-8.9640e-02],\n",
      "        [-1.5189e-02],\n",
      "        [-2.6359e-02],\n",
      "        [ 6.8399e-02],\n",
      "        [ 2.4053e-02],\n",
      "        [ 1.5279e-02],\n",
      "        [ 2.3263e-02],\n",
      "        [-2.0312e-01],\n",
      "        [-1.3382e-01],\n",
      "        [ 2.5479e-02],\n",
      "        [ 7.2784e-02],\n",
      "        [-1.2616e-02],\n",
      "        [ 1.0205e-01],\n",
      "        [ 5.4921e-02],\n",
      "        [-8.2279e-02],\n",
      "        [ 2.2924e-03],\n",
      "        [ 6.1290e-03],\n",
      "        [-7.0098e-02],\n",
      "        [ 2.9121e-02],\n",
      "        [ 2.0252e-02],\n",
      "        [-3.7536e-02],\n",
      "        [-4.5078e-02],\n",
      "        [ 5.9949e-03],\n",
      "        [-1.6042e-01],\n",
      "        [ 3.5054e-02],\n",
      "        [-2.5688e-03],\n",
      "        [ 7.3669e-02],\n",
      "        [ 5.0597e-02],\n",
      "        [ 1.0763e-01],\n",
      "        [ 5.6273e-02],\n",
      "        [ 3.6259e-02],\n",
      "        [-8.3650e-02],\n",
      "        [ 7.3876e-02],\n",
      "        [-8.9800e-02],\n",
      "        [-3.2272e-02],\n",
      "        [-1.9993e-01],\n",
      "        [ 6.1558e-02],\n",
      "        [-1.1215e-01],\n",
      "        [-1.6265e-05],\n",
      "        [ 6.2158e-02],\n",
      "        [-7.6094e-02],\n",
      "        [-1.1973e-01],\n",
      "        [-2.9476e-02],\n",
      "        [ 3.1942e-03],\n",
      "        [ 8.0395e-02],\n",
      "        [-4.0353e-02],\n",
      "        [ 7.9005e-02],\n",
      "        [ 2.7361e-02],\n",
      "        [-4.2100e-02],\n",
      "        [ 1.1449e-02],\n",
      "        [ 1.9118e-01],\n",
      "        [-3.6251e-03],\n",
      "        [ 1.1114e-01],\n",
      "        [-3.3285e-02],\n",
      "        [ 5.0968e-02],\n",
      "        [ 2.4377e-03],\n",
      "        [ 2.3741e-02],\n",
      "        [ 9.1958e-02],\n",
      "        [ 6.2021e-02],\n",
      "        [ 6.2577e-02],\n",
      "        [ 2.0412e-02],\n",
      "        [-6.2733e-02],\n",
      "        [-4.9129e-02],\n",
      "        [-5.7277e-04],\n",
      "        [-6.2523e-02],\n",
      "        [ 2.5148e-01],\n",
      "        [ 3.8998e-03],\n",
      "        [-5.3617e-02],\n",
      "        [ 1.2609e-01],\n",
      "        [ 1.5387e-01],\n",
      "        [-4.2968e-02],\n",
      "        [-4.2364e-02],\n",
      "        [ 4.3640e-02],\n",
      "        [-1.3637e-02],\n",
      "        [ 9.0729e-02],\n",
      "        [-1.3629e-02],\n",
      "        [ 1.1592e-01],\n",
      "        [ 7.3834e-02],\n",
      "        [ 6.0543e-03],\n",
      "        [-3.0035e-03],\n",
      "        [ 1.3479e-01],\n",
      "        [-4.7470e-04],\n",
      "        [ 1.2457e-01],\n",
      "        [ 2.3565e-02],\n",
      "        [ 7.3850e-02],\n",
      "        [-1.1150e-02],\n",
      "        [ 9.7916e-02],\n",
      "        [-1.3275e-01],\n",
      "        [-2.0394e-02],\n",
      "        [ 2.9424e-02],\n",
      "        [-1.0093e-01],\n",
      "        [ 3.1324e-02],\n",
      "        [-7.4731e-02],\n",
      "        [-3.7466e-02],\n",
      "        [ 4.8397e-02],\n",
      "        [ 6.5614e-02],\n",
      "        [-6.4930e-02],\n",
      "        [ 1.6264e-01],\n",
      "        [ 3.4311e-02],\n",
      "        [-1.8747e-01],\n",
      "        [-7.2621e-02],\n",
      "        [-1.2781e-02],\n",
      "        [-1.0020e-01],\n",
      "        [-9.3253e-03],\n",
      "        [ 1.0874e-02],\n",
      "        [ 2.2829e-02],\n",
      "        [ 5.4288e-03],\n",
      "        [ 5.3807e-02],\n",
      "        [ 2.4620e-02],\n",
      "        [-4.1480e-02],\n",
      "        [-1.8539e-01],\n",
      "        [-1.8910e-02],\n",
      "        [-1.4453e-02],\n",
      "        [-4.1520e-03],\n",
      "        [-1.0041e-02],\n",
      "        [-1.1183e-01],\n",
      "        [-6.7215e-03],\n",
      "        [ 7.1836e-03],\n",
      "        [ 3.9122e-03],\n",
      "        [ 1.5751e-02],\n",
      "        [ 1.8118e-02],\n",
      "        [ 1.6847e-02],\n",
      "        [ 3.0333e-02],\n",
      "        [-2.1987e-02],\n",
      "        [ 1.8484e-02],\n",
      "        [-7.3441e-02],\n",
      "        [-2.0167e-03],\n",
      "        [-3.7900e-02],\n",
      "        [ 5.9823e-03],\n",
      "        [ 7.7834e-02],\n",
      "        [ 1.3569e-01],\n",
      "        [-1.9417e-01],\n",
      "        [ 9.9923e-03],\n",
      "        [-2.6412e-02],\n",
      "        [-1.5973e-02],\n",
      "        [ 8.6447e-02],\n",
      "        [-5.8717e-02],\n",
      "        [-6.1778e-03],\n",
      "        [-1.8886e-02],\n",
      "        [ 4.8119e-02],\n",
      "        [ 3.5106e-02],\n",
      "        [-1.5969e-02],\n",
      "        [-1.4198e-01],\n",
      "        [ 6.0864e-02],\n",
      "        [-5.6029e-02],\n",
      "        [ 5.7213e-02],\n",
      "        [-8.3906e-03],\n",
      "        [ 1.4145e-01],\n",
      "        [-6.0917e-02],\n",
      "        [ 6.1581e-03],\n",
      "        [ 2.3562e-02],\n",
      "        [-1.1788e-03],\n",
      "        [-1.9619e-01],\n",
      "        [-3.0953e-02],\n",
      "        [-2.8910e-02],\n",
      "        [ 1.1091e-01],\n",
      "        [ 7.5980e-02],\n",
      "        [ 4.8837e-02],\n",
      "        [-9.1631e-02],\n",
      "        [ 2.0129e-02],\n",
      "        [ 3.3922e-02],\n",
      "        [-2.3130e-02],\n",
      "        [ 3.6244e-02],\n",
      "        [ 5.1170e-04]], requires_grad=True))\n",
      "('hidden_b_mean.1', Parameter containing:\n",
      "tensor([[-3.6312e-02],\n",
      "        [ 2.8490e-02],\n",
      "        [ 1.7232e-02],\n",
      "        [-8.2028e-04],\n",
      "        [-2.0611e-02],\n",
      "        [ 5.2917e-06],\n",
      "        [-5.1921e-03],\n",
      "        [-4.5960e-02],\n",
      "        [-6.0339e-04],\n",
      "        [ 8.2919e-04],\n",
      "        [-8.6911e-02],\n",
      "        [ 5.5099e-02],\n",
      "        [ 3.9794e-02],\n",
      "        [ 9.0809e-04],\n",
      "        [ 1.9659e-02],\n",
      "        [-5.9163e-03],\n",
      "        [ 1.5546e-01],\n",
      "        [ 1.0398e-01],\n",
      "        [ 5.0915e-02],\n",
      "        [ 2.4693e-04],\n",
      "        [-6.1373e-02],\n",
      "        [-9.2126e-03],\n",
      "        [ 5.1024e-02],\n",
      "        [ 2.1770e-02],\n",
      "        [ 8.0736e-02],\n",
      "        [-1.4892e-03],\n",
      "        [-1.4570e-02],\n",
      "        [-1.4143e-02],\n",
      "        [ 7.5381e-04],\n",
      "        [-1.1055e-02],\n",
      "        [ 9.1714e-05],\n",
      "        [ 2.3212e-01],\n",
      "        [-1.3751e-01],\n",
      "        [-1.0486e-01],\n",
      "        [ 2.5238e-02],\n",
      "        [ 1.7165e-01],\n",
      "        [ 8.6688e-02],\n",
      "        [-5.2379e-02],\n",
      "        [-3.4369e-02],\n",
      "        [ 3.0196e-02],\n",
      "        [ 9.4098e-02],\n",
      "        [-1.4265e-01],\n",
      "        [ 3.8996e-02],\n",
      "        [ 1.5041e-02],\n",
      "        [-1.4892e-01],\n",
      "        [ 1.6451e-01],\n",
      "        [-1.1236e-02],\n",
      "        [-2.6780e-03],\n",
      "        [-8.5430e-02],\n",
      "        [ 5.0094e-02],\n",
      "        [-7.1993e-03],\n",
      "        [ 2.2133e-04],\n",
      "        [ 6.8571e-02],\n",
      "        [ 5.4122e-02],\n",
      "        [ 4.9634e-03],\n",
      "        [ 2.6668e-04],\n",
      "        [-3.0734e-02],\n",
      "        [ 5.7961e-02],\n",
      "        [ 3.7422e-02],\n",
      "        [-1.2301e-02],\n",
      "        [-1.1103e-02],\n",
      "        [-2.7504e-02],\n",
      "        [-8.7961e-02],\n",
      "        [ 7.0205e-03],\n",
      "        [-1.0324e-04],\n",
      "        [ 2.9333e-02],\n",
      "        [-1.8926e-02],\n",
      "        [-3.0135e-02],\n",
      "        [-7.0762e-02],\n",
      "        [-5.2723e-02],\n",
      "        [ 5.1823e-02],\n",
      "        [-4.0588e-03],\n",
      "        [-4.3188e-02],\n",
      "        [-3.4668e-02],\n",
      "        [-7.3843e-03],\n",
      "        [-1.0768e-01],\n",
      "        [-5.3480e-02],\n",
      "        [-1.6077e-02],\n",
      "        [ 2.1873e-02],\n",
      "        [-5.3538e-02],\n",
      "        [-5.6250e-03],\n",
      "        [ 3.6169e-02],\n",
      "        [ 9.9156e-02],\n",
      "        [ 4.7618e-02],\n",
      "        [ 1.0213e-01],\n",
      "        [-4.7445e-02],\n",
      "        [-2.8014e-05],\n",
      "        [ 5.6816e-02],\n",
      "        [-3.9200e-03],\n",
      "        [-1.3610e-01],\n",
      "        [ 1.6939e-04],\n",
      "        [-1.3684e-01],\n",
      "        [-2.1485e-01],\n",
      "        [ 1.0958e-01],\n",
      "        [ 3.8358e-02],\n",
      "        [ 7.6454e-02],\n",
      "        [-1.1474e-02],\n",
      "        [ 2.0180e-02],\n",
      "        [-1.8300e-02],\n",
      "        [-1.9262e-01],\n",
      "        [ 5.5978e-03],\n",
      "        [ 8.1850e-03],\n",
      "        [ 3.8791e-03],\n",
      "        [ 9.7516e-02],\n",
      "        [-1.0050e-02],\n",
      "        [ 1.2541e-02],\n",
      "        [ 8.8610e-02],\n",
      "        [ 5.4529e-02],\n",
      "        [ 5.5793e-02],\n",
      "        [-1.2293e-02],\n",
      "        [ 8.2965e-03],\n",
      "        [ 3.8768e-02],\n",
      "        [-4.0867e-02],\n",
      "        [-4.2411e-02],\n",
      "        [ 8.2871e-02],\n",
      "        [ 5.6029e-03],\n",
      "        [ 7.9852e-03],\n",
      "        [ 2.2684e-01],\n",
      "        [-8.4999e-02],\n",
      "        [ 2.8090e-02],\n",
      "        [-1.1743e-01],\n",
      "        [ 3.1628e-03],\n",
      "        [ 6.0619e-02],\n",
      "        [ 1.6872e-03],\n",
      "        [-1.2819e-02],\n",
      "        [-2.9211e-02],\n",
      "        [ 1.1010e-01],\n",
      "        [-8.6713e-02],\n",
      "        [-9.5553e-02],\n",
      "        [ 8.2845e-02],\n",
      "        [-7.3256e-02],\n",
      "        [ 1.5994e-02],\n",
      "        [-7.0682e-02],\n",
      "        [ 5.1481e-02],\n",
      "        [ 4.1300e-02],\n",
      "        [-2.6187e-02],\n",
      "        [-4.4337e-02],\n",
      "        [-1.0265e-02],\n",
      "        [-1.2699e-03],\n",
      "        [-6.1088e-03],\n",
      "        [ 4.4475e-02],\n",
      "        [ 4.9197e-02],\n",
      "        [ 5.8605e-02],\n",
      "        [ 2.1622e-02],\n",
      "        [ 2.2149e-05],\n",
      "        [ 1.0019e-02],\n",
      "        [-4.0957e-03],\n",
      "        [ 1.3224e-02],\n",
      "        [-1.7268e-02],\n",
      "        [ 9.9644e-02],\n",
      "        [-2.3726e-02],\n",
      "        [ 5.4157e-03],\n",
      "        [-8.2303e-03],\n",
      "        [ 3.4480e-02],\n",
      "        [ 9.0135e-05],\n",
      "        [-4.5870e-02],\n",
      "        [ 1.7120e-04],\n",
      "        [-1.9764e-02],\n",
      "        [ 7.0180e-02],\n",
      "        [ 4.6269e-02],\n",
      "        [ 3.7916e-03],\n",
      "        [-7.2907e-03],\n",
      "        [ 2.4986e-02],\n",
      "        [-3.1198e-02],\n",
      "        [-9.3255e-02],\n",
      "        [ 3.9664e-06],\n",
      "        [ 3.6444e-04],\n",
      "        [-2.3269e-02],\n",
      "        [-6.1975e-02],\n",
      "        [-2.6172e-02],\n",
      "        [-2.4171e-02],\n",
      "        [ 2.6749e-01],\n",
      "        [ 2.0323e-02],\n",
      "        [ 3.1143e-02],\n",
      "        [ 3.5388e-02],\n",
      "        [ 3.7654e-02],\n",
      "        [ 1.4578e-02],\n",
      "        [ 1.9712e-02],\n",
      "        [ 2.0862e-01],\n",
      "        [ 1.5437e-01],\n",
      "        [ 4.3546e-02],\n",
      "        [-6.3923e-02],\n",
      "        [ 9.5858e-02],\n",
      "        [-1.5360e-01],\n",
      "        [ 4.2456e-02],\n",
      "        [-7.5079e-02],\n",
      "        [ 4.7066e-02],\n",
      "        [-1.1735e-01],\n",
      "        [-6.5835e-02],\n",
      "        [-2.1895e-01],\n",
      "        [ 5.7294e-02],\n",
      "        [ 1.1232e-01],\n",
      "        [-8.3381e-02],\n",
      "        [-1.8704e-02],\n",
      "        [-4.0886e-02],\n",
      "        [-1.2013e-01],\n",
      "        [-7.9746e-02],\n",
      "        [-5.4271e-02],\n",
      "        [-4.9910e-02],\n",
      "        [ 1.0582e-01],\n",
      "        [ 4.3431e-02],\n",
      "        [ 2.1714e-02],\n",
      "        [ 3.2109e-02],\n",
      "        [-2.0517e-02],\n",
      "        [-1.0546e-06],\n",
      "        [ 3.2413e-04],\n",
      "        [-1.8422e-02],\n",
      "        [ 3.0385e-04],\n",
      "        [-4.9367e-02],\n",
      "        [ 1.1012e-01],\n",
      "        [ 3.7239e-03],\n",
      "        [-4.0507e-02],\n",
      "        [-3.2552e-02],\n",
      "        [ 2.6026e-02],\n",
      "        [-3.0286e-05],\n",
      "        [ 4.2261e-02],\n",
      "        [-4.6734e-02],\n",
      "        [ 1.8901e-03],\n",
      "        [ 4.2066e-02],\n",
      "        [-7.0516e-05],\n",
      "        [ 3.0497e-02],\n",
      "        [-2.0328e-02],\n",
      "        [-3.2713e-02],\n",
      "        [ 3.0314e-04],\n",
      "        [ 1.8779e-02],\n",
      "        [-3.4621e-02],\n",
      "        [-2.4500e-01],\n",
      "        [ 8.7792e-03],\n",
      "        [ 8.9310e-02],\n",
      "        [ 1.0115e-01],\n",
      "        [-2.0192e-02],\n",
      "        [-4.9037e-03],\n",
      "        [ 5.1146e-02],\n",
      "        [ 4.6662e-02],\n",
      "        [-4.5354e-02],\n",
      "        [-1.9046e-02],\n",
      "        [-7.4044e-03],\n",
      "        [-1.1601e-01],\n",
      "        [-1.6050e-01],\n",
      "        [-9.5554e-05],\n",
      "        [ 7.9434e-02],\n",
      "        [ 1.2204e-01],\n",
      "        [ 8.7203e-03],\n",
      "        [-3.7262e-02],\n",
      "        [ 1.4124e-02],\n",
      "        [ 6.1389e-05],\n",
      "        [-1.3399e-01],\n",
      "        [-3.9269e-03],\n",
      "        [ 5.3738e-02],\n",
      "        [-9.8267e-02],\n",
      "        [ 7.6763e-02],\n",
      "        [ 1.8124e-04],\n",
      "        [-3.0153e-02],\n",
      "        [ 1.7637e-02],\n",
      "        [ 2.8279e-04],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        [ 6.7101e-03]], requires_grad=True))\n",
      "('hidden_w_var.0', Parameter containing:\n",
      "tensor([[-5.8802, -5.8802, -5.8802,  ..., -5.8802, -5.8802, -5.8802],\n",
      "        [-5.8802, -5.8802, -5.8802,  ..., -5.8802, -5.8802, -5.8802],\n",
      "        [-5.8802, -5.8802, -5.8802,  ..., -5.8802, -5.8802, -5.8802],\n",
      "        ...,\n",
      "        [-5.8802, -5.8802, -5.8802,  ..., -5.8802, -5.8802, -5.8802],\n",
      "        [-5.8802, -5.8802, -5.8802,  ..., -5.8802, -5.8802, -5.8802],\n",
      "        [-5.8802, -5.8802, -5.8802,  ..., -5.8802, -5.8802, -5.8802]],\n",
      "       requires_grad=True))\n",
      "('hidden_w_var.1', Parameter containing:\n",
      "tensor([[-5.8801, -5.8784, -5.8801,  ..., -5.8815, -5.8802, -5.8799],\n",
      "        [-5.8800, -5.8802, -5.8801,  ..., -5.8787, -5.8800, -5.8807],\n",
      "        [-5.8802, -5.8803, -5.8802,  ..., -5.8801, -5.8802, -5.8801],\n",
      "        ...,\n",
      "        [-5.8803, -5.8811, -5.8801,  ..., -5.8796, -5.8801, -5.8806],\n",
      "        [-5.8802, -5.8802, -5.8802,  ..., -5.8802, -5.8802, -5.8802],\n",
      "        [-5.8802, -5.8801, -5.8802,  ..., -5.8802, -5.8802, -5.8802]],\n",
      "       requires_grad=True))\n",
      "('hidden_b_var.0', Parameter containing:\n",
      "tensor([[-5.8710],\n",
      "        [-5.8713],\n",
      "        [-5.8802],\n",
      "        [-5.8796],\n",
      "        [-5.8801],\n",
      "        [-5.8805],\n",
      "        [-5.8778],\n",
      "        [-5.8797],\n",
      "        [-5.8696],\n",
      "        [-5.8805],\n",
      "        [-5.8817],\n",
      "        [-5.8749],\n",
      "        [-5.8819],\n",
      "        [-5.8794],\n",
      "        [-5.8729],\n",
      "        [-5.8823],\n",
      "        [-5.8800],\n",
      "        [-5.8743],\n",
      "        [-5.8743],\n",
      "        [-5.8803],\n",
      "        [-5.8800],\n",
      "        [-5.8841],\n",
      "        [-5.8686],\n",
      "        [-5.8775],\n",
      "        [-5.8771],\n",
      "        [-5.8768],\n",
      "        [-5.8761],\n",
      "        [-5.8799],\n",
      "        [-5.8808],\n",
      "        [-5.8790],\n",
      "        [-5.8811],\n",
      "        [-5.8841],\n",
      "        [-5.8792],\n",
      "        [-5.8750],\n",
      "        [-5.8802],\n",
      "        [-5.8800],\n",
      "        [-5.8824],\n",
      "        [-5.8803],\n",
      "        [-5.8757],\n",
      "        [-5.8802],\n",
      "        [-5.8747],\n",
      "        [-5.8739],\n",
      "        [-5.8751],\n",
      "        [-5.8771],\n",
      "        [-5.8613],\n",
      "        [-5.8805],\n",
      "        [-5.8814],\n",
      "        [-5.8795],\n",
      "        [-5.8753],\n",
      "        [-5.8805],\n",
      "        [-5.8784],\n",
      "        [-5.8831],\n",
      "        [-5.8701],\n",
      "        [-5.8649],\n",
      "        [-5.8762],\n",
      "        [-5.8824],\n",
      "        [-5.8800],\n",
      "        [-5.8801],\n",
      "        [-5.8740],\n",
      "        [-5.8728],\n",
      "        [-5.8846],\n",
      "        [-5.8814],\n",
      "        [-5.8782],\n",
      "        [-5.8827],\n",
      "        [-5.8808],\n",
      "        [-5.8804],\n",
      "        [-5.8825],\n",
      "        [-5.8755],\n",
      "        [-5.8738],\n",
      "        [-5.8739],\n",
      "        [-5.8783],\n",
      "        [-5.8866],\n",
      "        [-5.8807],\n",
      "        [-5.8798],\n",
      "        [-5.8781],\n",
      "        [-5.8802],\n",
      "        [-5.8802],\n",
      "        [-5.8803],\n",
      "        [-5.8801],\n",
      "        [-5.8794],\n",
      "        [-5.8853],\n",
      "        [-5.8809],\n",
      "        [-5.8781],\n",
      "        [-5.8736],\n",
      "        [-5.8685],\n",
      "        [-5.8785],\n",
      "        [-5.8791],\n",
      "        [-5.8802],\n",
      "        [-5.8792],\n",
      "        [-5.8780],\n",
      "        [-5.8807],\n",
      "        [-5.8780],\n",
      "        [-5.8777],\n",
      "        [-5.8790],\n",
      "        [-5.8681],\n",
      "        [-5.8797],\n",
      "        [-5.8818],\n",
      "        [-5.8802],\n",
      "        [-5.8710],\n",
      "        [-5.8799],\n",
      "        [-5.8835],\n",
      "        [-5.8837],\n",
      "        [-5.8849],\n",
      "        [-5.8808],\n",
      "        [-5.8873],\n",
      "        [-5.8791],\n",
      "        [-5.8706],\n",
      "        [-5.8802],\n",
      "        [-5.8808],\n",
      "        [-5.8908],\n",
      "        [-5.8791],\n",
      "        [-5.8780],\n",
      "        [-5.8805],\n",
      "        [-5.8720],\n",
      "        [-5.8809],\n",
      "        [-5.8756],\n",
      "        [-5.8801],\n",
      "        [-5.8803],\n",
      "        [-5.8805],\n",
      "        [-5.8803],\n",
      "        [-5.8785],\n",
      "        [-5.8759],\n",
      "        [-5.8830],\n",
      "        [-5.8836],\n",
      "        [-5.8762],\n",
      "        [-5.8777],\n",
      "        [-5.8803],\n",
      "        [-5.8804],\n",
      "        [-5.8711],\n",
      "        [-5.8781],\n",
      "        [-5.8801],\n",
      "        [-5.8829],\n",
      "        [-5.8804],\n",
      "        [-5.8801],\n",
      "        [-5.8801],\n",
      "        [-5.8803],\n",
      "        [-5.8805],\n",
      "        [-5.8791],\n",
      "        [-5.8797],\n",
      "        [-5.8886],\n",
      "        [-5.8818],\n",
      "        [-5.8834],\n",
      "        [-5.8768],\n",
      "        [-5.8802],\n",
      "        [-5.8840],\n",
      "        [-5.8803],\n",
      "        [-5.8779],\n",
      "        [-5.8801],\n",
      "        [-5.8801],\n",
      "        [-5.8646],\n",
      "        [-5.8715],\n",
      "        [-5.8769],\n",
      "        [-5.8891],\n",
      "        [-5.8765],\n",
      "        [-5.8814],\n",
      "        [-5.8798],\n",
      "        [-5.8747],\n",
      "        [-5.8718],\n",
      "        [-5.8806],\n",
      "        [-5.8816],\n",
      "        [-5.8796],\n",
      "        [-5.8868],\n",
      "        [-5.8896],\n",
      "        [-5.8704],\n",
      "        [-5.8849],\n",
      "        [-5.8807],\n",
      "        [-5.8728],\n",
      "        [-5.8790],\n",
      "        [-5.8771],\n",
      "        [-5.8787],\n",
      "        [-5.8822],\n",
      "        [-5.8810],\n",
      "        [-5.8701],\n",
      "        [-5.8806],\n",
      "        [-5.8749],\n",
      "        [-5.8671],\n",
      "        [-5.8796],\n",
      "        [-5.8717],\n",
      "        [-5.8755],\n",
      "        [-5.8751],\n",
      "        [-5.8746],\n",
      "        [-5.8795],\n",
      "        [-5.8757],\n",
      "        [-5.8768],\n",
      "        [-5.8802],\n",
      "        [-5.8816],\n",
      "        [-5.8889],\n",
      "        [-5.8758],\n",
      "        [-5.8799],\n",
      "        [-5.8699],\n",
      "        [-5.8720],\n",
      "        [-5.8812],\n",
      "        [-5.8802],\n",
      "        [-5.8823],\n",
      "        [-5.8824],\n",
      "        [-5.8802],\n",
      "        [-5.8822],\n",
      "        [-5.8899],\n",
      "        [-5.8816],\n",
      "        [-5.8752],\n",
      "        [-5.8797],\n",
      "        [-5.8804],\n",
      "        [-5.8850],\n",
      "        [-5.8803],\n",
      "        [-5.8792],\n",
      "        [-5.8780],\n",
      "        [-5.8790],\n",
      "        [-5.8801],\n",
      "        [-5.8724],\n",
      "        [-5.8811],\n",
      "        [-5.8802],\n",
      "        [-5.8848],\n",
      "        [-5.8749],\n",
      "        [-5.8791],\n",
      "        [-5.8832],\n",
      "        [-5.8836],\n",
      "        [-5.8800],\n",
      "        [-5.8750],\n",
      "        [-5.8805],\n",
      "        [-5.8807],\n",
      "        [-5.8794],\n",
      "        [-5.8754],\n",
      "        [-5.8745],\n",
      "        [-5.8815],\n",
      "        [-5.8768],\n",
      "        [-5.8774],\n",
      "        [-5.8800],\n",
      "        [-5.8791],\n",
      "        [-5.8762],\n",
      "        [-5.8722],\n",
      "        [-5.8822],\n",
      "        [-5.8804],\n",
      "        [-5.8818],\n",
      "        [-5.8808],\n",
      "        [-5.8805],\n",
      "        [-5.8808],\n",
      "        [-5.8795],\n",
      "        [-5.8805],\n",
      "        [-5.8804],\n",
      "        [-5.8822],\n",
      "        [-5.8801],\n",
      "        [-5.8825],\n",
      "        [-5.8802],\n",
      "        [-5.8800],\n",
      "        [-5.8803],\n",
      "        [-5.8803],\n",
      "        [-5.8793],\n",
      "        [-5.8743],\n",
      "        [-5.8818],\n",
      "        [-5.8804],\n",
      "        [-5.8746],\n",
      "        [-5.8783],\n",
      "        [-5.8804],\n",
      "        [-5.8840],\n",
      "        [-5.8802],\n",
      "        [-5.8802]], requires_grad=True))\n",
      "('hidden_b_var.1', Parameter containing:\n",
      "tensor([[-5.8800],\n",
      "        [-5.8779],\n",
      "        [-5.8802],\n",
      "        [-5.8802],\n",
      "        [-5.8803],\n",
      "        [-5.8802],\n",
      "        [-5.8789],\n",
      "        [-5.8758],\n",
      "        [-5.8806],\n",
      "        [-5.8771],\n",
      "        [-5.8794],\n",
      "        [-5.8685],\n",
      "        [-5.8793],\n",
      "        [-5.8805],\n",
      "        [-5.8792],\n",
      "        [-5.8796],\n",
      "        [-5.8723],\n",
      "        [-5.8798],\n",
      "        [-5.8798],\n",
      "        [-5.8802],\n",
      "        [-5.8802],\n",
      "        [-5.8796],\n",
      "        [-5.8796],\n",
      "        [-5.8773],\n",
      "        [-5.8775],\n",
      "        [-5.8717],\n",
      "        [-5.8814],\n",
      "        [-5.8806],\n",
      "        [-5.8801],\n",
      "        [-5.8802],\n",
      "        [-5.8802],\n",
      "        [-5.8743],\n",
      "        [-5.8802],\n",
      "        [-5.8763],\n",
      "        [-5.8764],\n",
      "        [-5.8726],\n",
      "        [-5.8754],\n",
      "        [-5.8795],\n",
      "        [-5.8804],\n",
      "        [-5.8795],\n",
      "        [-5.8725],\n",
      "        [-5.8837],\n",
      "        [-5.8761],\n",
      "        [-5.8816],\n",
      "        [-5.8817],\n",
      "        [-5.8809],\n",
      "        [-5.8768],\n",
      "        [-5.8670],\n",
      "        [-5.8861],\n",
      "        [-5.8800],\n",
      "        [-5.8787],\n",
      "        [-5.8802],\n",
      "        [-5.8792],\n",
      "        [-5.8730],\n",
      "        [-5.8824],\n",
      "        [-5.8802],\n",
      "        [-5.8788],\n",
      "        [-5.8782],\n",
      "        [-5.8801],\n",
      "        [-5.8796],\n",
      "        [-5.8802],\n",
      "        [-5.8772],\n",
      "        [-5.8790],\n",
      "        [-5.8802],\n",
      "        [-5.8801],\n",
      "        [-5.8799],\n",
      "        [-5.8813],\n",
      "        [-5.8805],\n",
      "        [-5.8805],\n",
      "        [-5.8805],\n",
      "        [-5.8827],\n",
      "        [-5.8802],\n",
      "        [-5.8795],\n",
      "        [-5.8779],\n",
      "        [-5.8802],\n",
      "        [-5.8809],\n",
      "        [-5.8805],\n",
      "        [-5.8843],\n",
      "        [-5.8783],\n",
      "        [-5.8802],\n",
      "        [-5.8796],\n",
      "        [-5.8804],\n",
      "        [-5.8818],\n",
      "        [-5.8791],\n",
      "        [-5.8753],\n",
      "        [-5.8812],\n",
      "        [-5.8802],\n",
      "        [-5.8788],\n",
      "        [-5.8798],\n",
      "        [-5.8770],\n",
      "        [-5.8802],\n",
      "        [-5.8763],\n",
      "        [-5.8750],\n",
      "        [-5.8819],\n",
      "        [-5.8805],\n",
      "        [-5.8821],\n",
      "        [-5.8766],\n",
      "        [-5.8782],\n",
      "        [-5.8801],\n",
      "        [-5.8802],\n",
      "        [-5.8792],\n",
      "        [-5.8802],\n",
      "        [-5.8802],\n",
      "        [-5.8780],\n",
      "        [-5.8809],\n",
      "        [-5.8797],\n",
      "        [-5.8743],\n",
      "        [-5.8750],\n",
      "        [-5.8772],\n",
      "        [-5.8802],\n",
      "        [-5.8802],\n",
      "        [-5.8801],\n",
      "        [-5.8784],\n",
      "        [-5.8802],\n",
      "        [-5.8788],\n",
      "        [-5.8814],\n",
      "        [-5.8775],\n",
      "        [-5.8770],\n",
      "        [-5.8792],\n",
      "        [-5.8787],\n",
      "        [-5.8720],\n",
      "        [-5.8802],\n",
      "        [-5.8799],\n",
      "        [-5.8802],\n",
      "        [-5.8802],\n",
      "        [-5.8801],\n",
      "        [-5.8807],\n",
      "        [-5.8800],\n",
      "        [-5.8810],\n",
      "        [-5.8798],\n",
      "        [-5.8797],\n",
      "        [-5.8836],\n",
      "        [-5.8764],\n",
      "        [-5.8794],\n",
      "        [-5.8800],\n",
      "        [-5.8779],\n",
      "        [-5.8770],\n",
      "        [-5.8802],\n",
      "        [-5.8802],\n",
      "        [-5.8802],\n",
      "        [-5.8840],\n",
      "        [-5.8817],\n",
      "        [-5.8802],\n",
      "        [-5.8760],\n",
      "        [-5.8802],\n",
      "        [-5.8802],\n",
      "        [-5.8807],\n",
      "        [-5.8805],\n",
      "        [-5.8784],\n",
      "        [-5.8779],\n",
      "        [-5.8810],\n",
      "        [-5.8804],\n",
      "        [-5.8802],\n",
      "        [-5.8798],\n",
      "        [-5.8802],\n",
      "        [-5.8812],\n",
      "        [-5.8802],\n",
      "        [-5.8800],\n",
      "        [-5.8820],\n",
      "        [-5.8812],\n",
      "        [-5.8799],\n",
      "        [-5.8799],\n",
      "        [-5.8790],\n",
      "        [-5.8720],\n",
      "        [-5.8766],\n",
      "        [-5.8802],\n",
      "        [-5.8806],\n",
      "        [-5.8798],\n",
      "        [-5.8800],\n",
      "        [-5.8767],\n",
      "        [-5.8731],\n",
      "        [-5.8847],\n",
      "        [-5.8799],\n",
      "        [-5.8791],\n",
      "        [-5.8786],\n",
      "        [-5.8781],\n",
      "        [-5.8780],\n",
      "        [-5.8802],\n",
      "        [-5.8812],\n",
      "        [-5.8800],\n",
      "        [-5.8812],\n",
      "        [-5.8799],\n",
      "        [-5.8796],\n",
      "        [-5.8789],\n",
      "        [-5.8795],\n",
      "        [-5.8797],\n",
      "        [-5.8753],\n",
      "        [-5.8742],\n",
      "        [-5.8796],\n",
      "        [-5.8785],\n",
      "        [-5.8727],\n",
      "        [-5.8782],\n",
      "        [-5.8768],\n",
      "        [-5.8703],\n",
      "        [-5.8811],\n",
      "        [-5.8804],\n",
      "        [-5.8807],\n",
      "        [-5.8800],\n",
      "        [-5.8783],\n",
      "        [-5.8796],\n",
      "        [-5.8789],\n",
      "        [-5.8815],\n",
      "        [-5.8842],\n",
      "        [-5.8786],\n",
      "        [-5.8802],\n",
      "        [-5.8802],\n",
      "        [-5.8807],\n",
      "        [-5.8802],\n",
      "        [-5.8810],\n",
      "        [-5.8736],\n",
      "        [-5.8805],\n",
      "        [-5.8805],\n",
      "        [-5.8802],\n",
      "        [-5.8829],\n",
      "        [-5.8802],\n",
      "        [-5.8843],\n",
      "        [-5.8808],\n",
      "        [-5.8797],\n",
      "        [-5.8803],\n",
      "        [-5.8804],\n",
      "        [-5.8815],\n",
      "        [-5.8816],\n",
      "        [-5.8801],\n",
      "        [-5.8802],\n",
      "        [-5.8808],\n",
      "        [-5.8799],\n",
      "        [-5.8782],\n",
      "        [-5.8759],\n",
      "        [-5.8804],\n",
      "        [-5.8769],\n",
      "        [-5.8799],\n",
      "        [-5.8802],\n",
      "        [-5.8892],\n",
      "        [-5.8805],\n",
      "        [-5.8828],\n",
      "        [-5.8809],\n",
      "        [-5.8771],\n",
      "        [-5.8806],\n",
      "        [-5.8820],\n",
      "        [-5.8805],\n",
      "        [-5.8801],\n",
      "        [-5.8799],\n",
      "        [-5.8802],\n",
      "        [-5.8811],\n",
      "        [-5.8779],\n",
      "        [-5.8802],\n",
      "        [-5.8777],\n",
      "        [-5.8731],\n",
      "        [-5.8771],\n",
      "        [-5.8793],\n",
      "        [-5.8805],\n",
      "        [-5.8802],\n",
      "        [-5.8799],\n",
      "        [-5.8806],\n",
      "        [-5.8801],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        [-5.8782]], requires_grad=True))\n",
      "('out_layers_w_mean.0', Parameter containing:\n",
      "tensor([[ 2.0165e-03],\n",
      "        [-1.7265e-01],\n",
      "        [-1.7662e-03],\n",
      "        [ 6.0971e-05],\n",
      "        [-2.1901e-02],\n",
      "        [-1.1287e-05],\n",
      "        [-4.3244e-02],\n",
      "        [-2.7533e-01],\n",
      "        [ 1.0805e-02],\n",
      "        [-1.5046e-01],\n",
      "        [-2.3073e-01],\n",
      "        [ 2.2659e-01],\n",
      "        [ 1.8993e-01],\n",
      "        [ 4.7582e-02],\n",
      "        [ 3.1509e-02],\n",
      "        [ 2.3535e-02],\n",
      "        [-2.1575e-01],\n",
      "        [-7.2218e-02],\n",
      "        [-9.3406e-03],\n",
      "        [ 4.8632e-02],\n",
      "        [-1.6986e-02],\n",
      "        [-1.0963e-01],\n",
      "        [-4.5792e-02],\n",
      "        [-1.0618e-01],\n",
      "        [-1.0393e-01],\n",
      "        [-2.4567e-01],\n",
      "        [ 4.0582e-02],\n",
      "        [-1.6268e-01],\n",
      "        [ 7.7381e-03],\n",
      "        [-3.3809e-03],\n",
      "        [-3.3744e-05],\n",
      "        [-1.5984e-01],\n",
      "        [ 1.8125e-02],\n",
      "        [ 1.4298e-01],\n",
      "        [-1.8286e-01],\n",
      "        [ 1.7189e-01],\n",
      "        [-2.0789e-01],\n",
      "        [ 1.0886e-01],\n",
      "        [ 1.2701e-02],\n",
      "        [ 2.7566e-02],\n",
      "        [ 1.9671e-01],\n",
      "        [ 3.3711e-01],\n",
      "        [ 1.4722e-01],\n",
      "        [ 6.0803e-02],\n",
      "        [ 2.1356e-01],\n",
      "        [-1.0040e-01],\n",
      "        [ 1.0812e-01],\n",
      "        [ 1.9492e-01],\n",
      "        [ 1.7953e-01],\n",
      "        [-1.4573e-02],\n",
      "        [ 5.3436e-02],\n",
      "        [-2.1415e-02],\n",
      "        [ 4.2533e-02],\n",
      "        [-2.0225e-01],\n",
      "        [-1.1261e-01],\n",
      "        [-9.3869e-04],\n",
      "        [ 2.8272e-02],\n",
      "        [ 8.5332e-02],\n",
      "        [-3.5247e-02],\n",
      "        [ 3.0079e-02],\n",
      "        [ 1.2146e-02],\n",
      "        [ 3.6067e-02],\n",
      "        [ 1.1110e-01],\n",
      "        [-6.0333e-03],\n",
      "        [ 7.4176e-03],\n",
      "        [-1.3151e-01],\n",
      "        [ 5.5640e-02],\n",
      "        [-3.8252e-02],\n",
      "        [-3.2399e-02],\n",
      "        [ 2.3668e-02],\n",
      "        [-4.8376e-02],\n",
      "        [-3.4830e-04],\n",
      "        [ 2.7197e-02],\n",
      "        [ 7.8716e-02],\n",
      "        [-1.2796e-02],\n",
      "        [-1.6526e-01],\n",
      "        [-8.2087e-02],\n",
      "        [-1.3573e-01],\n",
      "        [-1.1565e-01],\n",
      "        [ 3.2209e-05],\n",
      "        [-4.0873e-02],\n",
      "        [ 1.2867e-02],\n",
      "        [-1.4457e-01],\n",
      "        [ 4.1871e-02],\n",
      "        [-1.5032e-01],\n",
      "        [ 9.7116e-02],\n",
      "        [-5.6539e-05],\n",
      "        [ 2.9614e-01],\n",
      "        [-4.1324e-02],\n",
      "        [ 2.0103e-01],\n",
      "        [ 7.8375e-04],\n",
      "        [-1.8988e-01],\n",
      "        [-1.3029e-01],\n",
      "        [-1.8462e-01],\n",
      "        [-6.6442e-02],\n",
      "        [ 1.3860e-01],\n",
      "        [-1.1331e-01],\n",
      "        [-6.1270e-02],\n",
      "        [-3.7757e-02],\n",
      "        [ 5.8636e-02],\n",
      "        [-1.0323e-02],\n",
      "        [ 3.7217e-06],\n",
      "        [-2.5474e-02],\n",
      "        [-1.5164e-01],\n",
      "        [ 1.7393e-03],\n",
      "        [-6.5174e-02],\n",
      "        [-1.2442e-01],\n",
      "        [-9.5840e-02],\n",
      "        [-1.4544e-01],\n",
      "        [-4.0270e-02],\n",
      "        [ 9.5900e-03],\n",
      "        [-3.9272e-02],\n",
      "        [ 6.7754e-02],\n",
      "        [ 7.3464e-03],\n",
      "        [ 1.6788e-01],\n",
      "        [-1.2608e-01],\n",
      "        [-7.4308e-02],\n",
      "        [-9.5390e-02],\n",
      "        [-1.2233e-01],\n",
      "        [-6.7624e-02],\n",
      "        [ 1.9540e-01],\n",
      "        [-1.8006e-02],\n",
      "        [ 4.2555e-02],\n",
      "        [-2.5703e-03],\n",
      "        [-1.2810e-02],\n",
      "        [ 5.8363e-03],\n",
      "        [ 9.9593e-02],\n",
      "        [ 1.6739e-02],\n",
      "        [-2.5058e-02],\n",
      "        [-1.0319e-01],\n",
      "        [ 5.6963e-02],\n",
      "        [ 1.3156e-01],\n",
      "        [-1.5832e-01],\n",
      "        [-3.7526e-02],\n",
      "        [ 1.9300e-02],\n",
      "        [ 1.9082e-01],\n",
      "        [-9.3931e-02],\n",
      "        [ 3.4533e-05],\n",
      "        [-2.2223e-03],\n",
      "        [ 2.4717e-05],\n",
      "        [-2.1843e-01],\n",
      "        [-5.3448e-02],\n",
      "        [-1.5160e-01],\n",
      "        [ 5.5073e-02],\n",
      "        [ 7.4474e-05],\n",
      "        [ 5.5013e-03],\n",
      "        [-1.2494e-01],\n",
      "        [ 2.0745e-01],\n",
      "        [-1.9022e-01],\n",
      "        [ 7.5749e-02],\n",
      "        [ 6.3495e-02],\n",
      "        [-2.5964e-02],\n",
      "        [ 2.2943e-02],\n",
      "        [ 1.0298e-01],\n",
      "        [ 5.7047e-04],\n",
      "        [ 1.5890e-01],\n",
      "        [-1.7124e-02],\n",
      "        [-2.8453e-02],\n",
      "        [ 1.2347e-01],\n",
      "        [ 8.5631e-05],\n",
      "        [-3.0027e-02],\n",
      "        [-6.8924e-02],\n",
      "        [ 1.3846e-01],\n",
      "        [ 2.0017e-01],\n",
      "        [-1.5240e-01],\n",
      "        [-2.1329e-05],\n",
      "        [ 1.4738e-02],\n",
      "        [-6.9021e-02],\n",
      "        [ 3.0664e-02],\n",
      "        [ 1.0352e-01],\n",
      "        [-1.7516e-01],\n",
      "        [-1.8558e-01],\n",
      "        [ 8.8368e-02],\n",
      "        [-5.1097e-02],\n",
      "        [ 8.0328e-02],\n",
      "        [-2.4668e-01],\n",
      "        [ 1.6075e-01],\n",
      "        [ 1.3436e-01],\n",
      "        [-4.6077e-02],\n",
      "        [ 6.7098e-02],\n",
      "        [-9.2253e-02],\n",
      "        [-5.8747e-02],\n",
      "        [ 2.5989e-01],\n",
      "        [-1.4903e-01],\n",
      "        [-6.9673e-02],\n",
      "        [ 1.3124e-01],\n",
      "        [ 1.5722e-01],\n",
      "        [ 2.7166e-01],\n",
      "        [ 7.8081e-02],\n",
      "        [-1.0675e-01],\n",
      "        [-2.3126e-01],\n",
      "        [ 7.7908e-02],\n",
      "        [ 1.0076e-01],\n",
      "        [-1.5192e-01],\n",
      "        [ 1.8745e-01],\n",
      "        [-1.2625e-02],\n",
      "        [ 1.0418e-01],\n",
      "        [-1.9146e-02],\n",
      "        [-1.1025e-01],\n",
      "        [ 7.0198e-02],\n",
      "        [-8.7840e-02],\n",
      "        [-1.8933e-02],\n",
      "        [-8.9354e-02],\n",
      "        [ 8.6442e-02],\n",
      "        [ 4.9218e-05],\n",
      "        [-2.1364e-02],\n",
      "        [-4.3434e-04],\n",
      "        [-7.2917e-03],\n",
      "        [ 2.5427e-01],\n",
      "        [ 9.0911e-02],\n",
      "        [-2.7097e-02],\n",
      "        [ 1.3357e-02],\n",
      "        [-1.7363e-02],\n",
      "        [-1.2570e-01],\n",
      "        [ 1.3425e-02],\n",
      "        [ 1.3905e-01],\n",
      "        [-1.0716e-03],\n",
      "        [ 6.0342e-03],\n",
      "        [-3.1517e-03],\n",
      "        [ 4.9907e-04],\n",
      "        [ 4.6921e-02],\n",
      "        [-6.3788e-02],\n",
      "        [ 2.7610e-02],\n",
      "        [-2.5135e-03],\n",
      "        [-7.3243e-02],\n",
      "        [-2.9245e-02],\n",
      "        [-1.4567e-01],\n",
      "        [-1.3257e-01],\n",
      "        [ 8.6144e-02],\n",
      "        [-2.0133e-01],\n",
      "        [ 6.2276e-02],\n",
      "        [ 1.2694e-04],\n",
      "        [ 2.9202e-01],\n",
      "        [ 2.4067e-02],\n",
      "        [ 4.8462e-02],\n",
      "        [ 7.4806e-02],\n",
      "        [-1.2851e-01],\n",
      "        [ 7.6985e-02],\n",
      "        [-9.3647e-02],\n",
      "        [-3.3474e-02],\n",
      "        [-1.3625e-02],\n",
      "        [-3.3716e-02],\n",
      "        [ 3.4271e-02],\n",
      "        [ 2.2879e-01],\n",
      "        [ 1.2053e-01],\n",
      "        [-9.0215e-05],\n",
      "        [ 5.8853e-02],\n",
      "        [ 2.6185e-01],\n",
      "        [-1.6527e-01],\n",
      "        [ 7.2312e-02],\n",
      "        [ 5.9689e-02],\n",
      "        [ 1.0735e-02],\n",
      "        [-2.5367e-02],\n",
      "        [ 3.3574e-02],\n",
      "        [-3.0648e-04],\n",
      "        [-3.0215e-02]], requires_grad=True))\n",
      "('out_layers_w_mean.1', Parameter containing:\n",
      "tensor([[-2.6625e-02],\n",
      "        [-1.6624e-01],\n",
      "        [ 5.7399e-03],\n",
      "        [ 1.9261e-04],\n",
      "        [-2.3184e-02],\n",
      "        [ 6.5201e-05],\n",
      "        [-1.0464e-01],\n",
      "        [-2.6223e-01],\n",
      "        [ 2.5114e-02],\n",
      "        [-1.5653e-01],\n",
      "        [-2.2018e-01],\n",
      "        [ 2.1472e-01],\n",
      "        [ 1.9405e-01],\n",
      "        [ 3.4702e-02],\n",
      "        [ 5.6014e-02],\n",
      "        [ 2.3495e-02],\n",
      "        [-2.0396e-01],\n",
      "        [-5.8094e-02],\n",
      "        [-2.5476e-02],\n",
      "        [ 5.4170e-02],\n",
      "        [-1.8473e-02],\n",
      "        [-9.8852e-02],\n",
      "        [-3.2103e-02],\n",
      "        [-1.0305e-01],\n",
      "        [-9.4236e-02],\n",
      "        [-2.3559e-01],\n",
      "        [ 5.6100e-02],\n",
      "        [-1.5096e-01],\n",
      "        [ 3.3905e-03],\n",
      "        [-2.2513e-03],\n",
      "        [-2.9689e-05],\n",
      "        [-1.4735e-01],\n",
      "        [ 8.7698e-03],\n",
      "        [ 1.3341e-01],\n",
      "        [-1.6956e-01],\n",
      "        [ 1.6495e-01],\n",
      "        [-2.0219e-01],\n",
      "        [ 1.0160e-01],\n",
      "        [ 3.6916e-02],\n",
      "        [ 3.8737e-02],\n",
      "        [ 1.8712e-01],\n",
      "        [ 3.2487e-01],\n",
      "        [ 1.4782e-01],\n",
      "        [ 6.2572e-02],\n",
      "        [ 2.0468e-01],\n",
      "        [-1.2408e-01],\n",
      "        [ 1.1390e-01],\n",
      "        [ 1.9994e-01],\n",
      "        [ 2.1089e-01],\n",
      "        [-3.2696e-03],\n",
      "        [ 6.1724e-02],\n",
      "        [-6.3363e-03],\n",
      "        [ 7.1155e-02],\n",
      "        [-1.9371e-01],\n",
      "        [-1.0890e-01],\n",
      "        [-1.0386e-03],\n",
      "        [ 5.1320e-02],\n",
      "        [ 1.0645e-01],\n",
      "        [-3.7535e-02],\n",
      "        [ 4.9370e-02],\n",
      "        [ 1.5365e-02],\n",
      "        [ 1.0011e-01],\n",
      "        [ 1.1162e-01],\n",
      "        [-2.2224e-02],\n",
      "        [ 8.5546e-03],\n",
      "        [-1.3833e-01],\n",
      "        [ 5.8195e-02],\n",
      "        [-3.4802e-02],\n",
      "        [-1.9237e-02],\n",
      "        [ 2.3121e-02],\n",
      "        [-5.7560e-02],\n",
      "        [-1.0804e-04],\n",
      "        [ 6.9207e-03],\n",
      "        [ 1.2307e-01],\n",
      "        [-1.6080e-02],\n",
      "        [-1.5200e-01],\n",
      "        [-7.4296e-02],\n",
      "        [-1.2184e-01],\n",
      "        [-1.0708e-01],\n",
      "        [ 6.2202e-04],\n",
      "        [-7.1304e-02],\n",
      "        [-9.3273e-03],\n",
      "        [-1.3008e-01],\n",
      "        [ 4.0664e-02],\n",
      "        [-1.5225e-01],\n",
      "        [ 1.2799e-01],\n",
      "        [-2.5126e-04],\n",
      "        [ 2.8284e-01],\n",
      "        [-7.8665e-02],\n",
      "        [ 2.1083e-01],\n",
      "        [ 1.0267e-03],\n",
      "        [-1.8282e-01],\n",
      "        [-1.2481e-01],\n",
      "        [-1.6964e-01],\n",
      "        [-6.4227e-02],\n",
      "        [ 1.3005e-01],\n",
      "        [-1.0423e-01],\n",
      "        [-7.3288e-02],\n",
      "        [-4.3797e-02],\n",
      "        [ 5.8774e-02],\n",
      "        [-3.7260e-02],\n",
      "        [ 4.7140e-05],\n",
      "        [-2.6274e-02],\n",
      "        [-1.6303e-01],\n",
      "        [ 2.1429e-02],\n",
      "        [-6.2930e-02],\n",
      "        [-1.2496e-01],\n",
      "        [-9.1613e-02],\n",
      "        [-1.3777e-01],\n",
      "        [-3.7848e-02],\n",
      "        [ 1.0687e-02],\n",
      "        [-2.3933e-02],\n",
      "        [ 6.9966e-02],\n",
      "        [ 1.0510e-02],\n",
      "        [ 1.6477e-01],\n",
      "        [-1.1235e-01],\n",
      "        [-6.1350e-02],\n",
      "        [-8.5565e-02],\n",
      "        [-1.1116e-01],\n",
      "        [-1.0901e-01],\n",
      "        [ 1.8761e-01],\n",
      "        [-1.9455e-02],\n",
      "        [ 4.3589e-02],\n",
      "        [-4.1212e-03],\n",
      "        [ 5.7740e-03],\n",
      "        [ 8.0304e-03],\n",
      "        [ 9.1541e-02],\n",
      "        [ 1.7459e-02],\n",
      "        [-3.0181e-02],\n",
      "        [-8.9377e-02],\n",
      "        [ 8.3071e-02],\n",
      "        [ 1.2814e-01],\n",
      "        [-1.4544e-01],\n",
      "        [-4.9975e-02],\n",
      "        [ 2.0418e-02],\n",
      "        [ 1.7972e-01],\n",
      "        [-8.7488e-02],\n",
      "        [-7.2623e-05],\n",
      "        [-1.7535e-03],\n",
      "        [ 5.9818e-04],\n",
      "        [-2.2585e-01],\n",
      "        [-4.2852e-02],\n",
      "        [-1.4740e-01],\n",
      "        [ 1.1695e-01],\n",
      "        [ 1.4157e-04],\n",
      "        [ 4.0329e-02],\n",
      "        [-1.1276e-01],\n",
      "        [ 2.0908e-01],\n",
      "        [-1.7704e-01],\n",
      "        [ 1.4612e-01],\n",
      "        [ 9.4557e-02],\n",
      "        [-2.7524e-02],\n",
      "        [ 9.6712e-03],\n",
      "        [ 2.0360e-01],\n",
      "        [ 2.2709e-04],\n",
      "        [ 1.5172e-01],\n",
      "        [-1.8611e-02],\n",
      "        [-2.6899e-02],\n",
      "        [ 1.1894e-01],\n",
      "        [-1.6027e-02],\n",
      "        [-1.0959e-02],\n",
      "        [-6.3649e-02],\n",
      "        [ 1.3179e-01],\n",
      "        [ 1.8896e-01],\n",
      "        [-1.6183e-01],\n",
      "        [ 4.0052e-04],\n",
      "        [ 3.0681e-02],\n",
      "        [-9.9305e-02],\n",
      "        [ 3.7512e-02],\n",
      "        [ 9.8011e-02],\n",
      "        [-1.6076e-01],\n",
      "        [-1.7711e-01],\n",
      "        [ 7.7745e-02],\n",
      "        [-9.2503e-02],\n",
      "        [ 6.9691e-02],\n",
      "        [-2.4525e-01],\n",
      "        [ 1.5052e-01],\n",
      "        [ 1.3600e-01],\n",
      "        [-4.2224e-02],\n",
      "        [ 5.5856e-02],\n",
      "        [-7.8404e-02],\n",
      "        [-4.3066e-02],\n",
      "        [ 2.5814e-01],\n",
      "        [-2.1507e-01],\n",
      "        [-6.3632e-02],\n",
      "        [ 1.2737e-01],\n",
      "        [ 1.4542e-01],\n",
      "        [ 2.6482e-01],\n",
      "        [ 7.3749e-02],\n",
      "        [-9.5583e-02],\n",
      "        [-2.1847e-01],\n",
      "        [ 9.2079e-02],\n",
      "        [ 9.8690e-02],\n",
      "        [-1.4324e-01],\n",
      "        [ 1.7811e-01],\n",
      "        [-2.2434e-02],\n",
      "        [ 9.4701e-02],\n",
      "        [-1.7854e-02],\n",
      "        [-9.5958e-02],\n",
      "        [ 1.2983e-01],\n",
      "        [-9.1416e-02],\n",
      "        [-3.6301e-02],\n",
      "        [-8.1475e-02],\n",
      "        [ 1.3702e-01],\n",
      "        [ 5.6238e-05],\n",
      "        [-2.2436e-02],\n",
      "        [-2.2739e-02],\n",
      "        [-9.2212e-03],\n",
      "        [ 2.5282e-01],\n",
      "        [ 1.3243e-01],\n",
      "        [-3.2502e-02],\n",
      "        [ 2.4913e-02],\n",
      "        [-1.9079e-02],\n",
      "        [-1.1670e-01],\n",
      "        [ 1.6719e-02],\n",
      "        [ 1.3606e-01],\n",
      "        [ 9.3824e-03],\n",
      "        [ 7.6709e-03],\n",
      "        [ 1.0171e-04],\n",
      "        [ 3.3004e-03],\n",
      "        [ 6.2619e-02],\n",
      "        [-6.6339e-02],\n",
      "        [ 4.3841e-02],\n",
      "        [-3.3890e-03],\n",
      "        [-7.0888e-02],\n",
      "        [-4.4665e-02],\n",
      "        [-1.3170e-01],\n",
      "        [-1.4321e-01],\n",
      "        [ 7.4727e-02],\n",
      "        [-1.8780e-01],\n",
      "        [ 5.8113e-02],\n",
      "        [-1.7446e-05],\n",
      "        [ 2.8123e-01],\n",
      "        [ 2.1034e-02],\n",
      "        [ 8.5251e-02],\n",
      "        [ 7.8129e-02],\n",
      "        [-1.3399e-01],\n",
      "        [ 9.4832e-02],\n",
      "        [-1.1609e-01],\n",
      "        [-3.7085e-02],\n",
      "        [ 1.3070e-03],\n",
      "        [-1.6831e-02],\n",
      "        [ 3.4749e-02],\n",
      "        [ 2.1948e-01],\n",
      "        [ 1.2191e-01],\n",
      "        [-3.7298e-04],\n",
      "        [ 9.2613e-02],\n",
      "        [ 2.6641e-01],\n",
      "        [-1.6008e-01],\n",
      "        [ 6.7162e-02],\n",
      "        [ 9.8846e-02],\n",
      "        [ 8.1550e-03],\n",
      "        [-2.6441e-02],\n",
      "        [ 3.3783e-02],\n",
      "        [-2.8228e-03],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        [-4.7103e-02]], requires_grad=True))\n",
      "('out_layers_w_mean.2', Parameter containing:\n",
      "tensor([[-2.5595e-02],\n",
      "        [-1.7957e-01],\n",
      "        [ 1.7572e-03],\n",
      "        [ 1.9149e-04],\n",
      "        [-2.2487e-02],\n",
      "        [ 3.8739e-05],\n",
      "        [-1.0704e-01],\n",
      "        [-2.6382e-01],\n",
      "        [ 2.8660e-02],\n",
      "        [-1.6752e-01],\n",
      "        [-2.3961e-01],\n",
      "        [ 2.3455e-01],\n",
      "        [ 2.1700e-01],\n",
      "        [ 3.4622e-02],\n",
      "        [ 7.3774e-02],\n",
      "        [ 3.6391e-02],\n",
      "        [-2.0233e-01],\n",
      "        [-6.2057e-02],\n",
      "        [-5.1069e-02],\n",
      "        [ 5.4148e-02],\n",
      "        [-1.8493e-02],\n",
      "        [-1.1473e-01],\n",
      "        [-4.1932e-02],\n",
      "        [-9.9094e-02],\n",
      "        [-1.1776e-01],\n",
      "        [-2.3381e-01],\n",
      "        [ 8.1953e-02],\n",
      "        [-1.6519e-01],\n",
      "        [-3.7347e-03],\n",
      "        [-1.0247e-03],\n",
      "        [-3.4914e-05],\n",
      "        [-1.7858e-01],\n",
      "        [ 6.4974e-03],\n",
      "        [ 1.5321e-01],\n",
      "        [-1.9206e-01],\n",
      "        [ 1.6304e-01],\n",
      "        [-2.0761e-01],\n",
      "        [ 1.3125e-01],\n",
      "        [ 4.8443e-02],\n",
      "        [ 5.0458e-02],\n",
      "        [ 2.0285e-01],\n",
      "        [ 3.2829e-01],\n",
      "        [ 1.6897e-01],\n",
      "        [ 7.3788e-02],\n",
      "        [ 2.1627e-01],\n",
      "        [-1.1638e-01],\n",
      "        [ 1.3539e-01],\n",
      "        [ 2.0631e-01],\n",
      "        [ 2.4087e-01],\n",
      "        [-8.6038e-04],\n",
      "        [ 6.2313e-02],\n",
      "        [-4.4972e-03],\n",
      "        [ 7.4443e-02],\n",
      "        [-2.1602e-01],\n",
      "        [-1.4885e-01],\n",
      "        [-1.3585e-03],\n",
      "        [ 5.0074e-02],\n",
      "        [ 9.6745e-02],\n",
      "        [-3.7130e-02],\n",
      "        [ 4.9976e-02],\n",
      "        [ 1.5342e-02],\n",
      "        [ 1.3300e-01],\n",
      "        [ 1.1302e-01],\n",
      "        [-2.3249e-02],\n",
      "        [ 8.4239e-03],\n",
      "        [-1.6514e-01],\n",
      "        [ 5.3658e-02],\n",
      "        [-3.4944e-02],\n",
      "        [-2.0122e-02],\n",
      "        [ 1.6882e-02],\n",
      "        [-5.1150e-02],\n",
      "        [-1.2678e-04],\n",
      "        [-2.2477e-02],\n",
      "        [ 1.6678e-01],\n",
      "        [-1.6072e-02],\n",
      "        [-1.5143e-01],\n",
      "        [-6.3587e-02],\n",
      "        [-1.6475e-01],\n",
      "        [-1.1818e-01],\n",
      "        [ 6.1017e-04],\n",
      "        [-7.2909e-02],\n",
      "        [-2.5096e-02],\n",
      "        [-1.3917e-01],\n",
      "        [ 4.0295e-02],\n",
      "        [-1.7014e-01],\n",
      "        [ 1.5036e-01],\n",
      "        [ 1.4796e-04],\n",
      "        [ 3.3891e-01],\n",
      "        [-8.3875e-02],\n",
      "        [ 2.5281e-01],\n",
      "        [ 8.4900e-04],\n",
      "        [-2.0113e-01],\n",
      "        [-1.2583e-01],\n",
      "        [-1.7250e-01],\n",
      "        [-8.8019e-02],\n",
      "        [ 1.4006e-01],\n",
      "        [-1.5793e-01],\n",
      "        [-1.1001e-01],\n",
      "        [-4.3901e-02],\n",
      "        [ 5.8586e-02],\n",
      "        [-4.9923e-02],\n",
      "        [-5.6125e-05],\n",
      "        [-2.6363e-02],\n",
      "        [-2.0558e-01],\n",
      "        [ 1.1100e-02],\n",
      "        [-6.2757e-02],\n",
      "        [-1.3360e-01],\n",
      "        [-9.4186e-02],\n",
      "        [-1.6881e-01],\n",
      "        [-3.5882e-02],\n",
      "        [ 1.0017e-02],\n",
      "        [-2.4310e-02],\n",
      "        [ 1.1025e-01],\n",
      "        [ 1.0038e-02],\n",
      "        [ 1.6210e-01],\n",
      "        [-1.0926e-01],\n",
      "        [-5.4696e-02],\n",
      "        [-9.0221e-02],\n",
      "        [-1.0240e-01],\n",
      "        [-1.1728e-01],\n",
      "        [ 1.9075e-01],\n",
      "        [-1.9240e-02],\n",
      "        [ 6.6493e-02],\n",
      "        [-4.1047e-03],\n",
      "        [-4.1079e-03],\n",
      "        [ 8.8887e-03],\n",
      "        [ 9.1712e-02],\n",
      "        [ 1.9487e-02],\n",
      "        [-4.0667e-02],\n",
      "        [-7.5066e-02],\n",
      "        [ 7.2217e-02],\n",
      "        [ 1.4344e-01],\n",
      "        [-1.6112e-01],\n",
      "        [-7.5727e-02],\n",
      "        [ 2.0571e-02],\n",
      "        [ 1.8365e-01],\n",
      "        [-8.8572e-02],\n",
      "        [-1.9709e-03],\n",
      "        [-1.9054e-03],\n",
      "        [ 9.6138e-04],\n",
      "        [-2.4350e-01],\n",
      "        [-3.3729e-02],\n",
      "        [-1.4666e-01],\n",
      "        [ 1.6580e-01],\n",
      "        [ 1.3825e-04],\n",
      "        [ 4.1497e-02],\n",
      "        [-1.1800e-01],\n",
      "        [ 2.1700e-01],\n",
      "        [-1.7310e-01],\n",
      "        [ 1.5856e-01],\n",
      "        [ 9.5686e-02],\n",
      "        [-3.9655e-02],\n",
      "        [-1.8964e-02],\n",
      "        [ 2.4633e-01],\n",
      "        [ 2.8198e-04],\n",
      "        [ 1.7714e-01],\n",
      "        [-1.8740e-02],\n",
      "        [-2.9432e-02],\n",
      "        [ 1.2883e-01],\n",
      "        [-3.7217e-02],\n",
      "        [-1.6649e-02],\n",
      "        [-6.7505e-02],\n",
      "        [ 1.3644e-01],\n",
      "        [ 2.0936e-01],\n",
      "        [-1.6741e-01],\n",
      "        [ 3.9594e-04],\n",
      "        [ 3.9410e-02],\n",
      "        [-1.3258e-01],\n",
      "        [ 4.4249e-02],\n",
      "        [ 1.1305e-01],\n",
      "        [-1.6539e-01],\n",
      "        [-2.2161e-01],\n",
      "        [ 6.9234e-02],\n",
      "        [-1.0596e-01],\n",
      "        [ 8.2904e-02],\n",
      "        [-2.7613e-01],\n",
      "        [ 1.6909e-01],\n",
      "        [ 1.3544e-01],\n",
      "        [-4.3363e-02],\n",
      "        [ 4.7609e-02],\n",
      "        [-9.5191e-02],\n",
      "        [-5.0914e-02],\n",
      "        [ 2.9739e-01],\n",
      "        [-2.5884e-01],\n",
      "        [-7.5672e-02],\n",
      "        [ 1.2767e-01],\n",
      "        [ 1.6821e-01],\n",
      "        [ 2.8373e-01],\n",
      "        [ 1.0007e-01],\n",
      "        [-9.4208e-02],\n",
      "        [-2.1434e-01],\n",
      "        [ 9.4668e-02],\n",
      "        [ 1.1325e-01],\n",
      "        [-1.4306e-01],\n",
      "        [ 1.7138e-01],\n",
      "        [-2.4066e-02],\n",
      "        [ 9.6551e-02],\n",
      "        [-1.8527e-02],\n",
      "        [-8.6145e-02],\n",
      "        [ 1.3856e-01],\n",
      "        [-9.8335e-02],\n",
      "        [-5.8680e-02],\n",
      "        [-1.0173e-01],\n",
      "        [ 1.4646e-01],\n",
      "        [ 2.9520e-04],\n",
      "        [-2.2554e-02],\n",
      "        [-3.1781e-02],\n",
      "        [-9.1786e-03],\n",
      "        [ 2.7061e-01],\n",
      "        [ 1.5087e-01],\n",
      "        [-2.0148e-02],\n",
      "        [ 3.6542e-02],\n",
      "        [-1.9287e-02],\n",
      "        [-1.5517e-01],\n",
      "        [ 1.6706e-02],\n",
      "        [ 1.4661e-01],\n",
      "        [ 1.0390e-02],\n",
      "        [ 1.3464e-02],\n",
      "        [ 9.0292e-03],\n",
      "        [ 2.1668e-03],\n",
      "        [ 6.0015e-02],\n",
      "        [-6.7192e-02],\n",
      "        [ 6.0458e-02],\n",
      "        [-3.4006e-03],\n",
      "        [-6.9772e-02],\n",
      "        [-7.0796e-02],\n",
      "        [-1.3257e-01],\n",
      "        [-1.7639e-01],\n",
      "        [ 7.4179e-02],\n",
      "        [-2.3358e-01],\n",
      "        [ 5.1652e-02],\n",
      "        [-2.6836e-05],\n",
      "        [ 3.1157e-01],\n",
      "        [ 8.5630e-03],\n",
      "        [ 1.1370e-01],\n",
      "        [ 7.8357e-02],\n",
      "        [-1.7625e-01],\n",
      "        [ 1.1704e-01],\n",
      "        [-1.0526e-01],\n",
      "        [-3.6996e-02],\n",
      "        [ 2.0741e-03],\n",
      "        [-3.5452e-02],\n",
      "        [ 3.5077e-02],\n",
      "        [ 2.6009e-01],\n",
      "        [ 1.3125e-01],\n",
      "        [-3.7527e-04],\n",
      "        [ 1.1297e-01],\n",
      "        [ 2.9098e-01],\n",
      "        [-1.8596e-01],\n",
      "        [ 6.3186e-02],\n",
      "        [ 1.2858e-01],\n",
      "        [ 8.3484e-03],\n",
      "        [-2.6438e-02],\n",
      "        [ 2.9176e-02],\n",
      "        [-1.0293e-02],\n",
      "        [-6.1946e-02]], requires_grad=True))\n",
      "('out_layers_w_mean.3', Parameter containing:\n",
      "tensor([[-2.4340e-02],\n",
      "        [-1.8374e-01],\n",
      "        [ 1.7427e-03],\n",
      "        [ 1.8912e-04],\n",
      "        [-2.2698e-02],\n",
      "        [ 5.9969e-05],\n",
      "        [-1.1104e-01],\n",
      "        [-2.6680e-01],\n",
      "        [ 2.9078e-02],\n",
      "        [-1.7167e-01],\n",
      "        [-2.4386e-01],\n",
      "        [ 2.4211e-01],\n",
      "        [ 2.1630e-01],\n",
      "        [ 3.8830e-02],\n",
      "        [ 6.4122e-02],\n",
      "        [ 3.9288e-02],\n",
      "        [-1.9885e-01],\n",
      "        [-6.7405e-02],\n",
      "        [-4.6429e-02],\n",
      "        [ 5.4153e-02],\n",
      "        [-1.8471e-02],\n",
      "        [-1.2564e-01],\n",
      "        [-4.2200e-02],\n",
      "        [-1.1232e-01],\n",
      "        [-1.2424e-01],\n",
      "        [-2.3496e-01],\n",
      "        [ 7.4435e-02],\n",
      "        [-1.6225e-01],\n",
      "        [-4.2143e-03],\n",
      "        [-5.9628e-06],\n",
      "        [-3.9658e-05],\n",
      "        [-1.9462e-01],\n",
      "        [ 1.5309e-02],\n",
      "        [ 1.5973e-01],\n",
      "        [-1.8713e-01],\n",
      "        [ 1.5749e-01],\n",
      "        [-2.0269e-01],\n",
      "        [ 1.2398e-01],\n",
      "        [ 4.5394e-02],\n",
      "        [ 4.8243e-02],\n",
      "        [ 2.0035e-01],\n",
      "        [ 3.3088e-01],\n",
      "        [ 1.6335e-01],\n",
      "        [ 7.3379e-02],\n",
      "        [ 2.1149e-01],\n",
      "        [-1.3316e-01],\n",
      "        [ 1.3066e-01],\n",
      "        [ 2.2109e-01],\n",
      "        [ 2.4044e-01],\n",
      "        [-1.6131e-03],\n",
      "        [ 6.1734e-02],\n",
      "        [ 4.6467e-03],\n",
      "        [ 7.3121e-02],\n",
      "        [-2.1845e-01],\n",
      "        [-1.4816e-01],\n",
      "        [-1.2414e-03],\n",
      "        [ 5.1696e-02],\n",
      "        [ 1.0610e-01],\n",
      "        [-3.6673e-02],\n",
      "        [ 5.9915e-02],\n",
      "        [ 1.5332e-02],\n",
      "        [ 1.2837e-01],\n",
      "        [ 1.1274e-01],\n",
      "        [-3.7705e-02],\n",
      "        [ 8.4178e-03],\n",
      "        [-1.5982e-01],\n",
      "        [ 5.6649e-02],\n",
      "        [-3.4250e-02],\n",
      "        [-2.2282e-02],\n",
      "        [ 2.1467e-02],\n",
      "        [-6.2518e-02],\n",
      "        [-8.7481e-05],\n",
      "        [-2.3120e-02],\n",
      "        [ 1.6274e-01],\n",
      "        [-1.6076e-02],\n",
      "        [-1.5250e-01],\n",
      "        [-7.6744e-02],\n",
      "        [-1.6185e-01],\n",
      "        [-1.2139e-01],\n",
      "        [ 6.0635e-04],\n",
      "        [-6.3942e-02],\n",
      "        [-1.8392e-02],\n",
      "        [-1.5725e-01],\n",
      "        [ 4.2108e-02],\n",
      "        [-1.6472e-01],\n",
      "        [ 1.4964e-01],\n",
      "        [ 9.8199e-06],\n",
      "        [ 3.3342e-01],\n",
      "        [-8.2723e-02],\n",
      "        [ 2.5226e-01],\n",
      "        [ 8.1178e-04],\n",
      "        [-2.1332e-01],\n",
      "        [-1.3195e-01],\n",
      "        [-1.9148e-01],\n",
      "        [-8.2996e-02],\n",
      "        [ 1.3818e-01],\n",
      "        [-1.5558e-01],\n",
      "        [-1.0804e-01],\n",
      "        [-4.3882e-02],\n",
      "        [ 5.8552e-02],\n",
      "        [-5.6011e-02],\n",
      "        [ 6.5394e-05],\n",
      "        [-2.6420e-02],\n",
      "        [-2.0552e-01],\n",
      "        [ 6.2503e-03],\n",
      "        [-6.2189e-02],\n",
      "        [-1.4456e-01],\n",
      "        [-8.8991e-02],\n",
      "        [-1.6451e-01],\n",
      "        [-3.8210e-02],\n",
      "        [ 1.0136e-02],\n",
      "        [-2.5795e-02],\n",
      "        [ 1.0432e-01],\n",
      "        [ 1.0121e-02],\n",
      "        [ 1.7173e-01],\n",
      "        [-1.0895e-01],\n",
      "        [-5.9558e-02],\n",
      "        [-9.5068e-02],\n",
      "        [-1.0607e-01],\n",
      "        [-1.2032e-01],\n",
      "        [ 1.9539e-01],\n",
      "        [-1.9619e-02],\n",
      "        [ 6.0643e-02],\n",
      "        [-4.1075e-03],\n",
      "        [-6.1884e-03],\n",
      "        [ 8.9813e-03],\n",
      "        [ 8.5557e-02],\n",
      "        [ 2.9737e-02],\n",
      "        [-4.5410e-02],\n",
      "        [-7.2558e-02],\n",
      "        [ 7.9057e-02],\n",
      "        [ 1.4497e-01],\n",
      "        [-1.5342e-01],\n",
      "        [-6.7826e-02],\n",
      "        [ 2.0508e-02],\n",
      "        [ 1.9069e-01],\n",
      "        [-9.0156e-02],\n",
      "        [-1.8049e-03],\n",
      "        [-2.0038e-03],\n",
      "        [ 4.5491e-04],\n",
      "        [-2.5052e-01],\n",
      "        [-3.6623e-02],\n",
      "        [-1.4620e-01],\n",
      "        [ 1.5880e-01],\n",
      "        [ 1.3217e-04],\n",
      "        [ 4.7790e-02],\n",
      "        [-1.3250e-01],\n",
      "        [ 2.1806e-01],\n",
      "        [-1.8272e-01],\n",
      "        [ 1.5536e-01],\n",
      "        [ 9.2804e-02],\n",
      "        [-4.1200e-02],\n",
      "        [-8.7505e-03],\n",
      "        [ 2.4353e-01],\n",
      "        [ 3.0564e-04],\n",
      "        [ 1.7450e-01],\n",
      "        [-1.8726e-02],\n",
      "        [-3.1339e-02],\n",
      "        [ 1.3139e-01],\n",
      "        [-3.1610e-02],\n",
      "        [-2.3794e-02],\n",
      "        [-7.9993e-02],\n",
      "        [ 1.4508e-01],\n",
      "        [ 2.0619e-01],\n",
      "        [-1.7867e-01],\n",
      "        [ 4.1555e-04],\n",
      "        [ 3.9897e-02],\n",
      "        [-1.3014e-01],\n",
      "        [ 4.0552e-02],\n",
      "        [ 1.0994e-01],\n",
      "        [-1.7674e-01],\n",
      "        [-2.1429e-01],\n",
      "        [ 6.9721e-02],\n",
      "        [-1.0836e-01],\n",
      "        [ 7.9937e-02],\n",
      "        [-2.7280e-01],\n",
      "        [ 1.7287e-01],\n",
      "        [ 1.3561e-01],\n",
      "        [-5.1054e-02],\n",
      "        [ 5.7571e-02],\n",
      "        [-9.6460e-02],\n",
      "        [-5.9910e-02],\n",
      "        [ 2.8916e-01],\n",
      "        [-2.5051e-01],\n",
      "        [-7.2302e-02],\n",
      "        [ 1.2748e-01],\n",
      "        [ 1.6945e-01],\n",
      "        [ 2.8072e-01],\n",
      "        [ 1.0340e-01],\n",
      "        [-9.4590e-02],\n",
      "        [-2.4681e-01],\n",
      "        [ 9.4057e-02],\n",
      "        [ 1.0878e-01],\n",
      "        [-1.4294e-01],\n",
      "        [ 1.8917e-01],\n",
      "        [-2.4030e-02],\n",
      "        [ 1.0263e-01],\n",
      "        [-1.8579e-02],\n",
      "        [-9.0967e-02],\n",
      "        [ 1.3020e-01],\n",
      "        [-1.0459e-01],\n",
      "        [-5.4570e-02],\n",
      "        [-1.0499e-01],\n",
      "        [ 1.4651e-01],\n",
      "        [ 4.0629e-04],\n",
      "        [-2.2551e-02],\n",
      "        [-3.0347e-02],\n",
      "        [-9.1203e-03],\n",
      "        [ 2.6958e-01],\n",
      "        [ 1.4823e-01],\n",
      "        [-2.7292e-02],\n",
      "        [ 3.1109e-02],\n",
      "        [-1.9029e-02],\n",
      "        [-1.7413e-01],\n",
      "        [ 1.6696e-02],\n",
      "        [ 1.4340e-01],\n",
      "        [ 1.0165e-02],\n",
      "        [ 1.3829e-02],\n",
      "        [ 4.9559e-03],\n",
      "        [ 3.8314e-03],\n",
      "        [ 5.7499e-02],\n",
      "        [-5.8557e-02],\n",
      "        [ 5.3990e-02],\n",
      "        [-3.4271e-03],\n",
      "        [-7.2725e-02],\n",
      "        [-6.5706e-02],\n",
      "        [-1.4884e-01],\n",
      "        [-1.6721e-01],\n",
      "        [ 7.7455e-02],\n",
      "        [-2.5603e-01],\n",
      "        [ 5.6630e-02],\n",
      "        [-3.7013e-05],\n",
      "        [ 3.1069e-01],\n",
      "        [ 1.6356e-02],\n",
      "        [ 1.0716e-01],\n",
      "        [ 7.8311e-02],\n",
      "        [-1.7890e-01],\n",
      "        [ 1.1366e-01],\n",
      "        [-1.2633e-01],\n",
      "        [-3.7073e-02],\n",
      "        [-4.6537e-03],\n",
      "        [-3.9165e-02],\n",
      "        [ 3.5274e-02],\n",
      "        [ 2.5855e-01],\n",
      "        [ 1.3315e-01],\n",
      "        [-3.7441e-04],\n",
      "        [ 1.0648e-01],\n",
      "        [ 2.8797e-01],\n",
      "        [-1.8349e-01],\n",
      "        [ 5.8235e-02],\n",
      "        [ 1.2169e-01],\n",
      "        [ 1.0847e-02],\n",
      "        [-2.6480e-02],\n",
      "        [ 3.0285e-02],\n",
      "        [-8.2390e-03],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        [-6.8524e-02]], requires_grad=True))\n",
      "('out_layers_w_mean.4', Parameter containing:\n",
      "tensor([[-3.0061e-02],\n",
      "        [-1.8550e-01],\n",
      "        [ 3.6409e-03],\n",
      "        [ 1.8445e-04],\n",
      "        [-2.3359e-02],\n",
      "        [ 6.1399e-05],\n",
      "        [-1.1291e-01],\n",
      "        [-2.6698e-01],\n",
      "        [ 2.7116e-02],\n",
      "        [-1.7200e-01],\n",
      "        [-2.4335e-01],\n",
      "        [ 2.3395e-01],\n",
      "        [ 2.1410e-01],\n",
      "        [ 3.4118e-02],\n",
      "        [ 6.9820e-02],\n",
      "        [ 3.4166e-02],\n",
      "        [-1.9917e-01],\n",
      "        [-7.2559e-02],\n",
      "        [-4.1664e-02],\n",
      "        [ 5.4154e-02],\n",
      "        [-1.8498e-02],\n",
      "        [-1.2105e-01],\n",
      "        [-4.5167e-02],\n",
      "        [-1.1252e-01],\n",
      "        [-1.2613e-01],\n",
      "        [-2.4525e-01],\n",
      "        [ 8.1809e-02],\n",
      "        [-1.5854e-01],\n",
      "        [-5.7806e-03],\n",
      "        [ 3.7710e-04],\n",
      "        [-4.4191e-05],\n",
      "        [-1.9812e-01],\n",
      "        [ 1.3910e-02],\n",
      "        [ 1.5594e-01],\n",
      "        [-1.8489e-01],\n",
      "        [ 1.5601e-01],\n",
      "        [-2.0328e-01],\n",
      "        [ 1.2378e-01],\n",
      "        [ 4.6442e-02],\n",
      "        [ 4.8538e-02],\n",
      "        [ 1.9499e-01],\n",
      "        [ 3.2326e-01],\n",
      "        [ 1.6187e-01],\n",
      "        [ 7.0509e-02],\n",
      "        [ 2.1167e-01],\n",
      "        [-1.3420e-01],\n",
      "        [ 1.2722e-01],\n",
      "        [ 2.2303e-01],\n",
      "        [ 2.3831e-01],\n",
      "        [ 5.4457e-03],\n",
      "        [ 5.8518e-02],\n",
      "        [ 4.7814e-03],\n",
      "        [ 7.5781e-02],\n",
      "        [-2.2147e-01],\n",
      "        [-1.5012e-01],\n",
      "        [-1.3381e-03],\n",
      "        [ 6.3860e-02],\n",
      "        [ 1.1840e-01],\n",
      "        [-3.6727e-02],\n",
      "        [ 5.2459e-02],\n",
      "        [ 1.5354e-02],\n",
      "        [ 1.3165e-01],\n",
      "        [ 1.0993e-01],\n",
      "        [-4.0127e-02],\n",
      "        [ 8.4330e-03],\n",
      "        [-1.6628e-01],\n",
      "        [ 5.3983e-02],\n",
      "        [-3.2744e-02],\n",
      "        [-2.0691e-02],\n",
      "        [ 2.1615e-02],\n",
      "        [-6.5565e-02],\n",
      "        [-1.1160e-04],\n",
      "        [-3.3226e-02],\n",
      "        [ 1.7290e-01],\n",
      "        [-1.6068e-02],\n",
      "        [-1.5209e-01],\n",
      "        [-7.7043e-02],\n",
      "        [-1.6240e-01],\n",
      "        [-1.2022e-01],\n",
      "        [ 4.3588e-04],\n",
      "        [-6.5260e-02],\n",
      "        [-2.4680e-02],\n",
      "        [-1.5329e-01],\n",
      "        [ 3.9610e-02],\n",
      "        [-1.6421e-01],\n",
      "        [ 1.4668e-01],\n",
      "        [-2.0701e-04],\n",
      "        [ 3.2463e-01],\n",
      "        [-8.6204e-02],\n",
      "        [ 2.5250e-01],\n",
      "        [ 8.1745e-04],\n",
      "        [-2.1455e-01],\n",
      "        [-1.3193e-01],\n",
      "        [-1.9749e-01],\n",
      "        [-8.5163e-02],\n",
      "        [ 1.3058e-01],\n",
      "        [-1.5617e-01],\n",
      "        [-1.0561e-01],\n",
      "        [-4.3897e-02],\n",
      "        [ 5.8571e-02],\n",
      "        [-5.9894e-02],\n",
      "        [ 2.8572e-05],\n",
      "        [-2.6369e-02],\n",
      "        [-2.0819e-01],\n",
      "        [ 1.6116e-02],\n",
      "        [-6.2059e-02],\n",
      "        [-1.4715e-01],\n",
      "        [-9.0879e-02],\n",
      "        [-1.6377e-01],\n",
      "        [-3.4936e-02],\n",
      "        [ 1.0147e-02],\n",
      "        [-3.5590e-02],\n",
      "        [ 9.8899e-02],\n",
      "        [ 1.0105e-02],\n",
      "        [ 1.7266e-01],\n",
      "        [-1.0872e-01],\n",
      "        [-7.1971e-02],\n",
      "        [-9.4874e-02],\n",
      "        [-1.1607e-01],\n",
      "        [-1.2441e-01],\n",
      "        [ 1.8884e-01],\n",
      "        [-1.9611e-02],\n",
      "        [ 5.9671e-02],\n",
      "        [-4.1123e-03],\n",
      "        [-5.0496e-04],\n",
      "        [ 9.2411e-03],\n",
      "        [ 8.4445e-02],\n",
      "        [ 2.4513e-02],\n",
      "        [-4.5122e-02],\n",
      "        [-1.0042e-01],\n",
      "        [ 8.5160e-02],\n",
      "        [ 1.4384e-01],\n",
      "        [-1.5385e-01],\n",
      "        [-7.1751e-02],\n",
      "        [ 2.0066e-02],\n",
      "        [ 1.7989e-01],\n",
      "        [-9.0366e-02],\n",
      "        [-1.9412e-03],\n",
      "        [-1.8012e-03],\n",
      "        [ 3.8160e-04],\n",
      "        [-2.5086e-01],\n",
      "        [-3.9592e-02],\n",
      "        [-1.4372e-01],\n",
      "        [ 1.6437e-01],\n",
      "        [ 1.4454e-04],\n",
      "        [ 6.4564e-02],\n",
      "        [-1.3312e-01],\n",
      "        [ 2.2140e-01],\n",
      "        [-1.8023e-01],\n",
      "        [ 1.6134e-01],\n",
      "        [ 1.0311e-01],\n",
      "        [-4.1938e-02],\n",
      "        [-1.0932e-02],\n",
      "        [ 2.5259e-01],\n",
      "        [ 6.6750e-05],\n",
      "        [ 1.7128e-01],\n",
      "        [-1.8955e-02],\n",
      "        [-3.3512e-02],\n",
      "        [ 1.2904e-01],\n",
      "        [-3.2193e-02],\n",
      "        [-1.7268e-02],\n",
      "        [-8.3144e-02],\n",
      "        [ 1.3822e-01],\n",
      "        [ 2.0151e-01],\n",
      "        [-1.7813e-01],\n",
      "        [ 4.1285e-04],\n",
      "        [ 5.7137e-02],\n",
      "        [-1.3097e-01],\n",
      "        [ 4.0685e-02],\n",
      "        [ 1.0699e-01],\n",
      "        [-1.7630e-01],\n",
      "        [-2.1551e-01],\n",
      "        [ 6.5566e-02],\n",
      "        [-1.0805e-01],\n",
      "        [ 7.5292e-02],\n",
      "        [-2.7151e-01],\n",
      "        [ 1.6628e-01],\n",
      "        [ 1.6158e-01],\n",
      "        [-5.2980e-02],\n",
      "        [ 5.2510e-02],\n",
      "        [-9.5491e-02],\n",
      "        [-6.1351e-02],\n",
      "        [ 2.8145e-01],\n",
      "        [-2.5233e-01],\n",
      "        [-7.0708e-02],\n",
      "        [ 1.2519e-01],\n",
      "        [ 1.6202e-01],\n",
      "        [ 2.7825e-01],\n",
      "        [ 1.0598e-01],\n",
      "        [-1.2480e-01],\n",
      "        [-2.4749e-01],\n",
      "        [ 9.2817e-02],\n",
      "        [ 1.0797e-01],\n",
      "        [-1.4302e-01],\n",
      "        [ 1.8057e-01],\n",
      "        [-2.4018e-02],\n",
      "        [ 9.8293e-02],\n",
      "        [-1.8794e-02],\n",
      "        [-1.1570e-01],\n",
      "        [ 1.3421e-01],\n",
      "        [-1.1231e-01],\n",
      "        [-4.9660e-02],\n",
      "        [-1.0319e-01],\n",
      "        [ 1.4809e-01],\n",
      "        [ 7.0642e-04],\n",
      "        [-2.2565e-02],\n",
      "        [-3.2732e-02],\n",
      "        [-8.6931e-03],\n",
      "        [ 2.6622e-01],\n",
      "        [ 1.5753e-01],\n",
      "        [-3.2639e-02],\n",
      "        [ 2.9099e-02],\n",
      "        [-1.9080e-02],\n",
      "        [-1.7462e-01],\n",
      "        [ 1.6698e-02],\n",
      "        [ 1.4130e-01],\n",
      "        [ 1.7918e-02],\n",
      "        [ 1.0388e-02],\n",
      "        [ 4.3814e-04],\n",
      "        [ 4.4636e-03],\n",
      "        [ 5.2148e-02],\n",
      "        [-5.9608e-02],\n",
      "        [ 5.2320e-02],\n",
      "        [-3.4051e-03],\n",
      "        [-9.6792e-02],\n",
      "        [-6.7196e-02],\n",
      "        [-1.4798e-01],\n",
      "        [-1.6721e-01],\n",
      "        [ 7.2267e-02],\n",
      "        [-2.5658e-01],\n",
      "        [ 5.4205e-02],\n",
      "        [-2.4208e-05],\n",
      "        [ 3.0199e-01],\n",
      "        [ 1.3069e-02],\n",
      "        [ 1.1120e-01],\n",
      "        [ 7.8154e-02],\n",
      "        [-1.7676e-01],\n",
      "        [ 1.0777e-01],\n",
      "        [-1.3680e-01],\n",
      "        [-3.6893e-02],\n",
      "        [-1.6703e-03],\n",
      "        [-3.2953e-02],\n",
      "        [ 3.5583e-02],\n",
      "        [ 2.5410e-01],\n",
      "        [ 1.2674e-01],\n",
      "        [-3.6926e-04],\n",
      "        [ 1.0767e-01],\n",
      "        [ 2.8642e-01],\n",
      "        [-1.8199e-01],\n",
      "        [ 5.2766e-02],\n",
      "        [ 1.1471e-01],\n",
      "        [ 5.7474e-03],\n",
      "        [-2.8202e-02],\n",
      "        [ 4.5288e-02],\n",
      "        [-8.1799e-03],\n",
      "        [-6.5786e-02]], requires_grad=True))\n",
      "('out_layers_b_mean.0', Parameter containing:\n",
      "tensor([[-0.1453]], requires_grad=True))\n",
      "('out_layers_b_mean.1', Parameter containing:\n",
      "tensor([[-0.1227]], requires_grad=True))\n",
      "('out_layers_b_mean.2', Parameter containing:\n",
      "tensor([[-0.1351]], requires_grad=True))\n",
      "('out_layers_b_mean.3', Parameter containing:\n",
      "tensor([[-0.1399]], requires_grad=True))\n",
      "('out_layers_b_mean.4', Parameter containing:\n",
      "tensor([[-0.1418]], requires_grad=True))\n",
      "('out_layers_w_var.0', Parameter containing:\n",
      "tensor([[-5.8708],\n",
      "        [-5.8713],\n",
      "        [-5.8708],\n",
      "        [-5.8708],\n",
      "        [-5.8708],\n",
      "        [-5.8708],\n",
      "        [-5.8708],\n",
      "        [-5.8710],\n",
      "        [-5.8708],\n",
      "        [-5.8709],\n",
      "        [-5.8710],\n",
      "        [-5.8708],\n",
      "        [-5.8708],\n",
      "        [-5.8708],\n",
      "        [-5.8706],\n",
      "        [-5.8708],\n",
      "        [-5.8707],\n",
      "        [-5.8708],\n",
      "        [-5.8709],\n",
      "        [-5.8708],\n",
      "        [-5.8708],\n",
      "        [-5.8708],\n",
      "        [-5.8708],\n",
      "        [-5.8709],\n",
      "        [-5.8711],\n",
      "        [-5.8709],\n",
      "        [-5.8708],\n",
      "        [-5.8706],\n",
      "        [-5.8708],\n",
      "        [-5.8708],\n",
      "        [-5.8708],\n",
      "        [-5.8704],\n",
      "        [-5.8708],\n",
      "        [-5.8708],\n",
      "        [-5.8702],\n",
      "        [-5.8711],\n",
      "        [-5.8707],\n",
      "        [-5.8708],\n",
      "        [-5.8709],\n",
      "        [-5.8708],\n",
      "        [-5.8709],\n",
      "        [-5.8707],\n",
      "        [-5.8708],\n",
      "        [-5.8708],\n",
      "        [-5.8708],\n",
      "        [-5.8707],\n",
      "        [-5.8708],\n",
      "        [-5.8708],\n",
      "        [-5.8708],\n",
      "        [-5.8708],\n",
      "        [-5.8716],\n",
      "        [-5.8708],\n",
      "        [-5.8707],\n",
      "        [-5.8708],\n",
      "        [-5.8709],\n",
      "        [-5.8708],\n",
      "        [-5.8708],\n",
      "        [-5.8715],\n",
      "        [-5.8708],\n",
      "        [-5.8705],\n",
      "        [-5.8708],\n",
      "        [-5.8708],\n",
      "        [-5.8708],\n",
      "        [-5.8708],\n",
      "        [-5.8708],\n",
      "        [-5.8708],\n",
      "        [-5.8708],\n",
      "        [-5.8708],\n",
      "        [-5.8708],\n",
      "        [-5.8709],\n",
      "        [-5.8708],\n",
      "        [-5.8708],\n",
      "        [-5.8708],\n",
      "        [-5.8708],\n",
      "        [-5.8708],\n",
      "        [-5.8708],\n",
      "        [-5.8706],\n",
      "        [-5.8709],\n",
      "        [-5.8708],\n",
      "        [-5.8708],\n",
      "        [-5.8708],\n",
      "        [-5.8708],\n",
      "        [-5.8710],\n",
      "        [-5.8708],\n",
      "        [-5.8708],\n",
      "        [-5.8708],\n",
      "        [-5.8708],\n",
      "        [-5.8708],\n",
      "        [-5.8708],\n",
      "        [-5.8707],\n",
      "        [-5.8708],\n",
      "        [-5.8708],\n",
      "        [-5.8706],\n",
      "        [-5.8708],\n",
      "        [-5.8709],\n",
      "        [-5.8709],\n",
      "        [-5.8708],\n",
      "        [-5.8708],\n",
      "        [-5.8708],\n",
      "        [-5.8708],\n",
      "        [-5.8708],\n",
      "        [-5.8708],\n",
      "        [-5.8708],\n",
      "        [-5.8708],\n",
      "        [-5.8706],\n",
      "        [-5.8708],\n",
      "        [-5.8708],\n",
      "        [-5.8708],\n",
      "        [-5.8708],\n",
      "        [-5.8708],\n",
      "        [-5.8708],\n",
      "        [-5.8708],\n",
      "        [-5.8708],\n",
      "        [-5.8708],\n",
      "        [-5.8707],\n",
      "        [-5.8707],\n",
      "        [-5.8710],\n",
      "        [-5.8706],\n",
      "        [-5.8709],\n",
      "        [-5.8708],\n",
      "        [-5.8708],\n",
      "        [-5.8708],\n",
      "        [-5.8707],\n",
      "        [-5.8708],\n",
      "        [-5.8706],\n",
      "        [-5.8708],\n",
      "        [-5.8706],\n",
      "        [-5.8709],\n",
      "        [-5.8710],\n",
      "        [-5.8706],\n",
      "        [-5.8707],\n",
      "        [-5.8708],\n",
      "        [-5.8708],\n",
      "        [-5.8708],\n",
      "        [-5.8708],\n",
      "        [-5.8708],\n",
      "        [-5.8711],\n",
      "        [-5.8708],\n",
      "        [-5.8708],\n",
      "        [-5.8708],\n",
      "        [-5.8708],\n",
      "        [-5.8708],\n",
      "        [-5.8708],\n",
      "        [-5.8708],\n",
      "        [-5.8708],\n",
      "        [-5.8708],\n",
      "        [-5.8708],\n",
      "        [-5.8709],\n",
      "        [-5.8708],\n",
      "        [-5.8708],\n",
      "        [-5.8708],\n",
      "        [-5.8708],\n",
      "        [-5.8709],\n",
      "        [-5.8708],\n",
      "        [-5.8708],\n",
      "        [-5.8707],\n",
      "        [-5.8708],\n",
      "        [-5.8713],\n",
      "        [-5.8711],\n",
      "        [-5.8708],\n",
      "        [-5.8712],\n",
      "        [-5.8708],\n",
      "        [-5.8708],\n",
      "        [-5.8709],\n",
      "        [-5.8708],\n",
      "        [-5.8708],\n",
      "        [-5.8708],\n",
      "        [-5.8708],\n",
      "        [-5.8707],\n",
      "        [-5.8708],\n",
      "        [-5.8708],\n",
      "        [-5.8708],\n",
      "        [-5.8707],\n",
      "        [-5.8708],\n",
      "        [-5.8708],\n",
      "        [-5.8706],\n",
      "        [-5.8708],\n",
      "        [-5.8708],\n",
      "        [-5.8703],\n",
      "        [-5.8708],\n",
      "        [-5.8708],\n",
      "        [-5.8708],\n",
      "        [-5.8708],\n",
      "        [-5.8708],\n",
      "        [-5.8708],\n",
      "        [-5.8708],\n",
      "        [-5.8708],\n",
      "        [-5.8708],\n",
      "        [-5.8708],\n",
      "        [-5.8708],\n",
      "        [-5.8715],\n",
      "        [-5.8709],\n",
      "        [-5.8707],\n",
      "        [-5.8708],\n",
      "        [-5.8708],\n",
      "        [-5.8708],\n",
      "        [-5.8703],\n",
      "        [-5.8708],\n",
      "        [-5.8708],\n",
      "        [-5.8708],\n",
      "        [-5.8708],\n",
      "        [-5.8708],\n",
      "        [-5.8709],\n",
      "        [-5.8710],\n",
      "        [-5.8708],\n",
      "        [-5.8708],\n",
      "        [-5.8708],\n",
      "        [-5.8708],\n",
      "        [-5.8712],\n",
      "        [-5.8708],\n",
      "        [-5.8708],\n",
      "        [-5.8708],\n",
      "        [-5.8708],\n",
      "        [-5.8708],\n",
      "        [-5.8708],\n",
      "        [-5.8708],\n",
      "        [-5.8707],\n",
      "        [-5.8708],\n",
      "        [-5.8708],\n",
      "        [-5.8708],\n",
      "        [-5.8708],\n",
      "        [-5.8708],\n",
      "        [-5.8716],\n",
      "        [-5.8708],\n",
      "        [-5.8709],\n",
      "        [-5.8707],\n",
      "        [-5.8708],\n",
      "        [-5.8708],\n",
      "        [-5.8711],\n",
      "        [-5.8709],\n",
      "        [-5.8709],\n",
      "        [-5.8708],\n",
      "        [-5.8709],\n",
      "        [-5.8708],\n",
      "        [-5.8708],\n",
      "        [-5.8708],\n",
      "        [-5.8707],\n",
      "        [-5.8708],\n",
      "        [-5.8708],\n",
      "        [-5.8708],\n",
      "        [-5.8709],\n",
      "        [-5.8707],\n",
      "        [-5.8708],\n",
      "        [-5.8708],\n",
      "        [-5.8708],\n",
      "        [-5.8708],\n",
      "        [-5.8709],\n",
      "        [-5.8709],\n",
      "        [-5.8709],\n",
      "        [-5.8707],\n",
      "        [-5.8711],\n",
      "        [-5.8708],\n",
      "        [-5.8708],\n",
      "        [-5.8708],\n",
      "        [-5.8708],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        [-5.8708]], requires_grad=True))\n",
      "('out_layers_w_var.1', Parameter containing:\n",
      "tensor([[-5.8766],\n",
      "        [-5.8710],\n",
      "        [-5.8786],\n",
      "        [-5.8801],\n",
      "        [-5.8800],\n",
      "        [-5.8801],\n",
      "        [-5.8840],\n",
      "        [-5.8752],\n",
      "        [-5.8791],\n",
      "        [-5.8763],\n",
      "        [-5.8657],\n",
      "        [-5.8680],\n",
      "        [-5.8785],\n",
      "        [-5.8694],\n",
      "        [-5.8762],\n",
      "        [-5.8803],\n",
      "        [-5.8712],\n",
      "        [-5.8640],\n",
      "        [-5.8797],\n",
      "        [-5.8801],\n",
      "        [-5.8801],\n",
      "        [-5.8653],\n",
      "        [-5.8715],\n",
      "        [-5.8768],\n",
      "        [-5.8720],\n",
      "        [-5.8698],\n",
      "        [-5.8782],\n",
      "        [-5.8597],\n",
      "        [-5.8803],\n",
      "        [-5.8813],\n",
      "        [-5.8801],\n",
      "        [-5.8695],\n",
      "        [-5.8786],\n",
      "        [-5.8800],\n",
      "        [-5.8685],\n",
      "        [-5.8788],\n",
      "        [-5.8812],\n",
      "        [-5.8769],\n",
      "        [-5.8654],\n",
      "        [-5.8703],\n",
      "        [-5.8690],\n",
      "        [-5.8787],\n",
      "        [-5.8769],\n",
      "        [-5.8801],\n",
      "        [-5.8706],\n",
      "        [-5.8797],\n",
      "        [-5.8797],\n",
      "        [-5.8715],\n",
      "        [-5.8796],\n",
      "        [-5.8801],\n",
      "        [-5.8722],\n",
      "        [-5.8705],\n",
      "        [-5.8765],\n",
      "        [-5.8736],\n",
      "        [-5.8692],\n",
      "        [-5.8800],\n",
      "        [-5.8786],\n",
      "        [-5.8787],\n",
      "        [-5.8799],\n",
      "        [-5.8758],\n",
      "        [-5.8801],\n",
      "        [-5.8862],\n",
      "        [-5.8802],\n",
      "        [-5.8777],\n",
      "        [-5.8801],\n",
      "        [-5.8663],\n",
      "        [-5.8766],\n",
      "        [-5.8797],\n",
      "        [-5.8776],\n",
      "        [-5.8785],\n",
      "        [-5.8807],\n",
      "        [-5.8801],\n",
      "        [-5.8764],\n",
      "        [-5.8716],\n",
      "        [-5.8801],\n",
      "        [-5.8809],\n",
      "        [-5.8764],\n",
      "        [-5.8747],\n",
      "        [-5.8798],\n",
      "        [-5.8802],\n",
      "        [-5.8752],\n",
      "        [-5.8735],\n",
      "        [-5.8670],\n",
      "        [-5.8796],\n",
      "        [-5.8731],\n",
      "        [-5.8685],\n",
      "        [-5.8801],\n",
      "        [-5.8711],\n",
      "        [-5.8724],\n",
      "        [-5.8717],\n",
      "        [-5.8801],\n",
      "        [-5.8675],\n",
      "        [-5.8640],\n",
      "        [-5.8682],\n",
      "        [-5.8771],\n",
      "        [-5.8682],\n",
      "        [-5.8744],\n",
      "        [-5.8810],\n",
      "        [-5.8802],\n",
      "        [-5.8802],\n",
      "        [-5.8791],\n",
      "        [-5.8801],\n",
      "        [-5.8802],\n",
      "        [-5.8782],\n",
      "        [-5.8723],\n",
      "        [-5.8806],\n",
      "        [-5.8767],\n",
      "        [-5.8765],\n",
      "        [-5.8626],\n",
      "        [-5.8802],\n",
      "        [-5.8803],\n",
      "        [-5.8674],\n",
      "        [-5.8799],\n",
      "        [-5.8802],\n",
      "        [-5.8785],\n",
      "        [-5.8737],\n",
      "        [-5.8772],\n",
      "        [-5.8717],\n",
      "        [-5.8702],\n",
      "        [-5.8775],\n",
      "        [-5.8709],\n",
      "        [-5.8802],\n",
      "        [-5.8748],\n",
      "        [-5.8801],\n",
      "        [-5.8775],\n",
      "        [-5.8815],\n",
      "        [-5.8783],\n",
      "        [-5.8767],\n",
      "        [-5.8694],\n",
      "        [-5.8770],\n",
      "        [-5.8720],\n",
      "        [-5.8668],\n",
      "        [-5.8710],\n",
      "        [-5.8826],\n",
      "        [-5.8810],\n",
      "        [-5.8701],\n",
      "        [-5.8683],\n",
      "        [-5.8803],\n",
      "        [-5.8801],\n",
      "        [-5.8801],\n",
      "        [-5.8674],\n",
      "        [-5.8625],\n",
      "        [-5.8800],\n",
      "        [-5.8737],\n",
      "        [-5.8801],\n",
      "        [-5.8804],\n",
      "        [-5.8663],\n",
      "        [-5.8678],\n",
      "        [-5.8780],\n",
      "        [-5.8759],\n",
      "        [-5.8730],\n",
      "        [-5.8805],\n",
      "        [-5.8770],\n",
      "        [-5.8929],\n",
      "        [-5.8802],\n",
      "        [-5.8681],\n",
      "        [-5.8801],\n",
      "        [-5.8787],\n",
      "        [-5.8784],\n",
      "        [-5.8809],\n",
      "        [-5.8744],\n",
      "        [-5.8694],\n",
      "        [-5.8766],\n",
      "        [-5.8722],\n",
      "        [-5.8716],\n",
      "        [-5.8801],\n",
      "        [-5.8784],\n",
      "        [-5.8770],\n",
      "        [-5.8753],\n",
      "        [-5.8676],\n",
      "        [-5.8807],\n",
      "        [-5.8723],\n",
      "        [-5.8792],\n",
      "        [-5.8864],\n",
      "        [-5.8716],\n",
      "        [-5.8725],\n",
      "        [-5.8690],\n",
      "        [-5.8801],\n",
      "        [-5.8781],\n",
      "        [-5.8724],\n",
      "        [-5.8763],\n",
      "        [-5.8667],\n",
      "        [-5.8808],\n",
      "        [-5.8822],\n",
      "        [-5.8788],\n",
      "        [-5.8802],\n",
      "        [-5.8729],\n",
      "        [-5.8666],\n",
      "        [-5.8787],\n",
      "        [-5.8741],\n",
      "        [-5.8758],\n",
      "        [-5.8674],\n",
      "        [-5.8768],\n",
      "        [-5.8800],\n",
      "        [-5.8791],\n",
      "        [-5.8801],\n",
      "        [-5.8764],\n",
      "        [-5.8802],\n",
      "        [-5.8769],\n",
      "        [-5.8786],\n",
      "        [-5.8755],\n",
      "        [-5.8790],\n",
      "        [-5.8743],\n",
      "        [-5.8787],\n",
      "        [-5.8802],\n",
      "        [-5.8800],\n",
      "        [-5.8806],\n",
      "        [-5.8803],\n",
      "        [-5.8758],\n",
      "        [-5.8793],\n",
      "        [-5.8763],\n",
      "        [-5.8677],\n",
      "        [-5.8805],\n",
      "        [-5.8764],\n",
      "        [-5.8802],\n",
      "        [-5.8783],\n",
      "        [-5.8804],\n",
      "        [-5.8800],\n",
      "        [-5.8794],\n",
      "        [-5.8801],\n",
      "        [-5.8806],\n",
      "        [-5.8819],\n",
      "        [-5.8765],\n",
      "        [-5.8801],\n",
      "        [-5.8806],\n",
      "        [-5.8796],\n",
      "        [-5.8731],\n",
      "        [-5.8724],\n",
      "        [-5.8751],\n",
      "        [-5.8664],\n",
      "        [-5.8817],\n",
      "        [-5.8801],\n",
      "        [-5.8788],\n",
      "        [-5.8694],\n",
      "        [-5.8714],\n",
      "        [-5.8801],\n",
      "        [-5.8782],\n",
      "        [-5.8778],\n",
      "        [-5.8829],\n",
      "        [-5.8799],\n",
      "        [-5.8773],\n",
      "        [-5.8713],\n",
      "        [-5.8802],\n",
      "        [-5.8623],\n",
      "        [-5.8761],\n",
      "        [-5.8802],\n",
      "        [-5.8778],\n",
      "        [-5.8803],\n",
      "        [-5.8664],\n",
      "        [-5.8794],\n",
      "        [-5.8829],\n",
      "        [-5.8799],\n",
      "        [-5.8801],\n",
      "        [-5.8798],\n",
      "        [-5.8806],\n",
      "        [-5.8740]], requires_grad=True))\n",
      "('out_layers_w_var.2', Parameter containing:\n",
      "tensor([[-5.8759],\n",
      "        [-5.8650],\n",
      "        [-5.8767],\n",
      "        [-5.8802],\n",
      "        [-5.8793],\n",
      "        [-5.8802],\n",
      "        [-5.8889],\n",
      "        [-5.8752],\n",
      "        [-5.8787],\n",
      "        [-5.8746],\n",
      "        [-5.8598],\n",
      "        [-5.8701],\n",
      "        [-5.8806],\n",
      "        [-5.8689],\n",
      "        [-5.8736],\n",
      "        [-5.8837],\n",
      "        [-5.8674],\n",
      "        [-5.8631],\n",
      "        [-5.8785],\n",
      "        [-5.8802],\n",
      "        [-5.8802],\n",
      "        [-5.8714],\n",
      "        [-5.8712],\n",
      "        [-5.8804],\n",
      "        [-5.8760],\n",
      "        [-5.8689],\n",
      "        [-5.8826],\n",
      "        [-5.8564],\n",
      "        [-5.8789],\n",
      "        [-5.8800],\n",
      "        [-5.8802],\n",
      "        [-5.8686],\n",
      "        [-5.8775],\n",
      "        [-5.8818],\n",
      "        [-5.8715],\n",
      "        [-5.8788],\n",
      "        [-5.8846],\n",
      "        [-5.8747],\n",
      "        [-5.8667],\n",
      "        [-5.8684],\n",
      "        [-5.8707],\n",
      "        [-5.8820],\n",
      "        [-5.8756],\n",
      "        [-5.8821],\n",
      "        [-5.8779],\n",
      "        [-5.8789],\n",
      "        [-5.8811],\n",
      "        [-5.8743],\n",
      "        [-5.8727],\n",
      "        [-5.8793],\n",
      "        [-5.8672],\n",
      "        [-5.8703],\n",
      "        [-5.8756],\n",
      "        [-5.8769],\n",
      "        [-5.8734],\n",
      "        [-5.8803],\n",
      "        [-5.8781],\n",
      "        [-5.8762],\n",
      "        [-5.8800],\n",
      "        [-5.8757],\n",
      "        [-5.8802],\n",
      "        [-5.8912],\n",
      "        [-5.8801],\n",
      "        [-5.8770],\n",
      "        [-5.8803],\n",
      "        [-5.8603],\n",
      "        [-5.8758],\n",
      "        [-5.8796],\n",
      "        [-5.8762],\n",
      "        [-5.8768],\n",
      "        [-5.8788],\n",
      "        [-5.8802],\n",
      "        [-5.8742],\n",
      "        [-5.8708],\n",
      "        [-5.8802],\n",
      "        [-5.8816],\n",
      "        [-5.8737],\n",
      "        [-5.8779],\n",
      "        [-5.8744],\n",
      "        [-5.8802],\n",
      "        [-5.8746],\n",
      "        [-5.8732],\n",
      "        [-5.8711],\n",
      "        [-5.8804],\n",
      "        [-5.8698],\n",
      "        [-5.8678],\n",
      "        [-5.8801],\n",
      "        [-5.8709],\n",
      "        [-5.8710],\n",
      "        [-5.8594],\n",
      "        [-5.8805],\n",
      "        [-5.8622],\n",
      "        [-5.8674],\n",
      "        [-5.8677],\n",
      "        [-5.8768],\n",
      "        [-5.8656],\n",
      "        [-5.8763],\n",
      "        [-5.8782],\n",
      "        [-5.8802],\n",
      "        [-5.8802],\n",
      "        [-5.8776],\n",
      "        [-5.8803],\n",
      "        [-5.8802],\n",
      "        [-5.8698],\n",
      "        [-5.8682],\n",
      "        [-5.8806],\n",
      "        [-5.8800],\n",
      "        [-5.8748],\n",
      "        [-5.8632],\n",
      "        [-5.8794],\n",
      "        [-5.8797],\n",
      "        [-5.8667],\n",
      "        [-5.8813],\n",
      "        [-5.8797],\n",
      "        [-5.8816],\n",
      "        [-5.8728],\n",
      "        [-5.8791],\n",
      "        [-5.8712],\n",
      "        [-5.8723],\n",
      "        [-5.8772],\n",
      "        [-5.8699],\n",
      "        [-5.8802],\n",
      "        [-5.8756],\n",
      "        [-5.8802],\n",
      "        [-5.8759],\n",
      "        [-5.8799],\n",
      "        [-5.8797],\n",
      "        [-5.8763],\n",
      "        [-5.8669],\n",
      "        [-5.8776],\n",
      "        [-5.8691],\n",
      "        [-5.8696],\n",
      "        [-5.8739],\n",
      "        [-5.8796],\n",
      "        [-5.8807],\n",
      "        [-5.8683],\n",
      "        [-5.8613],\n",
      "        [-5.8795],\n",
      "        [-5.8802],\n",
      "        [-5.8802],\n",
      "        [-5.8655],\n",
      "        [-5.8619],\n",
      "        [-5.8800],\n",
      "        [-5.8792],\n",
      "        [-5.8802],\n",
      "        [-5.8784],\n",
      "        [-5.8668],\n",
      "        [-5.8716],\n",
      "        [-5.8774],\n",
      "        [-5.8783],\n",
      "        [-5.8733],\n",
      "        [-5.8787],\n",
      "        [-5.8802],\n",
      "        [-5.8936],\n",
      "        [-5.8802],\n",
      "        [-5.8688],\n",
      "        [-5.8802],\n",
      "        [-5.8813],\n",
      "        [-5.8714],\n",
      "        [-5.8787],\n",
      "        [-5.8728],\n",
      "        [-5.8718],\n",
      "        [-5.8762],\n",
      "        [-5.8672],\n",
      "        [-5.8715],\n",
      "        [-5.8802],\n",
      "        [-5.8787],\n",
      "        [-5.8779],\n",
      "        [-5.8751],\n",
      "        [-5.8659],\n",
      "        [-5.8790],\n",
      "        [-5.8763],\n",
      "        [-5.8785],\n",
      "        [-5.8871],\n",
      "        [-5.8779],\n",
      "        [-5.8764],\n",
      "        [-5.8721],\n",
      "        [-5.8800],\n",
      "        [-5.8735],\n",
      "        [-5.8726],\n",
      "        [-5.8792],\n",
      "        [-5.8646],\n",
      "        [-5.8704],\n",
      "        [-5.8864],\n",
      "        [-5.8802],\n",
      "        [-5.8802],\n",
      "        [-5.8712],\n",
      "        [-5.8633],\n",
      "        [-5.8810],\n",
      "        [-5.8740],\n",
      "        [-5.8719],\n",
      "        [-5.8665],\n",
      "        [-5.8699],\n",
      "        [-5.8800],\n",
      "        [-5.8796],\n",
      "        [-5.8801],\n",
      "        [-5.8751],\n",
      "        [-5.8806],\n",
      "        [-5.8755],\n",
      "        [-5.8774],\n",
      "        [-5.8735],\n",
      "        [-5.8815],\n",
      "        [-5.8794],\n",
      "        [-5.8818],\n",
      "        [-5.8802],\n",
      "        [-5.8801],\n",
      "        [-5.8797],\n",
      "        [-5.8802],\n",
      "        [-5.8754],\n",
      "        [-5.8745],\n",
      "        [-5.8764],\n",
      "        [-5.8643],\n",
      "        [-5.8802],\n",
      "        [-5.8726],\n",
      "        [-5.8801],\n",
      "        [-5.8766],\n",
      "        [-5.8808],\n",
      "        [-5.8794],\n",
      "        [-5.8804],\n",
      "        [-5.8803],\n",
      "        [-5.8811],\n",
      "        [-5.8804],\n",
      "        [-5.8731],\n",
      "        [-5.8802],\n",
      "        [-5.8810],\n",
      "        [-5.8759],\n",
      "        [-5.8755],\n",
      "        [-5.8753],\n",
      "        [-5.8775],\n",
      "        [-5.8580],\n",
      "        [-5.8811],\n",
      "        [-5.8802],\n",
      "        [-5.8661],\n",
      "        [-5.8661],\n",
      "        [-5.8702],\n",
      "        [-5.8805],\n",
      "        [-5.8797],\n",
      "        [-5.8799],\n",
      "        [-5.8841],\n",
      "        [-5.8801],\n",
      "        [-5.8756],\n",
      "        [-5.8717],\n",
      "        [-5.8802],\n",
      "        [-5.8649],\n",
      "        [-5.8751],\n",
      "        [-5.8802],\n",
      "        [-5.8753],\n",
      "        [-5.8850],\n",
      "        [-5.8581],\n",
      "        [-5.8795],\n",
      "        [-5.8790],\n",
      "        [-5.8804],\n",
      "        [-5.8801],\n",
      "        [-5.8788],\n",
      "        [-5.8808],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        [-5.8752]], requires_grad=True))\n",
      "('out_layers_w_var.3', Parameter containing:\n",
      "tensor([[-5.8761],\n",
      "        [-5.8590],\n",
      "        [-5.8774],\n",
      "        [-5.8802],\n",
      "        [-5.8799],\n",
      "        [-5.8802],\n",
      "        [-5.8896],\n",
      "        [-5.8752],\n",
      "        [-5.8786],\n",
      "        [-5.8753],\n",
      "        [-5.8639],\n",
      "        [-5.8670],\n",
      "        [-5.8779],\n",
      "        [-5.8656],\n",
      "        [-5.8716],\n",
      "        [-5.8808],\n",
      "        [-5.8623],\n",
      "        [-5.8631],\n",
      "        [-5.8835],\n",
      "        [-5.8802],\n",
      "        [-5.8802],\n",
      "        [-5.8712],\n",
      "        [-5.8690],\n",
      "        [-5.8806],\n",
      "        [-5.8746],\n",
      "        [-5.8697],\n",
      "        [-5.8842],\n",
      "        [-5.8527],\n",
      "        [-5.8792],\n",
      "        [-5.8799],\n",
      "        [-5.8802],\n",
      "        [-5.8675],\n",
      "        [-5.8765],\n",
      "        [-5.8797],\n",
      "        [-5.8756],\n",
      "        [-5.8797],\n",
      "        [-5.8915],\n",
      "        [-5.8741],\n",
      "        [-5.8655],\n",
      "        [-5.8722],\n",
      "        [-5.8751],\n",
      "        [-5.8821],\n",
      "        [-5.8715],\n",
      "        [-5.8814],\n",
      "        [-5.8830],\n",
      "        [-5.8791],\n",
      "        [-5.8759],\n",
      "        [-5.8683],\n",
      "        [-5.8644],\n",
      "        [-5.8798],\n",
      "        [-5.8600],\n",
      "        [-5.8672],\n",
      "        [-5.8767],\n",
      "        [-5.8727],\n",
      "        [-5.8710],\n",
      "        [-5.8802],\n",
      "        [-5.8782],\n",
      "        [-5.8747],\n",
      "        [-5.8805],\n",
      "        [-5.8746],\n",
      "        [-5.8802],\n",
      "        [-5.8939],\n",
      "        [-5.8799],\n",
      "        [-5.8760],\n",
      "        [-5.8802],\n",
      "        [-5.8567],\n",
      "        [-5.8758],\n",
      "        [-5.8795],\n",
      "        [-5.8767],\n",
      "        [-5.8771],\n",
      "        [-5.8777],\n",
      "        [-5.8802],\n",
      "        [-5.8744],\n",
      "        [-5.8746],\n",
      "        [-5.8802],\n",
      "        [-5.8815],\n",
      "        [-5.8733],\n",
      "        [-5.8808],\n",
      "        [-5.8738],\n",
      "        [-5.8803],\n",
      "        [-5.8772],\n",
      "        [-5.8728],\n",
      "        [-5.8670],\n",
      "        [-5.8798],\n",
      "        [-5.8684],\n",
      "        [-5.8706],\n",
      "        [-5.8800],\n",
      "        [-5.8681],\n",
      "        [-5.8695],\n",
      "        [-5.8650],\n",
      "        [-5.8802],\n",
      "        [-5.8594],\n",
      "        [-5.8733],\n",
      "        [-5.8600],\n",
      "        [-5.8784],\n",
      "        [-5.8605],\n",
      "        [-5.8770],\n",
      "        [-5.8799],\n",
      "        [-5.8802],\n",
      "        [-5.8802],\n",
      "        [-5.8811],\n",
      "        [-5.8802],\n",
      "        [-5.8801],\n",
      "        [-5.8704],\n",
      "        [-5.8702],\n",
      "        [-5.8810],\n",
      "        [-5.8803],\n",
      "        [-5.8762],\n",
      "        [-5.8649],\n",
      "        [-5.8804],\n",
      "        [-5.8801],\n",
      "        [-5.8668],\n",
      "        [-5.8860],\n",
      "        [-5.8800],\n",
      "        [-5.8832],\n",
      "        [-5.8737],\n",
      "        [-5.8782],\n",
      "        [-5.8775],\n",
      "        [-5.8724],\n",
      "        [-5.8750],\n",
      "        [-5.8693],\n",
      "        [-5.8809],\n",
      "        [-5.8711],\n",
      "        [-5.8802],\n",
      "        [-5.8763],\n",
      "        [-5.8798],\n",
      "        [-5.8730],\n",
      "        [-5.8767],\n",
      "        [-5.8669],\n",
      "        [-5.8750],\n",
      "        [-5.8739],\n",
      "        [-5.8658],\n",
      "        [-5.8785],\n",
      "        [-5.8809],\n",
      "        [-5.8802],\n",
      "        [-5.8662],\n",
      "        [-5.8639],\n",
      "        [-5.8804],\n",
      "        [-5.8801],\n",
      "        [-5.8810],\n",
      "        [-5.8636],\n",
      "        [-5.8617],\n",
      "        [-5.8789],\n",
      "        [-5.8783],\n",
      "        [-5.8802],\n",
      "        [-5.8790],\n",
      "        [-5.8660],\n",
      "        [-5.8773],\n",
      "        [-5.8738],\n",
      "        [-5.8746],\n",
      "        [-5.8744],\n",
      "        [-5.8798],\n",
      "        [-5.8807],\n",
      "        [-5.8895],\n",
      "        [-5.8801],\n",
      "        [-5.8649],\n",
      "        [-5.8802],\n",
      "        [-5.8799],\n",
      "        [-5.8658],\n",
      "        [-5.8788],\n",
      "        [-5.8731],\n",
      "        [-5.8680],\n",
      "        [-5.8761],\n",
      "        [-5.8709],\n",
      "        [-5.8760],\n",
      "        [-5.8802],\n",
      "        [-5.8777],\n",
      "        [-5.8731],\n",
      "        [-5.8760],\n",
      "        [-5.8692],\n",
      "        [-5.8781],\n",
      "        [-5.8705],\n",
      "        [-5.8762],\n",
      "        [-5.8890],\n",
      "        [-5.8702],\n",
      "        [-5.8785],\n",
      "        [-5.8757],\n",
      "        [-5.8800],\n",
      "        [-5.8678],\n",
      "        [-5.8726],\n",
      "        [-5.8827],\n",
      "        [-5.8657],\n",
      "        [-5.8721],\n",
      "        [-5.8793],\n",
      "        [-5.8794],\n",
      "        [-5.8802],\n",
      "        [-5.8699],\n",
      "        [-5.8676],\n",
      "        [-5.8817],\n",
      "        [-5.8748],\n",
      "        [-5.8739],\n",
      "        [-5.8699],\n",
      "        [-5.8703],\n",
      "        [-5.8799],\n",
      "        [-5.8753],\n",
      "        [-5.8801],\n",
      "        [-5.8753],\n",
      "        [-5.8806],\n",
      "        [-5.8766],\n",
      "        [-5.8816],\n",
      "        [-5.8763],\n",
      "        [-5.8816],\n",
      "        [-5.8821],\n",
      "        [-5.8771],\n",
      "        [-5.8801],\n",
      "        [-5.8801],\n",
      "        [-5.8789],\n",
      "        [-5.8802],\n",
      "        [-5.8762],\n",
      "        [-5.8726],\n",
      "        [-5.8804],\n",
      "        [-5.8684],\n",
      "        [-5.8802],\n",
      "        [-5.8716],\n",
      "        [-5.8802],\n",
      "        [-5.8822],\n",
      "        [-5.8804],\n",
      "        [-5.8789],\n",
      "        [-5.8785],\n",
      "        [-5.8808],\n",
      "        [-5.8807],\n",
      "        [-5.8815],\n",
      "        [-5.8737],\n",
      "        [-5.8802],\n",
      "        [-5.8800],\n",
      "        [-5.8744],\n",
      "        [-5.8777],\n",
      "        [-5.8841],\n",
      "        [-5.8717],\n",
      "        [-5.8611],\n",
      "        [-5.8846],\n",
      "        [-5.8802],\n",
      "        [-5.8670],\n",
      "        [-5.8677],\n",
      "        [-5.8762],\n",
      "        [-5.8803],\n",
      "        [-5.8783],\n",
      "        [-5.8781],\n",
      "        [-5.8834],\n",
      "        [-5.8802],\n",
      "        [-5.8767],\n",
      "        [-5.8721],\n",
      "        [-5.8800],\n",
      "        [-5.8691],\n",
      "        [-5.8799],\n",
      "        [-5.8802],\n",
      "        [-5.8733],\n",
      "        [-5.8798],\n",
      "        [-5.8557],\n",
      "        [-5.8802],\n",
      "        [-5.8810],\n",
      "        [-5.8804],\n",
      "        [-5.8802],\n",
      "        [-5.8792],\n",
      "        [-5.8793],\n",
      "        [-5.8726]], requires_grad=True))\n",
      "('out_layers_w_var.4', Parameter containing:\n",
      "tensor([[-5.8770],\n",
      "        [-5.8579],\n",
      "        [-5.8774],\n",
      "        [-5.8802],\n",
      "        [-5.8797],\n",
      "        [-5.8802],\n",
      "        [-5.8892],\n",
      "        [-5.8744],\n",
      "        [-5.8782],\n",
      "        [-5.8796],\n",
      "        [-5.8691],\n",
      "        [-5.8638],\n",
      "        [-5.8821],\n",
      "        [-5.8639],\n",
      "        [-5.8690],\n",
      "        [-5.8828],\n",
      "        [-5.8556],\n",
      "        [-5.8612],\n",
      "        [-5.8854],\n",
      "        [-5.8802],\n",
      "        [-5.8802],\n",
      "        [-5.8690],\n",
      "        [-5.8721],\n",
      "        [-5.8837],\n",
      "        [-5.8668],\n",
      "        [-5.8741],\n",
      "        [-5.8852],\n",
      "        [-5.8504],\n",
      "        [-5.8786],\n",
      "        [-5.8801],\n",
      "        [-5.8802],\n",
      "        [-5.8670],\n",
      "        [-5.8827],\n",
      "        [-5.8850],\n",
      "        [-5.8741],\n",
      "        [-5.8792],\n",
      "        [-5.8882],\n",
      "        [-5.8765],\n",
      "        [-5.8662],\n",
      "        [-5.8629],\n",
      "        [-5.8771],\n",
      "        [-5.8773],\n",
      "        [-5.8676],\n",
      "        [-5.8776],\n",
      "        [-5.8778],\n",
      "        [-5.8769],\n",
      "        [-5.8703],\n",
      "        [-5.8695],\n",
      "        [-5.8712],\n",
      "        [-5.8786],\n",
      "        [-5.8513],\n",
      "        [-5.8713],\n",
      "        [-5.8760],\n",
      "        [-5.8652],\n",
      "        [-5.8738],\n",
      "        [-5.8800],\n",
      "        [-5.8774],\n",
      "        [-5.8814],\n",
      "        [-5.8801],\n",
      "        [-5.8696],\n",
      "        [-5.8802],\n",
      "        [-5.8978],\n",
      "        [-5.8803],\n",
      "        [-5.8765],\n",
      "        [-5.8802],\n",
      "        [-5.8581],\n",
      "        [-5.8756],\n",
      "        [-5.8781],\n",
      "        [-5.8768],\n",
      "        [-5.8776],\n",
      "        [-5.8742],\n",
      "        [-5.8802],\n",
      "        [-5.8758],\n",
      "        [-5.8753],\n",
      "        [-5.8802],\n",
      "        [-5.8813],\n",
      "        [-5.8744],\n",
      "        [-5.8844],\n",
      "        [-5.8805],\n",
      "        [-5.8803],\n",
      "        [-5.8780],\n",
      "        [-5.8754],\n",
      "        [-5.8738],\n",
      "        [-5.8791],\n",
      "        [-5.8620],\n",
      "        [-5.8715],\n",
      "        [-5.8801],\n",
      "        [-5.8741],\n",
      "        [-5.8618],\n",
      "        [-5.8719],\n",
      "        [-5.8802],\n",
      "        [-5.8549],\n",
      "        [-5.8666],\n",
      "        [-5.8634],\n",
      "        [-5.8811],\n",
      "        [-5.8672],\n",
      "        [-5.8796],\n",
      "        [-5.8805],\n",
      "        [-5.8802],\n",
      "        [-5.8802],\n",
      "        [-5.8791],\n",
      "        [-5.8802],\n",
      "        [-5.8801],\n",
      "        [-5.8791],\n",
      "        [-5.8745],\n",
      "        [-5.8809],\n",
      "        [-5.8863],\n",
      "        [-5.8809],\n",
      "        [-5.8588],\n",
      "        [-5.8803],\n",
      "        [-5.8801],\n",
      "        [-5.8652],\n",
      "        [-5.8895],\n",
      "        [-5.8800],\n",
      "        [-5.8742],\n",
      "        [-5.8731],\n",
      "        [-5.8780],\n",
      "        [-5.8751],\n",
      "        [-5.8714],\n",
      "        [-5.8756],\n",
      "        [-5.8687],\n",
      "        [-5.8801],\n",
      "        [-5.8710],\n",
      "        [-5.8802],\n",
      "        [-5.8775],\n",
      "        [-5.8798],\n",
      "        [-5.8763],\n",
      "        [-5.8805],\n",
      "        [-5.8682],\n",
      "        [-5.8742],\n",
      "        [-5.8772],\n",
      "        [-5.8725],\n",
      "        [-5.8798],\n",
      "        [-5.8829],\n",
      "        [-5.8803],\n",
      "        [-5.8613],\n",
      "        [-5.8650],\n",
      "        [-5.8804],\n",
      "        [-5.8802],\n",
      "        [-5.8808],\n",
      "        [-5.8734],\n",
      "        [-5.8601],\n",
      "        [-5.8805],\n",
      "        [-5.8779],\n",
      "        [-5.8802],\n",
      "        [-5.8777],\n",
      "        [-5.8624],\n",
      "        [-5.8850],\n",
      "        [-5.8762],\n",
      "        [-5.8735],\n",
      "        [-5.8732],\n",
      "        [-5.8800],\n",
      "        [-5.8789],\n",
      "        [-5.8916],\n",
      "        [-5.8801],\n",
      "        [-5.8674],\n",
      "        [-5.8801],\n",
      "        [-5.8838],\n",
      "        [-5.8642],\n",
      "        [-5.8767],\n",
      "        [-5.8711],\n",
      "        [-5.8695],\n",
      "        [-5.8778],\n",
      "        [-5.8701],\n",
      "        [-5.8695],\n",
      "        [-5.8802],\n",
      "        [-5.8763],\n",
      "        [-5.8740],\n",
      "        [-5.8718],\n",
      "        [-5.8654],\n",
      "        [-5.8748],\n",
      "        [-5.8748],\n",
      "        [-5.8739],\n",
      "        [-5.8820],\n",
      "        [-5.8599],\n",
      "        [-5.8790],\n",
      "        [-5.8828],\n",
      "        [-5.8795],\n",
      "        [-5.8767],\n",
      "        [-5.8717],\n",
      "        [-5.8781],\n",
      "        [-5.8693],\n",
      "        [-5.8659],\n",
      "        [-5.8708],\n",
      "        [-5.8803],\n",
      "        [-5.8803],\n",
      "        [-5.8675],\n",
      "        [-5.8727],\n",
      "        [-5.8844],\n",
      "        [-5.8745],\n",
      "        [-5.8720],\n",
      "        [-5.8761],\n",
      "        [-5.8766],\n",
      "        [-5.8800],\n",
      "        [-5.8746],\n",
      "        [-5.8801],\n",
      "        [-5.8752],\n",
      "        [-5.8807],\n",
      "        [-5.8786],\n",
      "        [-5.8825],\n",
      "        [-5.8776],\n",
      "        [-5.8738],\n",
      "        [-5.8924],\n",
      "        [-5.8787],\n",
      "        [-5.8806],\n",
      "        [-5.8801],\n",
      "        [-5.8757],\n",
      "        [-5.8803],\n",
      "        [-5.8677],\n",
      "        [-5.8821],\n",
      "        [-5.8839],\n",
      "        [-5.8769],\n",
      "        [-5.8803],\n",
      "        [-5.8716],\n",
      "        [-5.8802],\n",
      "        [-5.8809],\n",
      "        [-5.8810],\n",
      "        [-5.8780],\n",
      "        [-5.8771],\n",
      "        [-5.8808],\n",
      "        [-5.8836],\n",
      "        [-5.8897],\n",
      "        [-5.8687],\n",
      "        [-5.8802],\n",
      "        [-5.8798],\n",
      "        [-5.8685],\n",
      "        [-5.8837],\n",
      "        [-5.8825],\n",
      "        [-5.8693],\n",
      "        [-5.8603],\n",
      "        [-5.8845],\n",
      "        [-5.8802],\n",
      "        [-5.8644],\n",
      "        [-5.8757],\n",
      "        [-5.8754],\n",
      "        [-5.8799],\n",
      "        [-5.8805],\n",
      "        [-5.8802],\n",
      "        [-5.8842],\n",
      "        [-5.8801],\n",
      "        [-5.8798],\n",
      "        [-5.8792],\n",
      "        [-5.8803],\n",
      "        [-5.8694],\n",
      "        [-5.8750],\n",
      "        [-5.8802],\n",
      "        [-5.8745],\n",
      "        [-5.8769],\n",
      "        [-5.8530],\n",
      "        [-5.8800],\n",
      "        [-5.8818],\n",
      "        [-5.8828],\n",
      "        [-5.8801],\n",
      "        [-5.8799],\n",
      "        [-5.8794],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        [-5.8766]], requires_grad=True))\n",
      "('out_layers_b_var.0', Parameter containing:\n",
      "tensor([[-5.8721]], requires_grad=True))\n",
      "('out_layers_b_var.1', Parameter containing:\n",
      "tensor([[-5.8816]], requires_grad=True))\n",
      "('out_layers_b_var.2', Parameter containing:\n",
      "tensor([[-5.8872]], requires_grad=True))\n",
      "('out_layers_b_var.3', Parameter containing:\n",
      "tensor([[-5.8926]], requires_grad=True))\n",
      "('out_layers_b_var.4', Parameter containing:\n",
      "tensor([[-5.8923]], requires_grad=True))\n"
     ]
    }
   ],
   "source": [
    "for i in net.named_parameters():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Module.named_parameters of Net()>\n"
     ]
    }
   ],
   "source": [
    "print(net.named_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-6.0323)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.log(torch.tensor(0.0024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(net.out_layers_b_var[0].requires_grad)\n",
    "print(net.pri_out_layers_b_var[0].requires_grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('hidden_w_mean.0', Parameter containing:\n",
      "tensor([[ 2.1353, -2.3715,  2.3291,  ..., -2.3068,  2.3694,  2.3030],\n",
      "        [ 2.3663, -2.2922,  2.3625,  ..., -2.3557, -2.3699, -2.3070],\n",
      "        [ 2.3397,  2.3608,  2.2949,  ..., -2.1888,  2.3516,  2.3501],\n",
      "        ...,\n",
      "        [ 2.3077, -2.3618, -2.2659,  ..., -2.3371, -2.3140,  2.2923],\n",
      "        [ 2.3638, -2.3181, -2.3490,  ...,  2.3543,  2.3608, -2.3579],\n",
      "        [-2.3075, -2.3575, -2.3455,  ...,  2.3712, -2.3460,  2.3696]],\n",
      "       requires_grad=True))\n",
      "('hidden_w_mean.1', Parameter containing:\n",
      "tensor([[-0.2620,  0.3067, -0.0851,  ..., -0.1688,  0.1854,  0.1521],\n",
      "        [ 1.0431,  2.3428,  0.9990,  ...,  2.3702,  2.2967,  2.1707],\n",
      "        [ 0.1489,  0.1878, -0.1783,  ...,  0.3391,  0.3604,  0.5605],\n",
      "        ...,\n",
      "        [ 2.0066,  2.3320, -0.0949,  ..., -2.3012, -2.3646, -2.3490],\n",
      "        [ 1.0625,  0.3373, -0.0715,  ..., -0.3154,  1.1491,  0.8978],\n",
      "        [ 1.0238,  0.9340, -0.1178,  ..., -1.0098, -1.0133, -0.9456]],\n",
      "       requires_grad=True))\n",
      "('hidden_b_mean.0', Parameter containing:\n",
      "tensor([[ 7.3184e-02],\n",
      "        [ 2.3095e+00],\n",
      "        [ 2.3928e-01],\n",
      "        [-6.4068e-01],\n",
      "        [-1.7990e+00],\n",
      "        [-1.4562e-01],\n",
      "        [-1.9122e+00],\n",
      "        [-2.1879e+00],\n",
      "        [-3.2464e-01],\n",
      "        [ 2.1377e+00],\n",
      "        [-1.0339e+00],\n",
      "        [-2.1257e+00],\n",
      "        [ 1.5887e-01],\n",
      "        [-2.2501e+00],\n",
      "        [ 1.8045e+00],\n",
      "        [-1.4799e-01],\n",
      "        [-2.0567e-01],\n",
      "        [ 1.8289e+00],\n",
      "        [ 3.6023e-01],\n",
      "        [-1.6240e+00],\n",
      "        [ 4.3147e-01],\n",
      "        [-4.6853e-01],\n",
      "        [ 1.6872e+00],\n",
      "        [ 1.2820e+00],\n",
      "        [-1.9373e+00],\n",
      "        [-4.3383e-02],\n",
      "        [-9.8448e-01],\n",
      "        [ 1.9939e+00],\n",
      "        [ 2.0662e+00],\n",
      "        [-1.8107e-01],\n",
      "        [-1.5909e-01],\n",
      "        [ 1.9802e-01],\n",
      "        [ 1.0824e+00],\n",
      "        [-2.9198e-01],\n",
      "        [ 3.6427e-01],\n",
      "        [ 1.4874e+00],\n",
      "        [ 6.5592e-01],\n",
      "        [ 2.2273e+00],\n",
      "        [ 1.6841e-01],\n",
      "        [-1.2515e+00],\n",
      "        [ 2.1709e+00],\n",
      "        [ 1.5919e+00],\n",
      "        [-1.5683e+00],\n",
      "        [ 2.0946e+00],\n",
      "        [-1.4337e-01],\n",
      "        [-4.7994e-01],\n",
      "        [ 2.1111e+00],\n",
      "        [-2.3243e+00],\n",
      "        [-1.6241e+00],\n",
      "        [ 1.4818e+00],\n",
      "        [-1.1841e+00],\n",
      "        [-2.3649e+00],\n",
      "        [ 8.9621e-01],\n",
      "        [ 2.2648e+00],\n",
      "        [-1.0091e+00],\n",
      "        [-2.2968e-02],\n",
      "        [-1.9315e+00],\n",
      "        [ 6.4050e-01],\n",
      "        [-1.0873e+00],\n",
      "        [-3.4036e-01],\n",
      "        [ 2.1877e-01],\n",
      "        [ 1.6753e+00],\n",
      "        [ 1.0929e-01],\n",
      "        [ 7.7575e-01],\n",
      "        [ 1.9257e+00],\n",
      "        [-1.9215e+00],\n",
      "        [-1.5054e+00],\n",
      "        [-2.1113e+00],\n",
      "        [ 6.9371e-01],\n",
      "        [ 8.2430e-01],\n",
      "        [ 2.0484e+00],\n",
      "        [-2.0822e+00],\n",
      "        [ 2.2434e+00],\n",
      "        [ 3.8886e-01],\n",
      "        [ 1.8353e+00],\n",
      "        [ 2.3222e+00],\n",
      "        [-9.8905e-01],\n",
      "        [ 1.9898e-01],\n",
      "        [ 1.7899e+00],\n",
      "        [-2.2631e+00],\n",
      "        [-2.9900e-01],\n",
      "        [ 1.3400e+00],\n",
      "        [-1.4398e+00],\n",
      "        [-4.5716e-01],\n",
      "        [-1.8784e+00],\n",
      "        [ 1.5705e+00],\n",
      "        [ 1.4737e+00],\n",
      "        [ 2.0819e+00],\n",
      "        [ 1.5742e+00],\n",
      "        [ 1.7253e+00],\n",
      "        [-4.8422e-04],\n",
      "        [ 2.2888e+00],\n",
      "        [ 1.7451e+00],\n",
      "        [-2.3041e+00],\n",
      "        [ 1.9469e+00],\n",
      "        [ 1.5928e-02],\n",
      "        [-1.6836e+00],\n",
      "        [ 3.2561e-02],\n",
      "        [-2.4873e-01],\n",
      "        [-9.2043e-02],\n",
      "        [ 1.0977e+00],\n",
      "        [-5.3692e-01],\n",
      "        [ 2.2932e+00],\n",
      "        [ 2.2232e+00],\n",
      "        [-8.9688e-01],\n",
      "        [-1.8646e+00],\n",
      "        [-1.7328e+00],\n",
      "        [-2.3480e+00],\n",
      "        [-2.3502e+00],\n",
      "        [-2.2532e+00],\n",
      "        [-5.4669e-02],\n",
      "        [ 2.3352e+00],\n",
      "        [-1.6414e+00],\n",
      "        [-2.3347e+00],\n",
      "        [-3.9569e-01],\n",
      "        [ 5.7118e-01],\n",
      "        [-2.1611e+00],\n",
      "        [ 3.3158e-01],\n",
      "        [-2.2303e+00],\n",
      "        [ 1.5624e+00],\n",
      "        [ 6.3964e-01],\n",
      "        [ 2.1244e+00],\n",
      "        [ 1.1079e+00],\n",
      "        [ 2.3239e+00],\n",
      "        [-2.0749e+00],\n",
      "        [-2.3692e+00],\n",
      "        [-6.4252e-01],\n",
      "        [-1.2143e-01],\n",
      "        [-3.6193e-01],\n",
      "        [-1.7455e+00],\n",
      "        [ 5.9657e-01],\n",
      "        [ 1.9306e+00],\n",
      "        [-1.2006e+00],\n",
      "        [-1.7680e+00],\n",
      "        [-4.0732e-01],\n",
      "        [ 1.6800e+00],\n",
      "        [ 1.9934e+00],\n",
      "        [ 1.8685e+00],\n",
      "        [-9.0322e-01],\n",
      "        [ 7.2160e-01],\n",
      "        [ 9.7510e-01],\n",
      "        [-1.6433e+00],\n",
      "        [ 1.6456e+00],\n",
      "        [ 2.2558e+00],\n",
      "        [-2.5056e-01],\n",
      "        [ 1.5971e+00],\n",
      "        [ 1.8624e+00],\n",
      "        [-1.3201e+00],\n",
      "        [-2.2802e+00],\n",
      "        [-9.4493e-01],\n",
      "        [ 2.2291e+00],\n",
      "        [ 2.0318e+00],\n",
      "        [-1.4662e+00],\n",
      "        [ 1.7874e+00],\n",
      "        [ 5.5809e-01],\n",
      "        [ 3.2492e-01],\n",
      "        [ 9.5593e-01],\n",
      "        [ 1.5520e+00],\n",
      "        [ 1.1289e+00],\n",
      "        [-1.9133e+00],\n",
      "        [-1.5007e+00],\n",
      "        [-5.1735e-01],\n",
      "        [-1.3164e+00],\n",
      "        [-1.1719e+00],\n",
      "        [-8.8193e-02],\n",
      "        [-2.7175e-01],\n",
      "        [ 8.1259e-01],\n",
      "        [ 1.5646e+00],\n",
      "        [-1.3514e+00],\n",
      "        [ 1.4241e+00],\n",
      "        [-2.1133e+00],\n",
      "        [ 4.6230e-01],\n",
      "        [ 8.1658e-01],\n",
      "        [ 1.9523e+00],\n",
      "        [ 3.3370e-01],\n",
      "        [-7.5962e-01],\n",
      "        [-6.6080e-01],\n",
      "        [ 1.0954e+00],\n",
      "        [ 5.7147e-01],\n",
      "        [ 2.2426e+00],\n",
      "        [ 2.0248e+00],\n",
      "        [-3.1514e-01],\n",
      "        [ 1.2594e+00],\n",
      "        [ 1.2579e+00],\n",
      "        [-1.4259e+00],\n",
      "        [-2.3306e+00],\n",
      "        [ 3.7031e-01],\n",
      "        [ 2.9298e-01],\n",
      "        [-6.0299e-01],\n",
      "        [ 4.9932e-03],\n",
      "        [ 1.6673e+00],\n",
      "        [ 1.3413e-01],\n",
      "        [ 1.8450e-01],\n",
      "        [-1.8123e+00],\n",
      "        [ 1.1755e+00],\n",
      "        [ 1.8019e+00],\n",
      "        [ 2.3171e+00],\n",
      "        [ 2.1262e+00],\n",
      "        [-1.4032e+00],\n",
      "        [ 9.0120e-03],\n",
      "        [-2.3621e+00],\n",
      "        [ 1.4306e+00],\n",
      "        [ 4.9074e-01],\n",
      "        [-2.1489e+00],\n",
      "        [-5.8219e-01],\n",
      "        [-1.9163e-01],\n",
      "        [-1.3347e+00],\n",
      "        [-1.9459e+00],\n",
      "        [ 1.8615e+00],\n",
      "        [ 2.2288e+00],\n",
      "        [-1.7391e+00],\n",
      "        [ 1.5195e+00],\n",
      "        [-2.2377e+00],\n",
      "        [-1.3816e+00],\n",
      "        [ 2.2324e+00],\n",
      "        [-1.6699e+00],\n",
      "        [ 1.6949e+00],\n",
      "        [ 1.4249e-01],\n",
      "        [ 2.2246e+00],\n",
      "        [-1.1422e+00],\n",
      "        [-7.6349e-02],\n",
      "        [-2.2832e+00],\n",
      "        [ 2.0721e+00],\n",
      "        [ 2.1739e+00],\n",
      "        [-5.3654e-01],\n",
      "        [ 1.3899e+00],\n",
      "        [-2.2327e+00],\n",
      "        [-2.2353e+00],\n",
      "        [-1.3775e+00],\n",
      "        [-2.0349e+00],\n",
      "        [-1.9381e-02],\n",
      "        [-2.2919e+00],\n",
      "        [-1.9251e+00],\n",
      "        [ 6.7566e-02],\n",
      "        [ 2.3295e+00],\n",
      "        [-5.6011e-01],\n",
      "        [-2.0987e+00],\n",
      "        [-4.4961e-02],\n",
      "        [ 2.1857e+00],\n",
      "        [-1.7167e+00],\n",
      "        [ 1.6171e+00],\n",
      "        [ 1.1269e+00],\n",
      "        [ 3.8000e-01],\n",
      "        [ 2.8544e-02],\n",
      "        [-1.7354e+00],\n",
      "        [-1.9645e-01],\n",
      "        [ 9.8288e-01],\n",
      "        [-1.9034e+00],\n",
      "        [ 1.1573e-01],\n",
      "        [-6.8918e-01],\n",
      "        [-1.2816e+00],\n",
      "        [ 1.0396e+00],\n",
      "        [-1.2297e+00],\n",
      "        [ 2.3051e+00],\n",
      "        [-1.7230e+00],\n",
      "        [ 9.5795e-01]], requires_grad=True))\n",
      "('hidden_b_mean.1', Parameter containing:\n",
      "tensor([[ 2.3284],\n",
      "        [-2.2878],\n",
      "        [ 2.3243],\n",
      "        [ 2.2949],\n",
      "        [ 2.2987],\n",
      "        [-2.3647],\n",
      "        [ 2.2774],\n",
      "        [-2.3026],\n",
      "        [-2.3524],\n",
      "        [-2.3574],\n",
      "        [-1.9553],\n",
      "        [ 2.2866],\n",
      "        [ 2.3241],\n",
      "        [ 2.2968],\n",
      "        [-2.3202],\n",
      "        [ 2.3536],\n",
      "        [-2.3653],\n",
      "        [-2.3072],\n",
      "        [ 2.2792],\n",
      "        [-2.3388],\n",
      "        [ 2.3295],\n",
      "        [ 2.3564],\n",
      "        [ 2.3659],\n",
      "        [ 2.2979],\n",
      "        [ 2.3445],\n",
      "        [-2.2872],\n",
      "        [ 2.3401],\n",
      "        [-2.3069],\n",
      "        [ 2.3704],\n",
      "        [ 2.3466],\n",
      "        [ 2.3628],\n",
      "        [-2.2957],\n",
      "        [-2.3082],\n",
      "        [-2.2802],\n",
      "        [-2.3298],\n",
      "        [ 2.3469],\n",
      "        [ 2.3365],\n",
      "        [-2.3269],\n",
      "        [-2.3694],\n",
      "        [-2.3457],\n",
      "        [ 2.2776],\n",
      "        [ 2.2621],\n",
      "        [-2.3373],\n",
      "        [-2.2931],\n",
      "        [-2.3598],\n",
      "        [-2.3182],\n",
      "        [ 2.3262],\n",
      "        [-2.3710],\n",
      "        [ 2.3650],\n",
      "        [-2.3577],\n",
      "        [-1.8887],\n",
      "        [-2.3508],\n",
      "        [ 2.3511],\n",
      "        [ 2.3332],\n",
      "        [ 2.3098],\n",
      "        [-2.3287],\n",
      "        [ 2.3653],\n",
      "        [ 2.3656],\n",
      "        [ 2.2948],\n",
      "        [-2.3042],\n",
      "        [-2.3792],\n",
      "        [ 2.3684],\n",
      "        [ 2.3656],\n",
      "        [ 2.3088],\n",
      "        [ 2.3282],\n",
      "        [ 2.3370],\n",
      "        [-2.3430],\n",
      "        [-2.3169],\n",
      "        [ 2.2968],\n",
      "        [ 2.3291],\n",
      "        [-2.3328],\n",
      "        [ 2.3664],\n",
      "        [-2.3319],\n",
      "        [ 2.3057],\n",
      "        [ 2.3442],\n",
      "        [-2.2979],\n",
      "        [-2.3433],\n",
      "        [-2.3009],\n",
      "        [ 2.3592],\n",
      "        [ 2.3472],\n",
      "        [-2.3461],\n",
      "        [ 2.3012],\n",
      "        [ 2.3420],\n",
      "        [-2.3672],\n",
      "        [-2.3662],\n",
      "        [ 2.3282],\n",
      "        [ 2.2819],\n",
      "        [ 2.3574],\n",
      "        [-2.3191],\n",
      "        [ 2.3138],\n",
      "        [ 2.3170],\n",
      "        [-2.3093],\n",
      "        [ 2.3199],\n",
      "        [-2.2916],\n",
      "        [-2.3305],\n",
      "        [-2.3099],\n",
      "        [-2.2948],\n",
      "        [-2.3504],\n",
      "        [-2.2695],\n",
      "        [-2.3451],\n",
      "        [ 2.3058],\n",
      "        [-2.3656],\n",
      "        [-2.2801],\n",
      "        [-2.3643],\n",
      "        [ 1.8750],\n",
      "        [-2.2265],\n",
      "        [ 2.2886],\n",
      "        [-2.3628],\n",
      "        [ 2.2371],\n",
      "        [-2.3357],\n",
      "        [-2.3137],\n",
      "        [-2.3420],\n",
      "        [-2.2875],\n",
      "        [-2.3271],\n",
      "        [-2.2890],\n",
      "        [-2.3204],\n",
      "        [ 2.3380],\n",
      "        [ 1.8184],\n",
      "        [-2.3558],\n",
      "        [-2.3475],\n",
      "        [ 2.3649],\n",
      "        [-2.3241],\n",
      "        [-2.3274],\n",
      "        [ 2.3552],\n",
      "        [-2.2963],\n",
      "        [ 2.3693],\n",
      "        [-2.3324],\n",
      "        [-2.3361],\n",
      "        [-2.3443],\n",
      "        [-2.3695],\n",
      "        [-2.3119],\n",
      "        [-2.3477],\n",
      "        [-2.3212],\n",
      "        [-2.3517],\n",
      "        [-2.2951],\n",
      "        [-2.3110],\n",
      "        [ 2.3643],\n",
      "        [-2.3010],\n",
      "        [-2.3569],\n",
      "        [ 2.3329],\n",
      "        [-2.3310],\n",
      "        [-1.7785],\n",
      "        [-2.3209],\n",
      "        [ 2.2924],\n",
      "        [ 2.3718],\n",
      "        [ 2.3123],\n",
      "        [-2.3400],\n",
      "        [-2.3383],\n",
      "        [ 2.3310],\n",
      "        [-2.3521],\n",
      "        [ 2.3703],\n",
      "        [-2.3221],\n",
      "        [ 2.3252],\n",
      "        [-2.1616],\n",
      "        [ 2.2350],\n",
      "        [-2.3053],\n",
      "        [ 2.3177],\n",
      "        [ 2.3522],\n",
      "        [-2.3342],\n",
      "        [-2.3005],\n",
      "        [-2.2968],\n",
      "        [ 2.3373],\n",
      "        [ 2.2686],\n",
      "        [ 2.3101],\n",
      "        [ 2.3499],\n",
      "        [-2.3389],\n",
      "        [-2.2929],\n",
      "        [ 2.3472],\n",
      "        [-2.3122],\n",
      "        [-2.3185],\n",
      "        [ 2.3090],\n",
      "        [ 2.3157],\n",
      "        [-2.2910],\n",
      "        [-2.3282],\n",
      "        [-2.2993],\n",
      "        [ 2.3680],\n",
      "        [-2.3551],\n",
      "        [ 2.3056],\n",
      "        [-2.3530],\n",
      "        [-2.3505],\n",
      "        [ 2.3072],\n",
      "        [ 2.3607],\n",
      "        [-2.2717],\n",
      "        [-2.3275],\n",
      "        [ 2.2858],\n",
      "        [-2.3661],\n",
      "        [-2.3214],\n",
      "        [ 2.3426],\n",
      "        [ 2.3242],\n",
      "        [-2.3657],\n",
      "        [-2.3042],\n",
      "        [ 2.3584],\n",
      "        [ 2.3047],\n",
      "        [-2.2786],\n",
      "        [-2.3598],\n",
      "        [-2.3631],\n",
      "        [ 2.3590],\n",
      "        [-2.3500],\n",
      "        [ 2.3580],\n",
      "        [-2.3660],\n",
      "        [ 2.3535],\n",
      "        [ 2.3250],\n",
      "        [-2.3521],\n",
      "        [ 2.2906],\n",
      "        [ 2.3656],\n",
      "        [ 2.3079],\n",
      "        [-2.3331],\n",
      "        [ 2.3408],\n",
      "        [-2.3617],\n",
      "        [-2.2947],\n",
      "        [-2.2921],\n",
      "        [ 2.3076],\n",
      "        [-2.3033],\n",
      "        [ 2.3467],\n",
      "        [-2.2843],\n",
      "        [ 2.3641],\n",
      "        [-2.3158],\n",
      "        [-2.3615],\n",
      "        [ 2.3284],\n",
      "        [-2.3645],\n",
      "        [ 2.3521],\n",
      "        [ 1.8501],\n",
      "        [ 2.3185],\n",
      "        [ 2.3118],\n",
      "        [-2.3156],\n",
      "        [-2.3615],\n",
      "        [ 2.3002],\n",
      "        [ 2.3662],\n",
      "        [-2.3595],\n",
      "        [-2.3262],\n",
      "        [-2.2898],\n",
      "        [-2.2942],\n",
      "        [-2.3319],\n",
      "        [-2.2954],\n",
      "        [-2.3178],\n",
      "        [-2.3587],\n",
      "        [-2.2946],\n",
      "        [ 2.3545],\n",
      "        [ 2.3711],\n",
      "        [ 2.3465],\n",
      "        [-2.3293],\n",
      "        [ 2.3500],\n",
      "        [ 2.3704],\n",
      "        [-2.3585],\n",
      "        [-2.3441],\n",
      "        [ 2.3054],\n",
      "        [ 2.2893],\n",
      "        [ 2.3242],\n",
      "        [-2.3352],\n",
      "        [ 2.3248],\n",
      "        [ 2.2629],\n",
      "        [ 2.3529],\n",
      "        [-2.3582],\n",
      "        [ 2.3660],\n",
      "        [-2.2939],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        [ 1.7611]], requires_grad=True))\n",
      "('hidden_w_var.0', Parameter containing:\n",
      "tensor([[-2.0060, -2.0060, -2.0060,  ..., -2.0060, -2.0060, -2.0060],\n",
      "        [-2.0060, -2.0060, -2.0060,  ..., -2.0060, -2.0060, -2.0060],\n",
      "        [-2.0060, -2.0060, -2.0060,  ..., -2.0060, -2.0060, -2.0060],\n",
      "        ...,\n",
      "        [-2.0060, -2.0060, -2.0060,  ..., -2.0060, -2.0060, -2.0060],\n",
      "        [-2.0060, -2.0060, -2.0060,  ..., -2.0060, -2.0060, -2.0060],\n",
      "        [-2.0060, -2.0060, -2.0060,  ..., -2.0060, -2.0060, -2.0060]],\n",
      "       requires_grad=True))\n",
      "('hidden_w_var.1', Parameter containing:\n",
      "tensor([[-2.0043, -1.9662, -1.9985,  ..., -2.0133, -2.0021, -2.0196],\n",
      "        [-1.9822, -2.0060, -2.0309,  ..., -2.0060, -2.0060, -2.0060],\n",
      "        [-1.9854, -2.0167, -1.9416,  ..., -2.0111, -2.0151, -1.9843],\n",
      "        ...,\n",
      "        [-2.0060, -2.0061, -2.0152,  ..., -2.0060, -2.0060, -2.0062],\n",
      "        [-2.0036, -2.0177, -1.9933,  ..., -2.0207, -1.9812, -2.0193],\n",
      "        [-1.9610, -2.0431, -1.9964,  ..., -2.0184, -1.9940, -2.0399]],\n",
      "       requires_grad=True))\n",
      "('hidden_b_var.0', Parameter containing:\n",
      "tensor([[-2.0101],\n",
      "        [-2.0063],\n",
      "        [-2.0104],\n",
      "        [-2.0159],\n",
      "        [-2.0087],\n",
      "        [-2.0054],\n",
      "        [-2.0009],\n",
      "        [-2.0058],\n",
      "        [-2.0135],\n",
      "        [-2.0087],\n",
      "        [-2.0133],\n",
      "        [-2.0069],\n",
      "        [-2.0025],\n",
      "        [-2.0082],\n",
      "        [-2.0030],\n",
      "        [-2.0068],\n",
      "        [-2.0014],\n",
      "        [-2.0048],\n",
      "        [-1.9935],\n",
      "        [-2.0063],\n",
      "        [-2.0007],\n",
      "        [-2.0073],\n",
      "        [-2.0175],\n",
      "        [-2.0107],\n",
      "        [-2.0053],\n",
      "        [-2.0074],\n",
      "        [-2.0087],\n",
      "        [-2.0094],\n",
      "        [-2.0048],\n",
      "        [-2.0108],\n",
      "        [-2.0042],\n",
      "        [-2.0062],\n",
      "        [-2.0028],\n",
      "        [-2.0072],\n",
      "        [-2.0056],\n",
      "        [-2.0032],\n",
      "        [-2.0057],\n",
      "        [-2.0073],\n",
      "        [-2.0074],\n",
      "        [-2.0134],\n",
      "        [-2.0010],\n",
      "        [-2.0171],\n",
      "        [-2.0095],\n",
      "        [-2.0109],\n",
      "        [-2.0014],\n",
      "        [-2.0016],\n",
      "        [-2.0065],\n",
      "        [-2.0059],\n",
      "        [-2.0039],\n",
      "        [-1.9952],\n",
      "        [-2.0125],\n",
      "        [-2.0060],\n",
      "        [-2.0204],\n",
      "        [-2.0062],\n",
      "        [-2.0043],\n",
      "        [-2.0124],\n",
      "        [-2.0060],\n",
      "        [-2.0061],\n",
      "        [-2.0111],\n",
      "        [-2.0034],\n",
      "        [-2.0058],\n",
      "        [-1.9966],\n",
      "        [-2.0069],\n",
      "        [-1.9872],\n",
      "        [-2.0090],\n",
      "        [-2.0001],\n",
      "        [-2.0036],\n",
      "        [-2.0069],\n",
      "        [-2.0111],\n",
      "        [-2.0021],\n",
      "        [-2.0064],\n",
      "        [-2.0044],\n",
      "        [-2.0053],\n",
      "        [-2.0086],\n",
      "        [-2.0017],\n",
      "        [-2.0079],\n",
      "        [-2.0048],\n",
      "        [-2.0113],\n",
      "        [-2.0017],\n",
      "        [-2.0064],\n",
      "        [-1.9941],\n",
      "        [-2.0079],\n",
      "        [-2.0107],\n",
      "        [-2.0097],\n",
      "        [-2.0072],\n",
      "        [-2.0078],\n",
      "        [-2.0086],\n",
      "        [-2.0065],\n",
      "        [-2.0031],\n",
      "        [-2.0183],\n",
      "        [-2.0065],\n",
      "        [-2.0055],\n",
      "        [-2.0117],\n",
      "        [-2.0094],\n",
      "        [-2.0049],\n",
      "        [-2.0131],\n",
      "        [-1.9951],\n",
      "        [-2.0027],\n",
      "        [-2.0082],\n",
      "        [-2.0073],\n",
      "        [-2.0050],\n",
      "        [-2.0063],\n",
      "        [-2.0050],\n",
      "        [-2.0026],\n",
      "        [-2.0010],\n",
      "        [-2.0096],\n",
      "        [-2.0064],\n",
      "        [-2.0066],\n",
      "        [-2.0060],\n",
      "        [-2.0066],\n",
      "        [-2.0013],\n",
      "        [-2.0063],\n",
      "        [-2.0059],\n",
      "        [-2.0061],\n",
      "        [-2.0084],\n",
      "        [-2.0078],\n",
      "        [-2.0081],\n",
      "        [-2.0111],\n",
      "        [-2.0061],\n",
      "        [-1.9988],\n",
      "        [-2.0034],\n",
      "        [-2.0077],\n",
      "        [-1.9921],\n",
      "        [-2.0059],\n",
      "        [-2.0113],\n",
      "        [-2.0060],\n",
      "        [-2.0018],\n",
      "        [-2.0096],\n",
      "        [-2.0079],\n",
      "        [-2.0130],\n",
      "        [-2.0019],\n",
      "        [-2.0062],\n",
      "        [-2.0035],\n",
      "        [-2.0042],\n",
      "        [-1.9977],\n",
      "        [-2.0091],\n",
      "        [-2.0061],\n",
      "        [-1.9996],\n",
      "        [-2.0078],\n",
      "        [-1.9990],\n",
      "        [-1.9877],\n",
      "        [-1.9953],\n",
      "        [-2.0069],\n",
      "        [-2.0092],\n",
      "        [-2.0102],\n",
      "        [-2.0072],\n",
      "        [-2.0144],\n",
      "        [-2.0274],\n",
      "        [-2.0061],\n",
      "        [-2.0083],\n",
      "        [-2.0063],\n",
      "        [-2.0097],\n",
      "        [-2.0142],\n",
      "        [-2.0083],\n",
      "        [-1.9908],\n",
      "        [-1.9995],\n",
      "        [-2.0051],\n",
      "        [-2.0081],\n",
      "        [-2.0121],\n",
      "        [-2.0018],\n",
      "        [-2.0018],\n",
      "        [-2.0002],\n",
      "        [-2.0078],\n",
      "        [-2.0035],\n",
      "        [-1.9985],\n",
      "        [-2.0131],\n",
      "        [-2.0095],\n",
      "        [-2.0092],\n",
      "        [-2.0184],\n",
      "        [-2.0050],\n",
      "        [-2.0073],\n",
      "        [-2.0098],\n",
      "        [-2.0039],\n",
      "        [-2.0061],\n",
      "        [-2.0031],\n",
      "        [-2.0078],\n",
      "        [-2.0055],\n",
      "        [-2.0065],\n",
      "        [-2.0126],\n",
      "        [-2.0057],\n",
      "        [-2.0040],\n",
      "        [-2.0015],\n",
      "        [-2.0131],\n",
      "        [-2.0094],\n",
      "        [-2.0109],\n",
      "        [-2.0060],\n",
      "        [-2.0096],\n",
      "        [-2.0061],\n",
      "        [-2.0048],\n",
      "        [-2.0158],\n",
      "        [-2.0007],\n",
      "        [-2.0118],\n",
      "        [-2.0074],\n",
      "        [-2.0099],\n",
      "        [-2.0048],\n",
      "        [-1.9970],\n",
      "        [-2.0060],\n",
      "        [-2.0047],\n",
      "        [-2.0082],\n",
      "        [-2.0048],\n",
      "        [-2.0060],\n",
      "        [-2.0051],\n",
      "        [-2.0054],\n",
      "        [-2.0029],\n",
      "        [-1.9978],\n",
      "        [-2.0140],\n",
      "        [-2.0060],\n",
      "        [-2.0056],\n",
      "        [-2.0139],\n",
      "        [-2.0053],\n",
      "        [-2.0061],\n",
      "        [-2.0106],\n",
      "        [-2.0060],\n",
      "        [-2.0056],\n",
      "        [-2.0066],\n",
      "        [-2.0058],\n",
      "        [-2.0053],\n",
      "        [-2.0129],\n",
      "        [-2.0085],\n",
      "        [-2.0033],\n",
      "        [-2.0090],\n",
      "        [-2.0069],\n",
      "        [-2.0084],\n",
      "        [-2.0064],\n",
      "        [-2.0051],\n",
      "        [-2.0069],\n",
      "        [-2.0088],\n",
      "        [-2.0052],\n",
      "        [-2.0102],\n",
      "        [-2.0066],\n",
      "        [-2.0023],\n",
      "        [-2.0064],\n",
      "        [-2.0062],\n",
      "        [-2.0010],\n",
      "        [-2.0060],\n",
      "        [-2.0003],\n",
      "        [-2.0064],\n",
      "        [-2.0046],\n",
      "        [-2.0054],\n",
      "        [-2.0075],\n",
      "        [-2.0028],\n",
      "        [-2.0136],\n",
      "        [-2.0021],\n",
      "        [-2.0050],\n",
      "        [-2.0121],\n",
      "        [-2.0113],\n",
      "        [-2.0069],\n",
      "        [-2.0112],\n",
      "        [-2.0068],\n",
      "        [-2.0152],\n",
      "        [-2.0085],\n",
      "        [-2.0128],\n",
      "        [-2.0059],\n",
      "        [-2.0060],\n",
      "        [-2.0033],\n",
      "        [-2.0102]], requires_grad=True))\n",
      "('hidden_b_var.1', Parameter containing:\n",
      "tensor([[-2.0057],\n",
      "        [-2.0049],\n",
      "        [-2.0053],\n",
      "        [-2.0057],\n",
      "        [-2.0065],\n",
      "        [-2.0060],\n",
      "        [-2.0062],\n",
      "        [-2.0066],\n",
      "        [-2.0066],\n",
      "        [-2.0061],\n",
      "        [-2.0048],\n",
      "        [-2.0061],\n",
      "        [-2.0057],\n",
      "        [-2.0067],\n",
      "        [-2.0048],\n",
      "        [-2.0060],\n",
      "        [-2.0061],\n",
      "        [-2.0062],\n",
      "        [-2.0061],\n",
      "        [-2.0061],\n",
      "        [-2.0057],\n",
      "        [-2.0052],\n",
      "        [-2.0065],\n",
      "        [-2.0063],\n",
      "        [-2.0058],\n",
      "        [-2.0056],\n",
      "        [-2.0070],\n",
      "        [-2.0065],\n",
      "        [-2.0058],\n",
      "        [-2.0061],\n",
      "        [-2.0061],\n",
      "        [-2.0052],\n",
      "        [-2.0060],\n",
      "        [-2.0064],\n",
      "        [-2.0061],\n",
      "        [-2.0056],\n",
      "        [-2.0060],\n",
      "        [-2.0061],\n",
      "        [-2.0060],\n",
      "        [-2.0062],\n",
      "        [-2.0053],\n",
      "        [-2.0061],\n",
      "        [-2.0063],\n",
      "        [-2.0055],\n",
      "        [-2.0061],\n",
      "        [-2.0054],\n",
      "        [-2.0059],\n",
      "        [-2.0061],\n",
      "        [-2.0060],\n",
      "        [-2.0068],\n",
      "        [-2.0059],\n",
      "        [-2.0045],\n",
      "        [-2.0058],\n",
      "        [-2.0056],\n",
      "        [-2.0061],\n",
      "        [-2.0066],\n",
      "        [-2.0052],\n",
      "        [-2.0065],\n",
      "        [-2.0051],\n",
      "        [-2.0064],\n",
      "        [-2.0060],\n",
      "        [-2.0058],\n",
      "        [-2.0069],\n",
      "        [-2.0074],\n",
      "        [-2.0070],\n",
      "        [-2.0055],\n",
      "        [-2.0061],\n",
      "        [-2.0060],\n",
      "        [-2.0066],\n",
      "        [-2.0078],\n",
      "        [-2.0059],\n",
      "        [-2.0060],\n",
      "        [-2.0069],\n",
      "        [-2.0049],\n",
      "        [-2.0072],\n",
      "        [-2.0064],\n",
      "        [-2.0054],\n",
      "        [-2.0061],\n",
      "        [-2.0053],\n",
      "        [-2.0063],\n",
      "        [-2.0052],\n",
      "        [-2.0064],\n",
      "        [-2.0062],\n",
      "        [-2.0060],\n",
      "        [-2.0070],\n",
      "        [-2.0048],\n",
      "        [-2.0068],\n",
      "        [-2.0060],\n",
      "        [-2.0062],\n",
      "        [-2.0057],\n",
      "        [-2.0053],\n",
      "        [-2.0060],\n",
      "        [-2.0051],\n",
      "        [-2.0072],\n",
      "        [-2.0060],\n",
      "        [-2.0051],\n",
      "        [-2.0052],\n",
      "        [-2.0067],\n",
      "        [-2.0048],\n",
      "        [-2.0047],\n",
      "        [-2.0055],\n",
      "        [-2.0060],\n",
      "        [-2.0063],\n",
      "        [-2.0061],\n",
      "        [-2.0057],\n",
      "        [-2.0044],\n",
      "        [-2.0059],\n",
      "        [-2.0059],\n",
      "        [-2.0042],\n",
      "        [-2.0063],\n",
      "        [-2.0062],\n",
      "        [-2.0061],\n",
      "        [-2.0066],\n",
      "        [-2.0058],\n",
      "        [-2.0054],\n",
      "        [-2.0054],\n",
      "        [-2.0061],\n",
      "        [-2.0057],\n",
      "        [-2.0070],\n",
      "        [-2.0062],\n",
      "        [-2.0065],\n",
      "        [-2.0070],\n",
      "        [-2.0069],\n",
      "        [-2.0055],\n",
      "        [-2.0064],\n",
      "        [-2.0057],\n",
      "        [-2.0072],\n",
      "        [-2.0060],\n",
      "        [-2.0059],\n",
      "        [-2.0061],\n",
      "        [-2.0064],\n",
      "        [-2.0060],\n",
      "        [-2.0055],\n",
      "        [-2.0069],\n",
      "        [-2.0062],\n",
      "        [-2.0058],\n",
      "        [-2.0057],\n",
      "        [-2.0064],\n",
      "        [-2.0066],\n",
      "        [-2.0060],\n",
      "        [-2.0068],\n",
      "        [-2.0062],\n",
      "        [-2.0059],\n",
      "        [-2.0064],\n",
      "        [-2.0061],\n",
      "        [-2.0067],\n",
      "        [-2.0060],\n",
      "        [-2.0060],\n",
      "        [-2.0064],\n",
      "        [-2.0071],\n",
      "        [-2.0060],\n",
      "        [-2.0060],\n",
      "        [-2.0056],\n",
      "        [-2.0071],\n",
      "        [-2.0061],\n",
      "        [-2.0063],\n",
      "        [-2.0066],\n",
      "        [-2.0062],\n",
      "        [-2.0064],\n",
      "        [-2.0064],\n",
      "        [-2.0060],\n",
      "        [-2.0066],\n",
      "        [-2.0066],\n",
      "        [-2.0060],\n",
      "        [-2.0061],\n",
      "        [-2.0060],\n",
      "        [-2.0058],\n",
      "        [-2.0058],\n",
      "        [-2.0062],\n",
      "        [-2.0058],\n",
      "        [-2.0060],\n",
      "        [-2.0060],\n",
      "        [-2.0078],\n",
      "        [-2.0060],\n",
      "        [-2.0060],\n",
      "        [-2.0068],\n",
      "        [-2.0062],\n",
      "        [-2.0064],\n",
      "        [-2.0057],\n",
      "        [-2.0062],\n",
      "        [-2.0069],\n",
      "        [-2.0063],\n",
      "        [-2.0063],\n",
      "        [-2.0066],\n",
      "        [-2.0067],\n",
      "        [-2.0061],\n",
      "        [-2.0060],\n",
      "        [-2.0069],\n",
      "        [-2.0061],\n",
      "        [-2.0064],\n",
      "        [-2.0060],\n",
      "        [-2.0070],\n",
      "        [-2.0061],\n",
      "        [-2.0059],\n",
      "        [-2.0060],\n",
      "        [-2.0059],\n",
      "        [-2.0057],\n",
      "        [-2.0056],\n",
      "        [-2.0063],\n",
      "        [-2.0055],\n",
      "        [-2.0060],\n",
      "        [-2.0063],\n",
      "        [-2.0062],\n",
      "        [-2.0058],\n",
      "        [-2.0062],\n",
      "        [-2.0053],\n",
      "        [-2.0060],\n",
      "        [-2.0059],\n",
      "        [-2.0060],\n",
      "        [-2.0060],\n",
      "        [-2.0056],\n",
      "        [-2.0062],\n",
      "        [-2.0054],\n",
      "        [-2.0058],\n",
      "        [-2.0062],\n",
      "        [-2.0061],\n",
      "        [-2.0060],\n",
      "        [-2.0060],\n",
      "        [-2.0074],\n",
      "        [-2.0062],\n",
      "        [-2.0060],\n",
      "        [-2.0075],\n",
      "        [-2.0054],\n",
      "        [-2.0061],\n",
      "        [-2.0058],\n",
      "        [-2.0074],\n",
      "        [-2.0059],\n",
      "        [-2.0061],\n",
      "        [-2.0058],\n",
      "        [-2.0061],\n",
      "        [-2.0063],\n",
      "        [-2.0063],\n",
      "        [-2.0060],\n",
      "        [-2.0062],\n",
      "        [-2.0065],\n",
      "        [-2.0055],\n",
      "        [-2.0060],\n",
      "        [-2.0060],\n",
      "        [-2.0061],\n",
      "        [-2.0054],\n",
      "        [-2.0060],\n",
      "        [-2.0063],\n",
      "        [-2.0060],\n",
      "        [-2.0056],\n",
      "        [-2.0070],\n",
      "        [-2.0061],\n",
      "        [-2.0052],\n",
      "        [-2.0066],\n",
      "        [-2.0068],\n",
      "        [-2.0060],\n",
      "        [-2.0045],\n",
      "        [-2.0069],\n",
      "        [-2.0060],\n",
      "        [-2.0059],\n",
      "        [-2.0059],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        [-2.0059]], requires_grad=True))\n",
      "('out_layers_w_mean.0', Parameter containing:\n",
      "tensor([[-0.1903],\n",
      "        [ 0.0262],\n",
      "        [-0.0187],\n",
      "        [ 0.0391],\n",
      "        [-0.0975],\n",
      "        [ 0.1049],\n",
      "        [-0.0994],\n",
      "        [ 0.0796],\n",
      "        [ 0.1510],\n",
      "        [-0.2374],\n",
      "        [ 0.1518],\n",
      "        [ 0.1402],\n",
      "        [ 0.1292],\n",
      "        [ 0.0045],\n",
      "        [ 0.0320],\n",
      "        [-0.0529],\n",
      "        [-0.1723],\n",
      "        [-0.0487],\n",
      "        [-0.0101],\n",
      "        [-0.1039],\n",
      "        [ 0.0980],\n",
      "        [-0.0898],\n",
      "        [ 0.0149],\n",
      "        [-0.2111],\n",
      "        [-0.2005],\n",
      "        [-0.0352],\n",
      "        [-0.0243],\n",
      "        [ 0.0324],\n",
      "        [ 0.1276],\n",
      "        [-0.0008],\n",
      "        [-0.0485],\n",
      "        [ 0.0467],\n",
      "        [-0.1710],\n",
      "        [-0.0421],\n",
      "        [ 0.1007],\n",
      "        [-0.0359],\n",
      "        [ 0.0170],\n",
      "        [-0.0017],\n",
      "        [ 0.0028],\n",
      "        [ 0.1358],\n",
      "        [ 0.1168],\n",
      "        [ 0.0400],\n",
      "        [-0.1053],\n",
      "        [ 0.1915],\n",
      "        [-0.0676],\n",
      "        [-0.1115],\n",
      "        [ 0.0798],\n",
      "        [-0.1054],\n",
      "        [-0.0671],\n",
      "        [ 0.1163],\n",
      "        [-0.0285],\n",
      "        [ 0.0332],\n",
      "        [ 0.0678],\n",
      "        [-0.1401],\n",
      "        [-0.0023],\n",
      "        [ 0.1802],\n",
      "        [-0.0314],\n",
      "        [ 0.1172],\n",
      "        [ 0.0535],\n",
      "        [ 0.0785],\n",
      "        [-0.0885],\n",
      "        [ 0.1050],\n",
      "        [ 0.1351],\n",
      "        [-0.0167],\n",
      "        [ 0.1122],\n",
      "        [-0.0486],\n",
      "        [ 0.0296],\n",
      "        [ 0.1171],\n",
      "        [-0.1351],\n",
      "        [-0.0918],\n",
      "        [-0.0133],\n",
      "        [-0.0442],\n",
      "        [ 0.1277],\n",
      "        [ 0.0197],\n",
      "        [ 0.1515],\n",
      "        [-0.1221],\n",
      "        [-0.0461],\n",
      "        [-0.0314],\n",
      "        [-0.1424],\n",
      "        [ 0.1186],\n",
      "        [ 0.0110],\n",
      "        [ 0.1450],\n",
      "        [-0.0545],\n",
      "        [ 0.1462],\n",
      "        [-0.1460],\n",
      "        [ 0.0074],\n",
      "        [-0.1978],\n",
      "        [-0.0649],\n",
      "        [-0.2025],\n",
      "        [-0.0720],\n",
      "        [ 0.0533],\n",
      "        [-0.1790],\n",
      "        [ 0.1728],\n",
      "        [ 0.0539],\n",
      "        [-0.1613],\n",
      "        [ 0.1614],\n",
      "        [-0.0549],\n",
      "        [ 0.2185],\n",
      "        [ 0.1172],\n",
      "        [-0.1163],\n",
      "        [ 0.0862],\n",
      "        [ 0.0622],\n",
      "        [ 0.0773],\n",
      "        [-0.0410],\n",
      "        [-0.1419],\n",
      "        [ 0.0584],\n",
      "        [-0.1348],\n",
      "        [ 0.0312],\n",
      "        [-0.0038],\n",
      "        [ 0.2127],\n",
      "        [ 0.1029],\n",
      "        [ 0.0215],\n",
      "        [ 0.1136],\n",
      "        [ 0.0414],\n",
      "        [ 0.0475],\n",
      "        [ 0.0827],\n",
      "        [-0.1627],\n",
      "        [-0.0519],\n",
      "        [ 0.1619],\n",
      "        [-0.2396],\n",
      "        [-0.0534],\n",
      "        [-0.0651],\n",
      "        [ 0.0792],\n",
      "        [-0.0251],\n",
      "        [ 0.1267],\n",
      "        [ 0.0574],\n",
      "        [ 0.2329],\n",
      "        [-0.0465],\n",
      "        [-0.0632],\n",
      "        [-0.0525],\n",
      "        [-0.1053],\n",
      "        [-0.0538],\n",
      "        [-0.0968],\n",
      "        [ 0.1452],\n",
      "        [ 0.0578],\n",
      "        [ 0.0832],\n",
      "        [-0.1313],\n",
      "        [ 0.0547],\n",
      "        [-0.1050],\n",
      "        [ 0.0265],\n",
      "        [ 0.0327],\n",
      "        [ 0.0266],\n",
      "        [-0.1569],\n",
      "        [-0.1224],\n",
      "        [ 0.1686],\n",
      "        [-0.1877],\n",
      "        [ 0.0422],\n",
      "        [-0.0204],\n",
      "        [ 0.0697],\n",
      "        [-0.1029],\n",
      "        [ 0.0396],\n",
      "        [-0.0398],\n",
      "        [-0.0204],\n",
      "        [ 0.1085],\n",
      "        [-0.1440],\n",
      "        [-0.0699],\n",
      "        [ 0.0198],\n",
      "        [ 0.2530],\n",
      "        [-0.1037],\n",
      "        [-0.0459],\n",
      "        [ 0.0216],\n",
      "        [ 0.1995],\n",
      "        [-0.0495],\n",
      "        [ 0.1277],\n",
      "        [-0.0535],\n",
      "        [-0.0426],\n",
      "        [-0.0938],\n",
      "        [ 0.0283],\n",
      "        [-0.0525],\n",
      "        [-0.0584],\n",
      "        [ 0.0382],\n",
      "        [ 0.0835],\n",
      "        [ 0.0363],\n",
      "        [-0.1242],\n",
      "        [ 0.2544],\n",
      "        [-0.1829],\n",
      "        [ 0.2067],\n",
      "        [ 0.0278],\n",
      "        [ 0.0611],\n",
      "        [-0.1123],\n",
      "        [-0.0398],\n",
      "        [-0.1421],\n",
      "        [ 0.1490],\n",
      "        [-0.0761],\n",
      "        [-0.2486],\n",
      "        [ 0.0662],\n",
      "        [-0.1427],\n",
      "        [-0.0423],\n",
      "        [-0.1042],\n",
      "        [ 0.0082],\n",
      "        [ 0.1954],\n",
      "        [-0.0098],\n",
      "        [-0.0691],\n",
      "        [-0.0991],\n",
      "        [-0.0357],\n",
      "        [ 0.0610],\n",
      "        [-0.1260],\n",
      "        [ 0.0281],\n",
      "        [-0.1012],\n",
      "        [-0.0977],\n",
      "        [-0.0024],\n",
      "        [-0.0953],\n",
      "        [ 0.1132],\n",
      "        [-0.0343],\n",
      "        [ 0.1176],\n",
      "        [ 0.0745],\n",
      "        [-0.1688],\n",
      "        [ 0.0394],\n",
      "        [-0.0161],\n",
      "        [ 0.1912],\n",
      "        [ 0.0105],\n",
      "        [-0.2989],\n",
      "        [-0.1098],\n",
      "        [ 0.0101],\n",
      "        [-0.0427],\n",
      "        [ 0.1688],\n",
      "        [-0.0034],\n",
      "        [-0.1422],\n",
      "        [-0.1134],\n",
      "        [ 0.0763],\n",
      "        [-0.0176],\n",
      "        [-0.0466],\n",
      "        [-0.0147],\n",
      "        [ 0.1515],\n",
      "        [ 0.1368],\n",
      "        [-0.0672],\n",
      "        [-0.0848],\n",
      "        [ 0.0688],\n",
      "        [ 0.0496],\n",
      "        [-0.1586],\n",
      "        [-0.0080],\n",
      "        [ 0.1623],\n",
      "        [ 0.1054],\n",
      "        [-0.1195],\n",
      "        [ 0.1287],\n",
      "        [-0.1423],\n",
      "        [-0.0843],\n",
      "        [-0.1362],\n",
      "        [-0.0705],\n",
      "        [ 0.0331],\n",
      "        [-0.2247],\n",
      "        [ 0.0587],\n",
      "        [ 0.1032],\n",
      "        [ 0.0142],\n",
      "        [-0.0516],\n",
      "        [-0.1659],\n",
      "        [ 0.0073],\n",
      "        [ 0.1867],\n",
      "        [-0.0581],\n",
      "        [ 0.1499],\n",
      "        [-0.0184],\n",
      "        [-0.0625],\n",
      "        [ 0.0140],\n",
      "        [ 0.0201],\n",
      "        [ 0.0906],\n",
      "        [-0.0815]], requires_grad=True))\n",
      "('out_layers_w_mean.1', Parameter containing:\n",
      "tensor([[-0.1903],\n",
      "        [ 0.0262],\n",
      "        [-0.0187],\n",
      "        [ 0.0391],\n",
      "        [-0.0975],\n",
      "        [ 0.1049],\n",
      "        [-0.0994],\n",
      "        [ 0.0796],\n",
      "        [ 0.1510],\n",
      "        [-0.2374],\n",
      "        [ 0.1518],\n",
      "        [ 0.1402],\n",
      "        [ 0.1292],\n",
      "        [ 0.0045],\n",
      "        [ 0.0320],\n",
      "        [-0.0529],\n",
      "        [-0.1723],\n",
      "        [-0.0487],\n",
      "        [-0.0101],\n",
      "        [-0.1039],\n",
      "        [ 0.0980],\n",
      "        [-0.0898],\n",
      "        [ 0.0149],\n",
      "        [-0.2111],\n",
      "        [-0.2005],\n",
      "        [-0.0352],\n",
      "        [-0.0243],\n",
      "        [ 0.0324],\n",
      "        [ 0.1276],\n",
      "        [-0.0008],\n",
      "        [-0.0485],\n",
      "        [ 0.0467],\n",
      "        [-0.1710],\n",
      "        [-0.0421],\n",
      "        [ 0.1007],\n",
      "        [-0.0359],\n",
      "        [ 0.0170],\n",
      "        [-0.0017],\n",
      "        [ 0.0028],\n",
      "        [ 0.1358],\n",
      "        [ 0.1168],\n",
      "        [ 0.0400],\n",
      "        [-0.1053],\n",
      "        [ 0.1915],\n",
      "        [-0.0676],\n",
      "        [-0.1115],\n",
      "        [ 0.0798],\n",
      "        [-0.1054],\n",
      "        [-0.0671],\n",
      "        [ 0.1163],\n",
      "        [-0.0285],\n",
      "        [ 0.0332],\n",
      "        [ 0.0678],\n",
      "        [-0.1401],\n",
      "        [-0.0023],\n",
      "        [ 0.1802],\n",
      "        [-0.0314],\n",
      "        [ 0.1172],\n",
      "        [ 0.0535],\n",
      "        [ 0.0785],\n",
      "        [-0.0885],\n",
      "        [ 0.1050],\n",
      "        [ 0.1351],\n",
      "        [-0.0167],\n",
      "        [ 0.1122],\n",
      "        [-0.0486],\n",
      "        [ 0.0296],\n",
      "        [ 0.1171],\n",
      "        [-0.1351],\n",
      "        [-0.0918],\n",
      "        [-0.0133],\n",
      "        [-0.0442],\n",
      "        [ 0.1277],\n",
      "        [ 0.0197],\n",
      "        [ 0.1515],\n",
      "        [-0.1221],\n",
      "        [-0.0461],\n",
      "        [-0.0314],\n",
      "        [-0.1424],\n",
      "        [ 0.1186],\n",
      "        [ 0.0110],\n",
      "        [ 0.1450],\n",
      "        [-0.0545],\n",
      "        [ 0.1462],\n",
      "        [-0.1460],\n",
      "        [ 0.0074],\n",
      "        [-0.1978],\n",
      "        [-0.0649],\n",
      "        [-0.2025],\n",
      "        [-0.0720],\n",
      "        [ 0.0533],\n",
      "        [-0.1790],\n",
      "        [ 0.1728],\n",
      "        [ 0.0539],\n",
      "        [-0.1613],\n",
      "        [ 0.1614],\n",
      "        [-0.0549],\n",
      "        [ 0.2185],\n",
      "        [ 0.1172],\n",
      "        [-0.1163],\n",
      "        [ 0.0862],\n",
      "        [ 0.0622],\n",
      "        [ 0.0773],\n",
      "        [-0.0410],\n",
      "        [-0.1419],\n",
      "        [ 0.0584],\n",
      "        [-0.1348],\n",
      "        [ 0.0312],\n",
      "        [-0.0038],\n",
      "        [ 0.2127],\n",
      "        [ 0.1029],\n",
      "        [ 0.0215],\n",
      "        [ 0.1136],\n",
      "        [ 0.0414],\n",
      "        [ 0.0475],\n",
      "        [ 0.0827],\n",
      "        [-0.1627],\n",
      "        [-0.0519],\n",
      "        [ 0.1619],\n",
      "        [-0.2396],\n",
      "        [-0.0534],\n",
      "        [-0.0651],\n",
      "        [ 0.0792],\n",
      "        [-0.0251],\n",
      "        [ 0.1267],\n",
      "        [ 0.0574],\n",
      "        [ 0.2329],\n",
      "        [-0.0465],\n",
      "        [-0.0632],\n",
      "        [-0.0525],\n",
      "        [-0.1053],\n",
      "        [-0.0538],\n",
      "        [-0.0968],\n",
      "        [ 0.1452],\n",
      "        [ 0.0578],\n",
      "        [ 0.0832],\n",
      "        [-0.1313],\n",
      "        [ 0.0547],\n",
      "        [-0.1050],\n",
      "        [ 0.0265],\n",
      "        [ 0.0327],\n",
      "        [ 0.0266],\n",
      "        [-0.1569],\n",
      "        [-0.1224],\n",
      "        [ 0.1686],\n",
      "        [-0.1877],\n",
      "        [ 0.0422],\n",
      "        [-0.0204],\n",
      "        [ 0.0697],\n",
      "        [-0.1029],\n",
      "        [ 0.0396],\n",
      "        [-0.0398],\n",
      "        [-0.0204],\n",
      "        [ 0.1085],\n",
      "        [-0.1440],\n",
      "        [-0.0699],\n",
      "        [ 0.0198],\n",
      "        [ 0.2530],\n",
      "        [-0.1037],\n",
      "        [-0.0459],\n",
      "        [ 0.0216],\n",
      "        [ 0.1995],\n",
      "        [-0.0495],\n",
      "        [ 0.1277],\n",
      "        [-0.0535],\n",
      "        [-0.0426],\n",
      "        [-0.0938],\n",
      "        [ 0.0283],\n",
      "        [-0.0525],\n",
      "        [-0.0584],\n",
      "        [ 0.0382],\n",
      "        [ 0.0835],\n",
      "        [ 0.0363],\n",
      "        [-0.1242],\n",
      "        [ 0.2544],\n",
      "        [-0.1829],\n",
      "        [ 0.2067],\n",
      "        [ 0.0278],\n",
      "        [ 0.0611],\n",
      "        [-0.1123],\n",
      "        [-0.0398],\n",
      "        [-0.1421],\n",
      "        [ 0.1490],\n",
      "        [-0.0761],\n",
      "        [-0.2486],\n",
      "        [ 0.0662],\n",
      "        [-0.1427],\n",
      "        [-0.0423],\n",
      "        [-0.1042],\n",
      "        [ 0.0082],\n",
      "        [ 0.1954],\n",
      "        [-0.0098],\n",
      "        [-0.0691],\n",
      "        [-0.0991],\n",
      "        [-0.0357],\n",
      "        [ 0.0610],\n",
      "        [-0.1260],\n",
      "        [ 0.0281],\n",
      "        [-0.1012],\n",
      "        [-0.0977],\n",
      "        [-0.0024],\n",
      "        [-0.0953],\n",
      "        [ 0.1132],\n",
      "        [-0.0343],\n",
      "        [ 0.1176],\n",
      "        [ 0.0745],\n",
      "        [-0.1688],\n",
      "        [ 0.0394],\n",
      "        [-0.0161],\n",
      "        [ 0.1912],\n",
      "        [ 0.0105],\n",
      "        [-0.2989],\n",
      "        [-0.1098],\n",
      "        [ 0.0101],\n",
      "        [-0.0427],\n",
      "        [ 0.1688],\n",
      "        [-0.0034],\n",
      "        [-0.1422],\n",
      "        [-0.1134],\n",
      "        [ 0.0763],\n",
      "        [-0.0176],\n",
      "        [-0.0466],\n",
      "        [-0.0147],\n",
      "        [ 0.1515],\n",
      "        [ 0.1368],\n",
      "        [-0.0672],\n",
      "        [-0.0848],\n",
      "        [ 0.0688],\n",
      "        [ 0.0496],\n",
      "        [-0.1586],\n",
      "        [-0.0080],\n",
      "        [ 0.1623],\n",
      "        [ 0.1054],\n",
      "        [-0.1195],\n",
      "        [ 0.1287],\n",
      "        [-0.1423],\n",
      "        [-0.0843],\n",
      "        [-0.1362],\n",
      "        [-0.0705],\n",
      "        [ 0.0331],\n",
      "        [-0.2247],\n",
      "        [ 0.0587],\n",
      "        [ 0.1032],\n",
      "        [ 0.0142],\n",
      "        [-0.0516],\n",
      "        [-0.1659],\n",
      "        [ 0.0073],\n",
      "        [ 0.1867],\n",
      "        [-0.0581],\n",
      "        [ 0.1499],\n",
      "        [-0.0184],\n",
      "        [-0.0625],\n",
      "        [ 0.0140],\n",
      "        [ 0.0201],\n",
      "        [ 0.0906],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        [-0.0815]], requires_grad=True))\n",
      "('out_layers_w_mean.2', Parameter containing:\n",
      "tensor([[-0.1903],\n",
      "        [ 0.0262],\n",
      "        [-0.0187],\n",
      "        [ 0.0391],\n",
      "        [-0.0975],\n",
      "        [ 0.1049],\n",
      "        [-0.0994],\n",
      "        [ 0.0796],\n",
      "        [ 0.1510],\n",
      "        [-0.2374],\n",
      "        [ 0.1518],\n",
      "        [ 0.1402],\n",
      "        [ 0.1292],\n",
      "        [ 0.0045],\n",
      "        [ 0.0320],\n",
      "        [-0.0529],\n",
      "        [-0.1723],\n",
      "        [-0.0487],\n",
      "        [-0.0101],\n",
      "        [-0.1039],\n",
      "        [ 0.0980],\n",
      "        [-0.0898],\n",
      "        [ 0.0149],\n",
      "        [-0.2111],\n",
      "        [-0.2005],\n",
      "        [-0.0352],\n",
      "        [-0.0243],\n",
      "        [ 0.0324],\n",
      "        [ 0.1276],\n",
      "        [-0.0008],\n",
      "        [-0.0485],\n",
      "        [ 0.0467],\n",
      "        [-0.1710],\n",
      "        [-0.0421],\n",
      "        [ 0.1007],\n",
      "        [-0.0359],\n",
      "        [ 0.0170],\n",
      "        [-0.0017],\n",
      "        [ 0.0028],\n",
      "        [ 0.1358],\n",
      "        [ 0.1168],\n",
      "        [ 0.0400],\n",
      "        [-0.1053],\n",
      "        [ 0.1915],\n",
      "        [-0.0676],\n",
      "        [-0.1115],\n",
      "        [ 0.0798],\n",
      "        [-0.1054],\n",
      "        [-0.0671],\n",
      "        [ 0.1163],\n",
      "        [-0.0285],\n",
      "        [ 0.0332],\n",
      "        [ 0.0678],\n",
      "        [-0.1401],\n",
      "        [-0.0023],\n",
      "        [ 0.1802],\n",
      "        [-0.0314],\n",
      "        [ 0.1172],\n",
      "        [ 0.0535],\n",
      "        [ 0.0785],\n",
      "        [-0.0885],\n",
      "        [ 0.1050],\n",
      "        [ 0.1351],\n",
      "        [-0.0167],\n",
      "        [ 0.1122],\n",
      "        [-0.0486],\n",
      "        [ 0.0296],\n",
      "        [ 0.1171],\n",
      "        [-0.1351],\n",
      "        [-0.0918],\n",
      "        [-0.0133],\n",
      "        [-0.0442],\n",
      "        [ 0.1277],\n",
      "        [ 0.0197],\n",
      "        [ 0.1515],\n",
      "        [-0.1221],\n",
      "        [-0.0461],\n",
      "        [-0.0314],\n",
      "        [-0.1424],\n",
      "        [ 0.1186],\n",
      "        [ 0.0110],\n",
      "        [ 0.1450],\n",
      "        [-0.0545],\n",
      "        [ 0.1462],\n",
      "        [-0.1460],\n",
      "        [ 0.0074],\n",
      "        [-0.1978],\n",
      "        [-0.0649],\n",
      "        [-0.2025],\n",
      "        [-0.0720],\n",
      "        [ 0.0533],\n",
      "        [-0.1790],\n",
      "        [ 0.1728],\n",
      "        [ 0.0539],\n",
      "        [-0.1613],\n",
      "        [ 0.1614],\n",
      "        [-0.0549],\n",
      "        [ 0.2185],\n",
      "        [ 0.1172],\n",
      "        [-0.1163],\n",
      "        [ 0.0862],\n",
      "        [ 0.0622],\n",
      "        [ 0.0773],\n",
      "        [-0.0410],\n",
      "        [-0.1419],\n",
      "        [ 0.0584],\n",
      "        [-0.1348],\n",
      "        [ 0.0312],\n",
      "        [-0.0038],\n",
      "        [ 0.2127],\n",
      "        [ 0.1029],\n",
      "        [ 0.0215],\n",
      "        [ 0.1136],\n",
      "        [ 0.0414],\n",
      "        [ 0.0475],\n",
      "        [ 0.0827],\n",
      "        [-0.1627],\n",
      "        [-0.0519],\n",
      "        [ 0.1619],\n",
      "        [-0.2396],\n",
      "        [-0.0534],\n",
      "        [-0.0651],\n",
      "        [ 0.0792],\n",
      "        [-0.0251],\n",
      "        [ 0.1267],\n",
      "        [ 0.0574],\n",
      "        [ 0.2329],\n",
      "        [-0.0465],\n",
      "        [-0.0632],\n",
      "        [-0.0525],\n",
      "        [-0.1053],\n",
      "        [-0.0538],\n",
      "        [-0.0968],\n",
      "        [ 0.1452],\n",
      "        [ 0.0578],\n",
      "        [ 0.0832],\n",
      "        [-0.1313],\n",
      "        [ 0.0547],\n",
      "        [-0.1050],\n",
      "        [ 0.0265],\n",
      "        [ 0.0327],\n",
      "        [ 0.0266],\n",
      "        [-0.1569],\n",
      "        [-0.1224],\n",
      "        [ 0.1686],\n",
      "        [-0.1877],\n",
      "        [ 0.0422],\n",
      "        [-0.0204],\n",
      "        [ 0.0697],\n",
      "        [-0.1029],\n",
      "        [ 0.0396],\n",
      "        [-0.0398],\n",
      "        [-0.0204],\n",
      "        [ 0.1085],\n",
      "        [-0.1440],\n",
      "        [-0.0699],\n",
      "        [ 0.0198],\n",
      "        [ 0.2530],\n",
      "        [-0.1037],\n",
      "        [-0.0459],\n",
      "        [ 0.0216],\n",
      "        [ 0.1995],\n",
      "        [-0.0495],\n",
      "        [ 0.1277],\n",
      "        [-0.0535],\n",
      "        [-0.0426],\n",
      "        [-0.0938],\n",
      "        [ 0.0283],\n",
      "        [-0.0525],\n",
      "        [-0.0584],\n",
      "        [ 0.0382],\n",
      "        [ 0.0835],\n",
      "        [ 0.0363],\n",
      "        [-0.1242],\n",
      "        [ 0.2544],\n",
      "        [-0.1829],\n",
      "        [ 0.2067],\n",
      "        [ 0.0278],\n",
      "        [ 0.0611],\n",
      "        [-0.1123],\n",
      "        [-0.0398],\n",
      "        [-0.1421],\n",
      "        [ 0.1490],\n",
      "        [-0.0761],\n",
      "        [-0.2486],\n",
      "        [ 0.0662],\n",
      "        [-0.1427],\n",
      "        [-0.0423],\n",
      "        [-0.1042],\n",
      "        [ 0.0082],\n",
      "        [ 0.1954],\n",
      "        [-0.0098],\n",
      "        [-0.0691],\n",
      "        [-0.0991],\n",
      "        [-0.0357],\n",
      "        [ 0.0610],\n",
      "        [-0.1260],\n",
      "        [ 0.0281],\n",
      "        [-0.1012],\n",
      "        [-0.0977],\n",
      "        [-0.0024],\n",
      "        [-0.0953],\n",
      "        [ 0.1132],\n",
      "        [-0.0343],\n",
      "        [ 0.1176],\n",
      "        [ 0.0745],\n",
      "        [-0.1688],\n",
      "        [ 0.0394],\n",
      "        [-0.0161],\n",
      "        [ 0.1912],\n",
      "        [ 0.0105],\n",
      "        [-0.2989],\n",
      "        [-0.1098],\n",
      "        [ 0.0101],\n",
      "        [-0.0427],\n",
      "        [ 0.1688],\n",
      "        [-0.0034],\n",
      "        [-0.1422],\n",
      "        [-0.1134],\n",
      "        [ 0.0763],\n",
      "        [-0.0176],\n",
      "        [-0.0466],\n",
      "        [-0.0147],\n",
      "        [ 0.1515],\n",
      "        [ 0.1368],\n",
      "        [-0.0672],\n",
      "        [-0.0848],\n",
      "        [ 0.0688],\n",
      "        [ 0.0496],\n",
      "        [-0.1586],\n",
      "        [-0.0080],\n",
      "        [ 0.1623],\n",
      "        [ 0.1054],\n",
      "        [-0.1195],\n",
      "        [ 0.1287],\n",
      "        [-0.1423],\n",
      "        [-0.0843],\n",
      "        [-0.1362],\n",
      "        [-0.0705],\n",
      "        [ 0.0331],\n",
      "        [-0.2247],\n",
      "        [ 0.0587],\n",
      "        [ 0.1032],\n",
      "        [ 0.0142],\n",
      "        [-0.0516],\n",
      "        [-0.1659],\n",
      "        [ 0.0073],\n",
      "        [ 0.1867],\n",
      "        [-0.0581],\n",
      "        [ 0.1499],\n",
      "        [-0.0184],\n",
      "        [-0.0625],\n",
      "        [ 0.0140],\n",
      "        [ 0.0201],\n",
      "        [ 0.0906],\n",
      "        [-0.0815]], requires_grad=True))\n",
      "('out_layers_b_mean.0', Parameter containing:\n",
      "tensor([[-0.0843]], requires_grad=True))\n",
      "('out_layers_b_mean.1', Parameter containing:\n",
      "tensor([[-0.0843]], requires_grad=True))\n",
      "('out_layers_b_mean.2', Parameter containing:\n",
      "tensor([[-0.0843]], requires_grad=True))\n",
      "('out_layers_w_var.0', Parameter containing:\n",
      "tensor([[-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        [-2.]], requires_grad=True))\n",
      "('out_layers_w_var.1', Parameter containing:\n",
      "tensor([[-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.]], requires_grad=True))\n",
      "('out_layers_w_var.2', Parameter containing:\n",
      "tensor([[-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.]], requires_grad=True))\n",
      "('out_layers_b_var.0', Parameter containing:\n",
      "tensor([[-2.]], requires_grad=True))\n",
      "('out_layers_b_var.1', Parameter containing:\n",
      "tensor([[-2.]], requires_grad=True))\n",
      "('out_layers_b_var.2', Parameter containing:\n",
      "tensor([[-2.]], requires_grad=True))\n"
     ]
    }
   ],
   "source": [
    "for i in net.named_parameters():\n",
    "    print(i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 4]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "a = [1,2,3,4]\n",
    "print(a[slice(1,4)])\n",
    "print(a[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "for i in range(1):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
